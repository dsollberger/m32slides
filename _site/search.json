[
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html",
    "title": "01_-_Introductions",
    "section": "",
    "text": "library(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#introducting-the-presenter",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#introducting-the-presenter",
    "title": "01_-_Introductions",
    "section": "Introducting the Presenter",
    "text": "Introducting the Presenter\n\n\n\nLecturer: Derek Sollberger\n\nI go by “Derek” or “teacher”\n\nOriginally from Los Angeles\nBA in Applied Mathematics, UC Berkeley\nMS in Applied Mathematics, CSULB\nMS in Applied Mathematics, UC Merced"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#introducting-the-presenter-1",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#introducting-the-presenter-1",
    "title": "01_-_Introductions",
    "section": "Introducting the Presenter",
    "text": "Introducting the Presenter\n\n\n\n\n\nContinuing Lecturer in Applied Mathematics\n10+ years of teaching at UC Merced\nCourses:\n\nBio 18: Data Science\nBio 175: Biostatistics\nBio 184: Python for Bioinformatics\nMath 32: Probability and Statistics"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#current-research-in-pedagogy",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#current-research-in-pedagogy",
    "title": "01_-_Introductions",
    "section": "Current Research in Pedagogy",
    "text": "Current Research in Pedagogy\n\n\n\n\n\nactive learning\ncomputer programming\naugmented reality"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#on-notetaking",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#on-notetaking",
    "title": "01_-_Introductions",
    "section": "On Notetaking",
    "text": "On Notetaking\n\n\n\nDo not write all of the information from the slides\nDo write along with what Derek writes on the whiteboard\nMake a few notes for main ideas and computer programming\nRetain placement notes, such as “Example #2” or “Survey#1”\nNo need to copy computer code from lecture (code will be provided)"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#why-probability",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#why-probability",
    "title": "01_-_Introductions",
    "section": "Why Probability?",
    "text": "Why Probability?\n\nThe Classic Birthday Problem\nHow many students have to enter the classroom until there are two students that share a birthday?"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#deterministic-vs-probabilistic",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#deterministic-vs-probabilistic",
    "title": "01_-_Introductions",
    "section": "Deterministic vs Probabilistic",
    "text": "Deterministic vs Probabilistic\nDeterministic: a situation that can be solved with equation solving and/or an algorithm\n\nExample: If water boils at 100 degrees Celsius, what is that threshold in Fahrenheit?\n\nProbabilistic: a situation that cannot be completely solved due to an element of chance\n\nExample: What is the chance that it will rain tomorrow?"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#probability-and-you",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#probability-and-you",
    "title": "01_-_Introductions",
    "section": "Probability and You",
    "text": "Probability and You\n\nApplied MathematicsBioengineeringChemical SciencesComputer Science and EngineeringEnvironmental EngineeringMaterials Science and EngineeringMechanical EngineeringPhysics\n\n\nDoes a probabilistic sequence converge or diverge?\n\n\nWhat percentage of lyme disease patients would be cured with the current but experimental treatments?\n\n\nWhat proportion of reactants undergo a reaction early in the reaction?\n\n\nHow many computers in a network would be affected after a virus infection?\n\n\nHow many of a certain species of plants are in the Vernal Pools Reserve?\n\n\nWhat percentage of a semiconductor is made of impurities?\n\n\nFor a commercial passenger airplane, what is the probability that at least two engines fail during a flight?\n\n\nHow many stars are in the Milky Way?"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#ugh-the-syllabus",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#ugh-the-syllabus",
    "title": "01_-_Introductions",
    "section": "Ugh, the Syllabus",
    "text": "Ugh, the Syllabus\n\nDescriptionCLOsPLOs\n\n\nConcepts of probability and statistics. Conditional probability, independence, random variables, distribution functions, descriptive statistics, transformations, sampling errors, confidence intervals, least squares and maximum likelihood. Exploratory data analysis and interactive computing.\n\n\n\nDevelop probabilistic models of random phenomena.\nInfer statistical models from real data.\nApply mathematical methods to probabilistic/statistical models to\n\n\nMake predictions and\nQuantify the uncertainty in these predictions.\n\n\nWrite and run “simple” R programs for the purposes of data analysis, modeling, and visualization.\n\n\n\n\nSolve mathematical problems using analytical methods.\nSolve mathematical problems using computational methods.\nRecognize the relationships between different areas of mathematics and the connections between mathematics and other disciplines.\nGive clear and organized written and verbal explanations of mathematical ideas to a variety of audiences\nModel real-world problems mathematically and analyze those models using their mastery of the core concepts."
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#assessment",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#assessment",
    "title": "01_-_Introductions",
    "section": "Assessment",
    "text": "Assessment\n\nBefore-LectureComputer ProgrammingWritten AssignmentsDiscussionSurveysExams\n\n\n\n5 percent of semester grade\nquizzes due before lecture (i.e. 3 PM on TuTh)\n\nno extensions\n\n5 to 10 minutes per quiz\n\nreview concepts and formulas\npreview thought exercises\n\n\n\n\n\n20 percent of semester grade\nlanguage: R\nplatform: LearnR apps\nanswers to frequently asked questions\n\nno, work may not be done in another language (e.g. Python)\nno, work may not be done in another IDE (e.g. VS Code)\n\n10 to 20 minutes per week\n\n\n\n\n20 percent of semester grade\nclassical math textbook homework\nadvice: do most of the work during your discussion section\n\n\n\n\n10 percent of semester grade for discussion section participation\nTA will track attendance\nadvised to work on written and computer assignments during discussion sections\n\n\n\n\n5 percent of semester grade\ngraded quickly on effort and completion\n5 to 10 minutes per survey\n\n\n\n\nExam 1: 10 percent of semester grade (Tuesday, Sept. 27)\nExam 2: 15 percent of semester grade (Tuesday, Nov. 1)\nFinal Exam: 15 percent of semester grade (date TBD, see survey)"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#student-accessibility-services",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#student-accessibility-services",
    "title": "01_-_Introductions",
    "section": "Student Accessibility Services",
    "text": "Student Accessibility Services\nSpecial Accommodations: University of California, Merced is committed to creating learning environments that are accessible to all. If you anticipate or experience physical or academic barriers based on a disability, please feel welcome to contact me privately so we can discuss options. In addition, please contact Student Accessibility Services (SAS) at (209) 228-6996 or disabilityservices@ucmerced.edu as soon as possible to explore reasonable accommodations. All accommodations must have prior approval from Student Accessibility Services on the basis of appropriate documentation. If you anticipate or experience barriers due to pregnancy, temporary medical condition, or injury,please feel welcome to contact me so we can discuss options. You are encouraged to contact the Dean of Students for support and resources at (209) 228-3633 or https://studentaffairs.ucmerced.edu/dean-students."
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#academic-integrity",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#academic-integrity",
    "title": "01_-_Introductions",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAcademic integrity is the foundation of an academic community and without it none of the educational or research goals of the university can be achieved. All members of the community are responsible for its academic integrity. Existing policies forbid cheating on examinations, plagiarism and other forms of academic dishonesty. The UC Merced Academic Honesty Policy The UC Merced Academic Honesty Policy can be found on the Student Conduct website. Infractions against academic integrity will incur consequences such as an “F” on the assignment/exam and/or a report to the Academic Senate."
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#nerdy-example",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#nerdy-example",
    "title": "01_-_Introductions",
    "section": "Nerdy Example",
    "text": "Nerdy Example\nHow many numbers between zero and one do we have to add up to have a sum that is greater than one?\n\nAssume selection from a uniform distribution"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#cumulative-summation",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#cumulative-summation",
    "title": "01_-_Introductions",
    "section": "Cumulative Summation",
    "text": "Cumulative Summation\nLet us start with the natural numbers\n\\[i = \\{1, 2, 3, ...\\}\\]\nThen cumulative summation takes place with\n\\[F(n) = \\sum_{i = 1}^{n} i\\]"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#cumulative-summation-1",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#cumulative-summation-1",
    "title": "01_-_Introductions",
    "section": "Cumulative Summation",
    "text": "Cumulative Summation\nIn R, we can define a sequence of natural numbers\n\nnatural_numbers <- 1:10\nprint(natural_numbers)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nand then employ the cumsum() function to perform the cumulative summation.\n\ncumsum(natural_numbers)\n\n [1]  1  3  6 10 15 21 28 36 45 55"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#random-number-generation",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#random-number-generation",
    "title": "01_-_Introductions",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nIn R, we generate a random number between zero and one (here: assumed from a uniform distribution) with the runif function.\n\nrunif(1)\n\n[1] 0.81505\n\n\nFrom there, we can (for example) produce a sample of \\(n = 32\\) such random numbers\n\nrunif(32)\n\n [1] 0.23201307 0.87127077 0.70026832 0.87895876 0.87331526 0.05036756\n [7] 0.70975768 0.04589654 0.81148165 0.67277373 0.36444518 0.05252578\n[13] 0.93071222 0.65069917 0.63382323 0.50296153 0.74808861 0.93567290\n[19] 0.21726304 0.03206728 0.93737557 0.21783477 0.90292332 0.21608644\n[25] 0.31198929 0.27790058 0.42172615 0.43130608 0.20329206 0.94117982\n[31] 0.95907322 0.59290890"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#one-iteration",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#one-iteration",
    "title": "01_-_Introductions",
    "section": "One Iteration",
    "text": "One Iteration\nNext, we employ function composition to apply cumulative summation to our vector of random numbers\n\nX <- cumsum(runif(32))\nprint(X)\n\n [1]  0.2451511  0.5850619  1.2441624  2.0179309  2.5838395  2.9461865\n [7]  3.1303979  3.8536004  4.5735742  4.6358679  4.8938637  5.6998413\n[13]  6.3232469  7.0119696  7.9724195  8.2188149  8.7869916  9.1473904\n[19]  9.8618203 10.3766007 11.3126492 11.9842738 12.1088123 12.6723460\n[25] 13.6674356 14.3986901 15.0511902 15.1968950 16.1945759 16.9695972\n[31] 17.9123227 17.9644575\n\n\nand then we can check when our cumulative summation first exceeded 1.0\n\nwhich.max(X > 1.0)\n\n[1] 3"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#simulation",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#simulation",
    "title": "01_-_Introductions",
    "section": "Simulation",
    "text": "Simulation\nTo try to understand the randomness, we can repeat the procedure for many iterations (here, \\(N = 10000\\)).\n\nN <- 1e5 #number of iterations\nour_results <- rep(NA, N) #initialize space for results\nfor(i in 1:N){\n  this_vector <- cumsum(runif(10))\n  this_result <- which.max(this_vector > 1.0)\n  our_results[i] <- this_result\n}"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#visualization",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#visualization",
    "title": "01_-_Introductions",
    "section": "Visualization",
    "text": "Visualization\nTo understand a distribution of a probabilistic setting, we can visualize the results.\n\nCodeGraph\n\n\n\ndf <- data.frame(our_results)\ndf |>\n  ggplot() +\n  geom_histogram(aes(x = our_results), binwidth = 1,\n                 color = \"black\", fill = \"blue\") +\n  labs(title = \"Histogram of Results\",\n       subtitle = \"How would you describe the distribution?\",\n       caption = \"Math 32\",\n       x = \"number of numbers needed\",\n       y = \"count\") +\n  scale_x_continuous(breaks = seq(2,8))"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#measure-of-centrality",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#measure-of-centrality",
    "title": "01_-_Introductions",
    "section": "Measure of Centrality",
    "text": "Measure of Centrality\nTo hone in on our understanding of the distribution, let us take the mean() of our_results\n\nmean(our_results, na.rm = TRUE)\n\n[1] 2.71847\n\n\n\nNote: R stops execution upon evaluating a missing value. For our intents and purposes, we will suppress that exception with na.rm = TRUE"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#nerdy-example-1",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#nerdy-example-1",
    "title": "01_-_Introductions",
    "section": "Nerdy Example",
    "text": "Nerdy Example\nHow many numbers between zero and one do we have to add up to have a sum that is greater than one?\n\\[ e \\approx 2.718282\\]\n\n# theoretical answer\nexp(1)\n\n[1] 2.718282\n\n\nThought questions:\n\nhow do we know that the answer converges?\nhow many iterations did we need for a sufficient answer?"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#looking-ahead",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#looking-ahead",
    "title": "01_-_Introductions",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\nBe mindful of before-lecture quizzes\n\ndue Fri., Aug. 26:\n\nFinal Exam Date (survey)\nSoftware Installation\n\n\nExam 1 will be on Tues., Sept. 27"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 32",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "m32slides",
    "section": "",
    "text": "Nov 10, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html",
    "title": "02_-_Independence",
    "section": "",
    "text": "library(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#starter-examples",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#starter-examples",
    "title": "02_-_Independence",
    "section": "Starter Examples",
    "text": "Starter Examples\nIn order to introduce probability concepts, you will notice that many sections of this book start out by talking about simple situations such as flipping a coin or rolling a die (or rolling a pair of dice). This is to ease the reader into more complicated examples.\n\n\n\n\n\n\nNote\n\n\n\nA possibility space is a set of all of the possible outcomes."
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-1-one-die",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-1-one-die",
    "title": "02_-_Independence",
    "section": "Example 1: One Die",
    "text": "Example 1: One Die\n\n\nConsider rolling one six-sided die. For each of the following events, list their possible ways, and then find their probabilities:\n\nA: rolling an even number\nB: rolling a number greater than 3\nC: rolling a double-digit number\n\n\n\n\n\none, six-sided die"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#starting-to-combine",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#starting-to-combine",
    "title": "02_-_Independence",
    "section": "Starting to Combine",
    "text": "Starting to Combine\nAt first, we consider simply adding up the probabilities to compute the probability of the union of the probabilities:\n\\[P(A \\cup B) = P(A) + P(B) = ?\\]"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#inconsistent",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#inconsistent",
    "title": "02_-_Independence",
    "section": "Inconsistent?",
    "text": "Inconsistent?\nSo far,\n\\[P(A \\cup B) = P(A) + P(B) = 100\\%\\]\nWhat went wrong? (Discuss with a neighbor)"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#some-set-notation",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#some-set-notation",
    "title": "02_-_Independence",
    "section": "Some Set Notation",
    "text": "Some Set Notation\n\n\n\n\n\n\nTip\n\n\n\nThe union of sets A and B is the set of all elements that appear either in set A OR set B\n\\[A \\cup B = \\{x : x \\in A \\text{ OR } x \\in B\\}\\]\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe intersection of sets A and B is the set of all elements that appear both in set A AND set B\n\\[A \\cap B = \\{x : x \\in A \\text{ AND } x \\in B\\}\\]"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#inclusion-exclusion-principle",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#inclusion-exclusion-principle",
    "title": "02_-_Independence",
    "section": "Inclusion-Exclusion Principle",
    "text": "Inclusion-Exclusion Principle\nWe observe that there was an overlap in the calculation. Since some elements were counted twice, we can compensate by subtracting one copy of that overlap. This notion is called the Inclusion-Exclusion Principle\n\n\n\n\n\n\nNote\n\n\n\nTo compute the probability of a set union, we need to consider the overlapping portions. For two sets A and B, the probability of observing the union is\n\\[P(A \\cup B) = {\\color{red}P(A)} + {\\color{blue}P(B)} - {\\color{purple}P(A \\cap B)}\\]"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-2-two-dice",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-2-two-dice",
    "title": "02_-_Independence",
    "section": "Example 2: Two dice",
    "text": "Example 2: Two dice\n\n\nConsider rolling two six-sided dice. Find the probability that their total is 8 or the second die shows a number greater or equal to 5.\n\n\n\n\ntwo dice"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#subtlety-in-assumptions",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#subtlety-in-assumptions",
    "title": "02_-_Independence",
    "section": "Subtlety in Assumptions",
    "text": "Subtlety in Assumptions\nIn the previous example, we assumed a notion of sampling with replacement. That is, when rolling dice, we know that a number can be repeated. In other situations were observations cannot repeat, we are sampling without replacement."
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-3-faculty-matters",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-3-faculty-matters",
    "title": "02_-_Independence",
    "section": "Example 3: Faculty Matters",
    "text": "Example 3: Faculty Matters\n\nPromptSample Space 1Sample Space 2\n\n\nConsider a subset of the UC Merced Applied Math department with 6 faculty members—Blanchette, Buvoli, Sandoval, Stepanian, Thompson, Yatskar—must select two of its members to serve on a personnel review committee. Because the work will be time-consuming, no one is anxious to serve, so it is decided that the representatives will be selected by putting 6 slips of paper in a box, mixing them, and selecting two without replacement.\n\nWhat is the probability that both Thompson and Yatskar will be selected?\nWhat is the probability that at least one of the two members whose name begins with ‘B’ is selected?\nIf the 6 faculty members have taught for 15, 3, 5, 9, 4, and 12 years, respectively, at the university, what is the probability that the two chosen representatives have at least a combined 10 years’ teaching experience at the university?"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#assuming-a-distribution",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#assuming-a-distribution",
    "title": "02_-_Independence",
    "section": "Assuming a Distribution",
    "text": "Assuming a Distribution\nUnless otherwise noted, the coin flip has two disjoint outcomes—heads or tails—with probabilities\n\\[P(\\text{heads}) = 0.5, \\quad P(\\text{tails}) = 0.5\\]\n\nIn the early formation of the field of statistics, there were considerations such as the following. How should the possibility space for a trial of flipping two coins be represented?\n\n3 elements: \\(\\{\\)two heads, mixed result, two tails\\(\\}\\), OR\n4 elements: \\(\\{\\)HH, HT, TH, TT\\(\\}\\)"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#complements",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#complements",
    "title": "02_-_Independence",
    "section": "Complements",
    "text": "Complements\n\n\n\n\n\n\nNote\n\n\n\nTo sets are disjoint if\n\\[P(A \\cap B) = 0\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nA possibility space is a set of all of the possible outcomes for an event. It is usually denoted by the Greek letter capital omega.\n\n\nFor example, the set of all outcomes for two coin flips of a fair coin turns out to be\n\\[\\Omega = \\{HH, HT, TH, TT\\}\\]\n\n\n\n\n\n\nNote\n\n\n\nIf \\(A\\) is a set (and a subset of the possibility space), then the complement of A, denoted \\(A^{c}\\), is the set of outcomes that is in the universal set but not in the set A\n\\[A \\subseteq \\Omega \\quad\\Rightarrow\\quad A^{c} = \\Omega - A\\]"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-4-one-die",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-4-one-die",
    "title": "02_-_Independence",
    "section": "Example 4: One Die",
    "text": "Example 4: One Die\n\n\n\n\nFor example, if we think of our roll of a six-sided die, the possibility space was\n\\[\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\]\nIf we have a subset consisting of the even numbers\n\\[E = \\{2, 4, 6\\}\\]\nwhat do you think the complement \\(E^{c}\\) will be?"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#empty-set",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#empty-set",
    "title": "02_-_Independence",
    "section": "Empty Set",
    "text": "Empty Set\n\n\n\n\n\n\nNote\n\n\n\nThe empty set \\(\\{\\}\\) literally has zero elements in the set\n\n\nClaim: Set \\(A\\) and its complement are disjoint."
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-5-replacement",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-5-replacement",
    "title": "02_-_Independence",
    "section": "Example 5: Replacement",
    "text": "Example 5: Replacement\nThere may be situations where we need to be careful about whether selections from a set were done with replacement or without replacement.\nIn the wardrobe, there are 8 blue socks and 6 red socks.\n\\[B, B, B, B, B, B, B, B\\]\n\\[R, R, R, R, R, R\\]\nCompute the following probabilities\n\nSelecting 3 red socks with replacement\nSelecting 3 red socks without replacement\nSelecting 4 blue socks with replacement\nSelecting 4 blue socks without replacement\n\n\n\n\n\n\n\nTip\n\n\n\nNotice how when we sample with replacement, each iteration is independent of the previous iterations. When we sample without replacement, each iteration depends on the previous iterations."
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#de-morgans-law",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#de-morgans-law",
    "title": "02_-_Independence",
    "section": "De Morgan’s Law",
    "text": "De Morgan’s Law\nOne relationship between the notions of complements, intersections, and unions is as follows.\n\n\n\n\n\n\nNote\n\n\n\n\\[(A \\cup B)^{c} = A^{c} \\cap B^{c}\\]\nThe complement of the union is the intersection of the complements."
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#looking-ahead",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#looking-ahead",
    "title": "02_-_Independence",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 2:\n\nWHW1\nLHW1\nCLO (survey)\n\nBe mindful of before-lecture quizzes\n\nExam 1 will be on Tues., Sept. 27\n\n\n\n\nRafa Moral\n\n\n40-second song"
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#example-subsetting",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#example-subsetting",
    "title": "Dependence",
    "section": "Example: Subsetting",
    "text": "Example: Subsetting\nConsider the months of the year\n\\[M = \\{\\text{Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec} \\}\\]\nLet us say that a month is ``long’’ if it has 31 days. What is the probability that we have a long month given that we are in the Fall semester?\n\\[F = \\{ \\text{Aug, Sep, Oct, Nov, Dec} \\}\\]"
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#conditional-probability",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#conditional-probability",
    "title": "Dependence",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nWe can condense this process into a formula for conditional probability:\n\n\n\n\n\n\nNote\n\n\n\nThe conditional probability of observing event \\(A\\) given event \\(B\\) has already taken place is\n\\[P(A|B) = \\displaystyle\\frac{P(A \\cap B)}{P(B)}\\]"
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#example-contigency-tables",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#example-contigency-tables",
    "title": "Dependence",
    "section": "Example: Contigency Tables",
    "text": "Example: Contigency Tables\nIn this hypothetical example, suppose that we are following an epidemiologist who is testing patients at a hospital in for the novel strain of coronavirus.\n\nBuild a contingency table with the following data\n\n\n175 true positives\n32 false negatives\n18 false positives\n2019 true negatives\n\n\nCompute the probability that a randomly selected patient is disease free given that the drug test is positive."
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#prosecutors-fallacy",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#prosecutors-fallacy",
    "title": "Dependence",
    "section": "Prosecutor’s Fallacy",
    "text": "Prosecutor’s Fallacy\n\nUsing the same counts as the previous example, compute the probability that for a randomly selected patient the test returns positive given that the patient is disease free.\n\n\n\n\n\n\n\nWarning\n\n\n\nConverses for conditional probability are almost never equal.\n\\[P(A|B) \\neq P(B|A)\\]"
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#bayes-rule",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#bayes-rule",
    "title": "Dependence",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\nIn the previous section, we studied conditional probability\n\\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\]\nand we talked about how the inverse probabilities \\(P(A|B)\\) and \\(P(B|A)\\) are almost never equal. In this section, we discuss how to properly think and calculate that inverse probability.\n\n\n\n\n\n\nTip\n\n\n\nAnother look at conditional probability is\n\\[P(A \\text{ and } B) = P(B|A) \\cdot P(A)\\]\nThis is read as “The probability of the intersection \\(A\\) and \\(B\\) is the probability of event \\(B\\) conditioned on event \\(A\\).”\n\n\n\n\n\n\n\n\nTip\n\n\n\nMoreover, if we consider how if event \\(B\\) is dependent on event \\(A\\), then sometimes \\(B\\) happens when \\(A\\) happens and sometimes when \\(A\\) does not occur. More succinctly, the total probability of event \\(B\\) is\n\\[P(B) = P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nStaring with the conditional probability formula\n\\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\]\nBayes’ Rule combines the ideas of conditioned probability and total probability as\n\\[P(A|B) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(B)} = \\displaystyle\\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})}\\]"
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#a-deep-dive",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#a-deep-dive",
    "title": "Dependence",
    "section": "A Deep Dive",
    "text": "A Deep Dive\n\nExampleTree DiagramNumeratorDenominator\n\n\nAn executive has their blood tested for boneitis. Let \\(B\\) be the event that an executive has the disease, and let \\(T\\) be the event that the test returns positive. Laboratory trials yielded the following information:\n\\[P(T|B) = 0.70 \\quad\\text{and}\\quad P(T|B^{c}) = 0.10\\]\nAssume a prior probability of \\(P(B) = 0.0032\\). Compute \\(P(B|T)\\)\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram"
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#more-practice",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#more-practice",
    "title": "Dependence",
    "section": "More Practice",
    "text": "More Practice\n\nExampleTree DiagramNumeratorDenominator\n\n\nAn executive has their blood tested for boneitis. Let \\(B\\) be the event that an executive has the disease, and let \\(T\\) be the event that the test returns positive. Laboratory trials yielded the following information:\n\\[P(T|B) = 0.70 \\quad\\text{and}\\quad P(T|B^{c}) = 0.10\\]\nAssume a prior probability of \\(P(B) = 0.0032\\). Compute \\(P(B|T^{c})\\)\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram"
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#looking-ahead",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#looking-ahead",
    "title": "Dependence",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 2:\n\nWHW1\nLHW1\nCLO (survey)\n\nBe mindful of before-lecture quizzes\nNo discussions next week for Math 32 (Sept. 5-7)\nExam 1 will be on Tues., Sept. 27\n\n\n\n\n\nsong about Bayes’ Rule"
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-spam-filtering",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-spam-filtering",
    "title": "Bayes’ Rule (Examples)",
    "section": "Example: Spam Filtering",
    "text": "Example: Spam Filtering\nIn 2002, Paul Graham used Bayes’ Rule as part of his algorithms to greatly decrease false positive rates of unwanted e-mails (“spam”). Let \\(H^{c}\\) be the event that an e-mail is “spam”. Let \\(W\\) be the event that an e-mail contains a trigger word such as “watches”. Suppose that\n\nthe probability that an e-mail contains that word given that it is spam is 17%\nthe probability that an e-mail contains that word given that it is not spam is 9%\nthe probability that a randomly selected e-mail message is spam is 80%\n\nFind the probability that an e-mail message is spam, given that the trigger word appears."
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-quality-control",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-quality-control",
    "title": "Bayes’ Rule (Examples)",
    "section": "Example: Quality Control",
    "text": "Example: Quality Control\nA manufacturing process produces integrated circuit chips. Over the long run the fraction of bad chips produced by the process is around 20%. Thoroughly testing a chip to determine whether it is good or bad is rather expensive, so a cheap test is tried. All good chips will pass the cheap test, but so will 10% of the bad chips. Given that a chip passes the test, what is the probability that the chip was defective?"
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-monty-hall-problem",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-monty-hall-problem",
    "title": "Bayes’ Rule (Examples)",
    "section": "Example: Monty Hall Problem",
    "text": "Example: Monty Hall Problem\n\n\n\n\nMonty Hall asks you to choose one of three doors. One of the doors hides a prize and the other two doors have no prize. You state out loud which door you pick, but you don’t open it right away.\n“Monty opens one of the other two doors, and there is no prize behind it.\n“At this moment, there are two closed doors, one of which you picked. The prize is behind one of the closed doors, but you don’t know which one. Monty asks you, ‘Do you want to switch doors?’”\n\nswitch doors\ndo not switch doors"
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-dui-checkpoint",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-dui-checkpoint",
    "title": "Bayes’ Rule (Examples)",
    "section": "Example: DUI Checkpoint",
    "text": "Example: DUI Checkpoint\nA breath analyzer, used by the police to test whether drivers exceed the legal limit set for the blood alcohol percentage while driving, is known to satisfy\n\\[P(A|B) = P(A^{c}|B^{c}) = x\\]\nwhere \\(A\\) is the event “breath analyzer indicates that legal limit is exceeded” and \\(B\\) “driver’s blood alcohol percentage exceeds legal limit.” On Saturday nights, about 4% of the drivers are known to exceed the limit.\n\nDescribe in words the meaning of \\(P(B|A)\\)\nDetermine \\(P(B|A)\\) if \\(x = 0.90\\)\nHow big should \\(x\\) be so that \\(P(B|A) \\geq 0.95\\)?"
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-disease-outbreak",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-disease-outbreak",
    "title": "Bayes’ Rule (Examples)",
    "section": "Example: Disease Outbreak",
    "text": "Example: Disease Outbreak\nSuppose that at UC Merced, there is a two percent chance that a freshman has herpes at the end of the school year. Let \\(H\\) be the event of having the virus, while \\(C\\) represents the event that the freshman is from the Cathedral dorm. Among the herpes carriers, the probably of being a Cathedral resident is 32%. Among those free of disease, the probably of being a Cathedral resident is 13%. What is the probability that a freshman has herpes, given that you know that he or she lived in the Cathedral dorm?"
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#generalized-bayes-rule",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#generalized-bayes-rule",
    "title": "Bayes’ Rule (Examples)",
    "section": "Generalized Bayes’ Rule",
    "text": "Generalized Bayes’ Rule\nIf we are conditioning \\(B\\) on an event \\(A\\), where the latter can be partitioned into several subsets,\n\\[A = \\{ A_{1}, A_{2}, ..., A_{j} \\}\\]\nthen the total probability is\n\\[P(B) = P(B|A_{1}) \\cdot P(A_{1}) + P(B|A_{2}) \\cdot P(A_{2}) + ... + P(B|A_{n}) \\cdot P(A_{n})\\]\nand Bayes Rule for computing the probability of \\(A_{i}\\) given \\(B\\) becomes\n\\[P(A_{i}|B) = \\displaystyle\\frac{ P(B|A_{i}) \\cdot P(A_{i}) }{ P(B|A_{1}) \\cdot P(A_{1}) + P(B|A_{2}) \\cdot P(A_{2}) + ... + P(B|A_{n}) \\cdot P(A_{n}) }\\]"
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#bayesian-odds",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#bayesian-odds",
    "title": "Bayes’ Rule (Examples)",
    "section": "Bayesian Odds",
    "text": "Bayesian Odds\n\n\n\n\n\n\nNote\n\n\n\nThe Bayesian odds of event \\(A\\) to event \\(B\\) given that event \\(C\\) has already taken place is\n\\[\\displaystyle\\frac{ P(A|C) }{ P(B|C) }\\]"
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#looking-ahead",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#looking-ahead",
    "title": "Bayes’ Rule (Examples)",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 9:\n\nWHW2\nLHW2\nPerceptions of Probability (survey)\n\nBe mindful of before-lecture quizzes\nNo discussions this week for Math 32 (Sept. 5-7)\nExam 1 will be on Tues., Sept. 27\n\n\n\nsource"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#review-summation-notation",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#review-summation-notation",
    "title": "05: Measures of Centrality",
    "section": "Review: Summation Notation",
    "text": "Review: Summation Notation\nThe Fibonnaci Sequence \\(\\{1, 1, 2, 3, 5, 8, 13, ...\\}\\) is an example of a sequence where\n\\(F_{1} = 1, F_{2} = 1, F_{n} = F_{n-1} + F_{n-2}\\)\nCompute\n\n\\(\\displaystyle\\sum_{i=1}^{1} a_{i}\\)\n\\(\\displaystyle\\sum_{i=1}^{2} a_{i}\\)\n\\(\\displaystyle\\sum_{i=1}^{3} a_{i}\\)\n\\(\\displaystyle\\sum_{i=1}^{4} a_{i}\\)"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#populations-versus-samples",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#populations-versus-samples",
    "title": "05: Measures of Centrality",
    "section": "Populations versus Samples",
    "text": "Populations versus Samples\nWe tend to study a relatively small sample to understand the behavior of a much larger population."
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#mean-average",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#mean-average",
    "title": "05: Measures of Centrality",
    "section": "Mean (Average)",
    "text": "Mean (Average)\n\n“mean” and “average” are synonymous and will be used interchangably\nThe mean of \\(\\{ x_{1}, x_{2}, ..., x_{n} \\}\\) is denoted by\n\nGreek letter \\(\\mu\\) (“mu”) for a population mean (where we know all of the elements)\nAnglicized \\(\\bar{x}\\) (“x bar”) for a sample mean (where we are working with a sample of data)\n\nTo calculate the mean\n\nAdd up all of the numbers\nDivide by the amount of numbers\n\n\n\\[\\mu = \\displaystyle\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N} x_{i} \\quad\\text{or}\\quad \\bar{x} = \\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} x_{i}\\]"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#example-one-die",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#example-one-die",
    "title": "05: Measures of Centrality",
    "section": "Example: One Die",
    "text": "Example: One Die\n\nFind the mean of the roll of one six-sided die.\nFind the mean of the sample \\(\\{21, 22, 23, 32\\}\\)."
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#median",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#median",
    "title": "05: Measures of Centrality",
    "section": "Median",
    "text": "Median\nThe median of an ordered, discrete set of numbers is the number in the middle. If there are an even amount of data, then the median is the average of the middle two numbers in the ordered data set.\n\nCompute the median of \\(\\{1, 2, 1, 5, 3\\}\\)\nCompute the median of \\(\\{1, 1, 2, 3, 5, 8\\}\\)"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#weighted-mean",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#weighted-mean",
    "title": "05: Measures of Centrality",
    "section": "Weighted Mean",
    "text": "Weighted Mean\n\\[\\bar{x} = \\displaystyle\\frac{ \\displaystyle\\sum_{i=1}^{n} {\\color{red}w_{i}} \\cdot {\\color{blue}x_{i}}  }{ \\displaystyle\\sum_{i=1}^{n} {\\color{red}w_{i}} }\\]"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#example-cal-kulas",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#example-cal-kulas",
    "title": "05: Measures of Centrality",
    "section": "Example: Cal Kulas",
    "text": "Example: Cal Kulas\n\n\n\nSetting\nGoing into the final exam for a Statistics course, Cal Kulas had earned the following marks in the other categories.\n\n\n\n\n\nCategory\n\n\nWeight\n\n\nCal Kulas\n\n\n\n\n\n\nattendance\n\n\n10%\n\n\n95%\n\n\n\n\nquizzes\n\n\n20%\n\n\n75%\n\n\n\n\nmidterms\n\n\n25%\n\n\n60%\n\n\n\n\nproject\n\n\n20%\n\n\n90%\n\n\n\n\n\n\n\n\nTasks\n\nWhat is his current grade in the course?\nWhat does Cal Kulas need on the final exam so that he earns at least 80% in the course?"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#review-summation-notation-1",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#review-summation-notation-1",
    "title": "05: Measures of Centrality",
    "section": "Review: Summation Notation",
    "text": "Review: Summation Notation\nThe Fibonnaci Sequence \\(\\{1, 1, 2, 3, 5, 8, 13, ...\\}\\) is an example of a sequence where\n\\(F_{1} = 1, F_{2} = 1, F_{n} = F_{n-1} + F_{n-2}\\)\nCompute\n\n\\(\\displaystyle\\prod_{i=1}^{1} a_{i}\\)\n\\(\\displaystyle\\prod_{i=1}^{2} a_{i}\\)\n\\(\\displaystyle\\prod_{i=1}^{3} a_{i}\\)\n\\(\\displaystyle\\prod_{i=1}^{4} a_{i}\\)"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#for-formulas",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#for-formulas",
    "title": "05: Measures of Centrality",
    "section": "For Formulas",
    "text": "For Formulas\nIn particular, sigma and product notation will allow us to express probability formulas more efficiently. For example, the independence formula for two events \\(A\\) and \\(B\\),\n\\[P(AB) = P(A) \\cdot P(B)\\]\nbecomes the following for \\(n\\) independent events:\n\\[P\\left(  \\{X_{i}\\}_{i=1}^{n} \\right) = \\displaystyle\\prod_{i=1}^{n} P(X_{i})\\]"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#looking-ahead",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#looking-ahead",
    "title": "05: Measures of Centrality",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 9:\n\nWHW2\nLHW2\nPerceptions of Probability (survey)\n\nBe mindful of before-lecture quizzes\nExam 1 will be on Tues., Sept. 27\n\n\n\n\n\nEven NASA makes mistakes!\n\n\nsource"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#overview",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#overview",
    "title": "06: Variance",
    "section": "Overview",
    "text": "Overview\n\\[s^{2} = \\displaystyle\\frac{1}{n-1}\\displaystyle\\sum_{i = 1}^{n} (x_{i} - \\bar{x})^{2}\\]\nToday’s main questions are “What is variance and what is a standard deviation?” We will go through\n\nthe formulas and calculations\ndemostrations\napplications (word problems)"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#notation",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#notation",
    "title": "06: Variance",
    "section": "Notation",
    "text": "Notation\n\n\n\n\nWe tend to study a relatively small sample to understand the behavior of a much larger population."
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#example-nathans-hot-dog-eating-contest",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#example-nathans-hot-dog-eating-contest",
    "title": "06: Variance",
    "section": "Example: Nathan’s Hot Dog Eating Contest",
    "text": "Example: Nathan’s Hot Dog Eating Contest\n\n\nEach year on July 4, the Nathan’s Hot Dog Eating Contest takes place on Coney Island in New York. The rule is simple: eat as many hot dogs (and buns) as you can in 10 minutes. The past 5 winning amounts were: 63, 70, 72, 74, 71. Compute the variance.\n\n\n\nsource\n\n\n\n\n\n\nwinners Joey Chestnut and Miki Sudo"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#units",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#units",
    "title": "06: Variance",
    "section": "Units?",
    "text": "Units?\n\n\n\nsquare hot dogs?"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#demostrations",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#demostrations",
    "title": "06: Variance",
    "section": "Demostrations",
    "text": "Demostrations\n\nSetupRA to BC to D\n\n\nFor each of the following sets\n\n\\(A = \\{1, 2, 3, 4, 5, 6, 7\\}\\)\n\\(B = \\{3, 4, 5, 6, 7, 8, 9\\}\\)\n\\(C = \\{-3, -2, -1, 0, 1, 2, 3\\}\\)\n\\(D = \\{-9, -6, -3, 0, 3, 6, 9\\}\\)\n\nwe will compute the sample mean, sample median, and sample standard deviation.\n\n\n\nA <- seq(1, 7, 1)\nB <- A + 2\nC <- seq(-3, 3, 3)\nD <- 3*C\n\n\n\n\nmean(A)\n\n[1] 4\n\nmean(B)\n\n[1] 6\n\n\n\nmedian(A)\n\n[1] 4\n\nmedian(B)\n\n[1] 6\n\n\n\nsd(A)\n\n[1] 2.160247\n\nsd(B)\n\n[1] 2.160247\n\n\n\n\n\nmean(C)\n\n[1] 0\n\nmean(D)\n\n[1] 0\n\n\n\nmedian(C)\n\n[1] 0\n\nmedian(D)\n\n[1] 0\n\n\n\nsd(C)\n\n[1] 3\n\nsd(D)\n\n[1] 9"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#shiny",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#shiny",
    "title": "06: Variance",
    "section": "Shiny",
    "text": "Shiny\n\n# https://quarto.org/docs/interactive/shiny/\nsliderInput(\"mu\", \"mean:\", \n            min = 1, max = 10, value = 5)\nsliderInput(\"s\", \"deviation:\", \n            min = 1, max = 5, value = 2)\nplotOutput(\"distPlot\")\n\n\noutput$distPlot <- renderPlot({\n  x <- rnorm(100, mean = mu, sd = s)\n  df <- data.frame(x = x)\n  \n  df |>\n    ggplot(aes(x = x)) +\n    geom_density()\n})"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#standardization",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#standardization",
    "title": "06: Variance",
    "section": "Standardization",
    "text": "Standardization\n\\[z = \\displaystyle\\frac{x - \\mu}{\\sigma}\\]\nTo standardize data, compute a z-score by\n\nsubtracting by the mean\nthen dividing by the standard deviation\n\nThis calculation is considered to be “unitless”, and the units are usually said as “[number of] standard deviations above/below the mean”\nMost data falls within two standard deviations of the mean,\n\\[\\text{usually } z \\in (-2, 2)\\]\nbut \\(z \\in (-\\infty, \\infty)\\)"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#example-dating-website-data",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#example-dating-website-data",
    "title": "06: Variance",
    "section": "Example: Dating Website Data",
    "text": "Example: Dating Website Data\n\nScenario 1Scenario 2\n\n\n\nAccording to OkCupid data, if men rate women on a scale from 1 = “least attractive” to 7 = “most attractive”, the average score is 3.99 with a sample standard deviation of 1.6401.\n\nWhat is the \\(z\\)-score of a woman rated a “6”?\nWhat is the attractiveness score of a woman at a \\(z\\)-score of 1.5?\n\n\n\n\nAccording to OkCupid data, if women rate men on a scale from 1 = “least attractive” to 7 = “most attractive”, the average score is 2.43 with a sample standard deviation of 1.2510.\n\nWhat is the \\(z\\)-score of a woman rated a “6”?\nWhat is the attractiveness score of a woman at a \\(z\\)-score of 1.5?"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#looking-ahead",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#looking-ahead",
    "title": "06: Variance",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 16:\n\nWHW3\nLHW3\nDemographics Part 1 (survey)\n\nBe mindful of before-lecture quizzes\nExam 1 will be on Tues., Sept. 27\n\nmore information in weekly announcements\n\n\n\n\nsource"
  },
  {
    "objectID": "posts/07_-_Expectation/Math_32_07_Expectation.html#example-demographics",
    "href": "posts/07_-_Expectation/Math_32_07_Expectation.html#example-demographics",
    "title": "07: Expectation",
    "section": "Example: Demographics",
    "text": "Example: Demographics\n\nSuppose that all of the students in Math 32 are between ages 19 and 21 inclusively with the following distribution:\n\nAge 19: 35%\nAge 20: 45%\nAge 21: 20%\n\nRewrite the data as a discrete mass function."
  },
  {
    "objectID": "posts/07_-_Expectation/Math_32_07_Expectation.html#discrete-probability-distributions",
    "href": "posts/07_-_Expectation/Math_32_07_Expectation.html#discrete-probability-distributions",
    "title": "07: Expectation",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\n\n\n\n\n\n\nNote\n\n\n\nA discrete probability distribution is a population where we can list the possible values\n\\[X = \\{ x_{1}, x_{2}, x_{3}, ... x_{n} \\}\\]\nand measure the respective probabilities\n\\[P(X = x_{1}), P(X = x_{2}), P(X = x_{3}), ..., P(X = x_{n})\\]\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs usual, the probabilities rules include that each probability is between zero and one inclusively\n\\[0 \\leq P(X = x_{i}) \\leq 1\\]\nand that all probabilities add up to 100 percent\n\\[\\displaystyle\\sum_{x \\in X} P(X = x) = 1\\]"
  },
  {
    "objectID": "posts/07_-_Expectation/Math_32_07_Expectation.html#expectation",
    "href": "posts/07_-_Expectation/Math_32_07_Expectation.html#expectation",
    "title": "07: Expectation",
    "section": "Expectation",
    "text": "Expectation\n\n\n\n\n\n\nNote\n\n\n\nFor a random variable \\(X\\) (understood through a discrete probability distribution), its expected value is\n\\[\\mu = \\text{E}[{\\color{blue}X}] = \\displaystyle\\sum_{x \\in X} {\\color{blue}x} \\cdot {\\color{red}P(X = x)}\\]\n\n\n\nCompute the expected value of the roll of one six-sided die."
  },
  {
    "objectID": "posts/07_-_Expectation/Math_32_07_Expectation.html#variance",
    "href": "posts/07_-_Expectation/Math_32_07_Expectation.html#variance",
    "title": "07: Expectation",
    "section": "Variance",
    "text": "Variance\n\n\n\n\n\n\nNote\n\n\n\nThe variance of a random variable \\(X\\) is defined as the expected squared deviation from the mean\n\\[\\text{Var}(X) = \\text{E}[(X - \\text{E}[X])^{2}]\\]\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe above theoretical formula is built from first principles and is good for building the math foundation. However, the following practical formula is better for hand calculations and computer calculations.\n\n\nClaim:\n\\[\\text{Var}(X) = \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2}\\]"
  },
  {
    "objectID": "posts/07_-_Expectation/Math_32_07_Expectation.html#example-boxing-bets",
    "href": "posts/07_-_Expectation/Math_32_07_Expectation.html#example-boxing-bets",
    "title": "07: Expectation",
    "section": "Example: Boxing Bets",
    "text": "Example: Boxing Bets\n\n\nBefore watching a boxing match, my friends and I made bets over which round the fight would end. A boxing match lasts up to 12 rounds. Each gambler pays $5 and is assigned a round randomly. The winner garners the whole pot of money. What is the expected value of the bet? What is the variance of the bet?"
  },
  {
    "objectID": "posts/07_-_Expectation/Math_32_07_Expectation.html#looking-ahead",
    "href": "posts/07_-_Expectation/Math_32_07_Expectation.html#looking-ahead",
    "title": "07: Expectation",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 16:\n\nWHW3\nLHW3\nDemographics Part 1 (survey)\n\nBe mindful of before-lecture quizzes\nExam 1 will be on Tues., Sept. 27\n\nmore information in weekly announcements\n\n\n\n\nsource"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#bernoulli-trials",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#bernoulli-trials",
    "title": "08: Binomial Distribution",
    "section": "Bernoulli Trials",
    "text": "Bernoulli Trials\nTo continue our exploration of discrete distributions, we will look at situations that have two disjoint possibilities.\n\n\n\n\n\n\nBernoulli Trials\n\n\n\nFor math symbols to represent a Bernoulli trial, the events \\(\\{1, 0\\}\\) have respective probabilities \\(p\\) and \\(1-p\\).\n\n\nFor example, for one flip of a coin\n\\[P(\\text{heads}) = p, \\quad P(\\text{tails}) = 1-p\\]\n\n\n\none coin, but not necessarily fair"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#arrangements",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#arrangements",
    "title": "08: Binomial Distribution",
    "section": "Arrangements",
    "text": "Arrangements\n\n\n\n\n\n\nPermutations\n\n\n\nPermutations (and the number of permutations) are the arrangements when order matters\n\n\n\n\n\n\n\n\nCombinations\n\n\n\nCombinations (and the number of combinations) are the arrangements when order does not matter\n\n\nFlipping 3 fair coins, what is the probability that heads will be observed exactly twice?"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#choose",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#choose",
    "title": "08: Binomial Distribution",
    "section": "Choose",
    "text": "Choose\n\n\n\\[\\binom{n}{k} = \\displaystyle\\frac{n!}{k!(n-k)!}\\]\n\nsaid ``n choose k’’\nThis choose operator keeps track of the number of permutations in a certain combination\nnote \\(0! = 1\\) (to avoid dividing by zero)\n\n\n\n\n\nfrom The Simpsons"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#binomial-distribution",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#binomial-distribution",
    "title": "08: Binomial Distribution",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\n\n\n\n\nBinomial Distribution\n\n\n\n\\[P(x = k) = \\binom{n}{k} p^{k}(1-p)^{n-k}\\]\n\n\\(0 \\leq k \\leq n\\), where \\(n\\) and \\(k\\) are whole numbers\n\\(0 \\leq p \\leq 1\\)"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#example-squirtle",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#example-squirtle",
    "title": "08: Binomial Distribution",
    "section": "Example: Squirtle",
    "text": "Example: Squirtle\n\n\n\n\n\n\n\n\nBinomial Distribution\n\n\n\n\\[P(x = k) = \\binom{n}{k} p^{k}(1-p)^{n-k}\\]\n\n\\(0 \\leq k \\leq n\\), where \\(n\\) and \\(k\\) are whole numbers\n\\(0 \\leq p \\leq 1\\)\n\n\n\n\nHistorically, Squirtle defeats Charizard 32% of the time. If there are 5 battles, what is the probability that Squirtle wins exactly 2 times?"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#example-charizard",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#example-charizard",
    "title": "08: Binomial Distribution",
    "section": "Example: Charizard",
    "text": "Example: Charizard\n\n\n\n\n\nCharizard\n\n\n\nHistorically, Charizard defeats Squirtle 68% of the time. If there are 5 battles, what is the probability that Charizard wins exactly 3 times?"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#symmetry",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#symmetry",
    "title": "08: Binomial Distribution",
    "section": "Symmetry",
    "text": "Symmetry\n\nPropertySquirtleCharizardR code\n\n\nThe previous two examples had the same answer, which is true due to a symmetry property in the choose operator:\n\\[\\binom{n}{k} = \\binom{n}{n-k}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk <- 0:5\npk <- dbinom(k, 5, 0.32)\nk_bool <- k == 2\ndf <- data.frame(k, pk, k_bool)\n\ndf |>\n  ggplot(aes(x = k, y = pk, \n             color = k_bool, fill = k_bool)) +\n  geom_bar(stat = \"identity\") +\n  geom_label(aes(x = k, y = pk, \n                 label = round(pk, 4)),\n             color = \"black\", fill = \"white\") +\n  labs(title = \"2 Squirtle Wins\",\n       subtitle = \"n = 5, k = 2, p = 0.32, P(k = 2) = 0.3220\",\n       caption = \"Math 32\",\n       x = \"wins\",\n       y = \"probability\") +\n  scale_color_manual(values = c(\"black\", \"#ca7721\")) +\n  scale_fill_manual(values = c(\"gray70\", \"#297383\")) +\n  theme(\n    legend.position = \"none\",\n    panel.background = element_blank()\n  )\n\n\n\n# plotly::ggplotly(ex2_plot)\n\n\nk <- 0:5\npk <- dbinom(k, 5, 0.68)\nk_bool <- k == 3\ndf <- data.frame(k, pk, k_bool)\n\ndf |>\n  ggplot(aes(x = k, y = pk, \n             color = k_bool, fill = k_bool)) +\n  geom_bar(stat = \"identity\") +\n  geom_label(aes(x = k, y = pk, \n                 label = round(pk, 4)),\n             color = \"black\", fill = \"white\") +\n  labs(title = \"3 Charizard Wins\",\n       subtitle = \"n = 5, k = 3, p = 0.68, P(k = 3) = 0.3220\",\n       caption = \"Math 32\",\n       x = \"wins\",\n       y = \"probability\") +\n  scale_color_manual(values = c(\"black\", \"#de5138\")) +\n  scale_fill_manual(values = c(\"gray70\", \"#e53800\")) +\n  theme(\n    legend.position = \"none\",\n    panel.background = element_blank()\n  )\n\n\n\n# plotly::ggplotly(ex2_plot)\n\n\n\n\n\n\n\n\n\n\nHow do we pick between \\(p\\) and \\(1-p\\)?\n\n\n\nAt first, it does not matter how you define the binomial setting for what corresponds to \\(p\\) and what corresponds to \\(1-p\\), but you need to be consistent in the rest of the task for how you defined your variables and use the value(s) for \\(k\\)."
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#parameters",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#parameters",
    "title": "08: Binomial Distribution",
    "section": "Parameters",
    "text": "Parameters\nThe notation \\(X \\sim Ber(p)\\) is read as “random variable \\(X\\) has a Bernoulli distribution with parameter \\(p\\)”. Compute the expected value and variance for a Bernoulli trial."
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#parameters-1",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#parameters-1",
    "title": "08: Binomial Distribution",
    "section": "Parameters",
    "text": "Parameters\nThe notation \\(X \\sim Bin(n,p)\\) is read as ``random variable \\(X\\) has a binomial distribution with parameters \\(n\\) and \\(p\\)’’. Compute the expected value and variance for a binomial distribution.\nWe are assuming that the \\(n\\) trials are from each other, where independence in probability means that\n\\[P\\left(  \\{X_{i}\\}_{i=1}^{n} \\right) = \\displaystyle\\prod_{i=1}^{n} P(X_{i})\\]\nIn other words, we are sampling the Bernoulli trial \\(n\\) times with replacement, so we can simply multiply the results from the previous example by \\(n\\).\n\\[\\begin{array}{|c|c|c|}\n\\hline\n\\textbf{mean}               & \\mu & np \\\\ \\hline\n\\textbf{variance}           & \\sigma^{2} & np(1-p) \\\\ \\hline\n\\textbf{standard deviation} & \\sigma & \\sqrt{np(1-p)} \\\\ \\hline\n\\end{array}\\]"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#looking-ahead",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#looking-ahead",
    "title": "08: Binomial Distribution",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 23:\n\nWHW4\nLHW4\nDemographics Part 2 (survey)\n\nExam 1 will be on Tues., Sept. 27\n\nmore information in weekly announcements\n\n\n\n\nsource"
  },
  {
    "objectID": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#probability-mass-function",
    "href": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#probability-mass-function",
    "title": "9: Cumulative Computation",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\nLast time, we developed the probability mass function for the binomial distribution. The probability of choosing \\(k\\) observations among a sample size of \\(n\\), each observation with prior probability \\(p\\), is given by\n\\[P(k) = \\binom{n}{k}p^{k}(1-p)^{n-k}\\]\nNote the usual properties of probability:\n\neach probability is between zero and one (inclusively)\n\n\\[0 \\leq P(k) \\leq 1 \\quad\\text{for each } k\\]\n\nall probabilities add up to 100%\n\n\\[1 = \\displaystyle\\sum_{k = 0}^{n} \\binom{n}{k}p^{k}(1-p)^{n-k}\\]"
  },
  {
    "objectID": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#from-one-to-many",
    "href": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#from-one-to-many",
    "title": "9: Cumulative Computation",
    "section": "From One to Many",
    "text": "From One to Many\n\nExactlyAt MostExactlyMore Than\n\n\nThere are 4 parking spaces in front of the boba place. Suppose that each parking space tends to be occupied about 57 percent of the time. What is the probability that exactly 3 of the parking spaces are open?\n\n\n\nboba!\n\n\n\n\nThere are 4 parking spaces in front of the boba place. Suppose that each parking space tends to be occupied about 57 percent of the time. What is the probability that at most 2 of the parking spaces are open?\n\n\n\nboba!\n\n\n\n\nThere are 32 parking spaces in a row in a UC Merced parking lot. Suppose that each parking space tends to be occupied about 81 percent of the time. What is the probability that exactly 4 of the parking spaces are open?\n\n\n\nparking\n\n\n\n\nThere are 32 parking spaces in a row in a UC Merced parking lot. Suppose that each parking space tends to be occupied about 81 percent of the time. What is the probability that more than 5 of the parking spaces are open?\n\n\n\nparking"
  },
  {
    "objectID": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#leveraging-complements",
    "href": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#leveraging-complements",
    "title": "9: Cumulative Computation",
    "section": "Leveraging Complements",
    "text": "Leveraging Complements\nThere are 32 parking spaces in a row in a UC Merced parking lot. Suppose that each parking space tends to be occupied about 97 percent of the time. What is the probability that at least one of the parking spaces is open?\n\n\n\nparking"
  },
  {
    "objectID": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#looking-ahead",
    "href": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#looking-ahead",
    "title": "9: Cumulative Computation",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 23:\n\nWHW4\nLHW4\nDemographics Part 2 (survey)\n\nExam 1 will be on Tues., Sept. 27\n\nmore information in weekly announcements\n\n\n\n\n\n\nsome found sign in Sausalito"
  },
  {
    "objectID": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#infinite-support",
    "href": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#infinite-support",
    "title": "10: Geometric Distribution",
    "section": "Infinite Support",
    "text": "Infinite Support\n\n\nHere let us assume an endless box of chocolates with random selection with replacement of\nWrite out some of the sample space. Let \\(F\\) be the event of choosing a favorite chocolate, so \\(F^{c}\\) is the event of choosing an average chocolate, \\(P(F) = p\\) and \\(P(F^{c}) = 1-p\\). Let \\(k\\) be the amount of chocolates chosen reaching a favorite chocolate\n\n\n\n\nForest Gump"
  },
  {
    "objectID": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#probability-mass-function",
    "href": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#probability-mass-function",
    "title": "10: Geometric Distribution",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\n\n\n\n\n\n\nGeometric Distribution Probability Mass Function\n\n\n\nA geometric distribution is a discrete probability distribution with\n\nprobability \\(p\\) for “success”\nprobability \\(1-p\\) for “failure”\nfor \\(k = 0, 1, 2, 3, ...\\)\n“success” on \\((k+1)^{\\text{th}}\\) trial\n\nThe probability mass function (PMF) is\n\\[f(X = k) = (1-p)^{k}p\\]\nThe cumulative mass function (CMF) is\n\\[F(X \\leq k) = 1 - (1-p)^{k+1}\\]"
  },
  {
    "objectID": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#cumulative-calculations",
    "href": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#cumulative-calculations",
    "title": "10: Geometric Distribution",
    "section": "Cumulative Calculations",
    "text": "Cumulative Calculations\n\n\n\n\n\nTed Mosby\n\n\n\nTed Mosby is setting up his weekend plans. Barney convinced him to try out an app called Tinder. “Ted Mosby, Architect” has a 2/3 chance of setting up a date among those he “swipes right”. Suppose that Ted stops using Tinder this session once he sets up a date.\n\nCompute the probability that Ted needs exactly 4 ``swipes right’’ to set up a date.\nCompute the probability that Ted needs at most 4 ``swipes right’’ to set up a date."
  },
  {
    "objectID": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#complementary-cumulative-mass-function",
    "href": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#complementary-cumulative-mass-function",
    "title": "10: Geometric Distribution",
    "section": "Complementary Cumulative Mass Function",
    "text": "Complementary Cumulative Mass Function\n\n\n\n\n\n\nComplementary Cumulative Mass Function\n\n\n\nWhen modeling with a geometric distribution—i.e. \\(X \\sim Geo(p)\\)—the probability that “success” takes more than \\(k\\) trials is\n\\[P(X > k) = (1-p)^{k+1}\\]\n\n\n\nMore ThanConditional ProbabilityConditional Probability 2\n\n\n\n\n\n\n\nRebecca Bunch\n\n\n\nRebecca Bunch follows Josh Chan, whom she briefly dated as a teenager, by moving to West Covina, California. Suppose that it may take a while for Rebecca and Josh to reunite and there is a 29 percent chance of them meeting during any particular week. What is the probability that it will take Rebecca more than 3 weeks to reunite with Josh?\n\n\n\n\n\n\n\n\n\nRebecca Bunch\n\n\n\nWhat is the probability that it will take Rebecca more than 9 weeks to reunite with Josh given that she has already spent more than 5 weeks in West Covina?\n\n\n\n\n\n\n\n\n\nRebecca Bunch\n\n\n\nStill holding on to that 29 percent chance of reuniting with Josh, compute the probability that it will take Rebecca more than 36 weeks to reunite with Josh given that she has already spent more than 32 weeks in West Covina."
  },
  {
    "objectID": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#memoryless-property",
    "href": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#memoryless-property",
    "title": "10: Geometric Distribution",
    "section": "Memoryless Property",
    "text": "Memoryless Property\nThe previous two examples demonstrated the memoryless property.\n\n\n\n\n\n\nMemoryless Property\n\n\n\nThe geometric distribution is the only discrete probability distribution that has the memoryless property:\n\\[P(X \\geq a + b | X \\geq b) = P(X \\geq a)\\]"
  },
  {
    "objectID": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#looking-ahead",
    "href": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#looking-ahead",
    "title": "10: Geometric Distribution",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 7:\n\nWHW5\nLHW5\nMid-Semester Survey\n\nExam 1 will be on Tues., Nov. 1"
  },
  {
    "objectID": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#continuous-variables",
    "href": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#continuous-variables",
    "title": "11: Continuous Distributions",
    "section": "Continuous Variables",
    "text": "Continuous Variables\n\n\n\nImage Credit: G2 Learing Hub\n\n\n\nA discrete variable is “countable” (has values that can be in a list)\nA continuous variable has values that cannot be written as a list (“uncountable”)"
  },
  {
    "objectID": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#uniform-distribution",
    "href": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#uniform-distribution",
    "title": "11: Continuous Distributions",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\nYou arrive at a bus stop at 10 o’clock, knowing that the bus will arrive at some time uniformly distributed between 10:00 and 10:30.\n\nNormalizationFewerMoreConditional\n\n\n\n\nWe build a probability density function (PDF) by ensuring that the area under the curve equals 100 percent (i.e. one square unit).\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait fewer than 12 minutes?\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait longer than 10 minutes?\n\n\n\n\n\n\n\n\nIf at 10:15 the bus has not yet arrived, what is the probability that you will have to wait at least an additional 10 minutes?"
  },
  {
    "objectID": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#linear-distribution",
    "href": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#linear-distribution",
    "title": "11: Continuous Distributions",
    "section": "Linear Distribution",
    "text": "Linear Distribution\n\nYou arrive at a bus stop at 10 o’clock, knowing that the bus will arrive at some time linearly distributed between 10:00 and 10:30. The probability density function (PDF) is\n\\[f(x) = \\begin{cases} -\\displaystyle\\frac{1}{450}x + \\displaystyle\\frac{1}{15} & 0 \\leq x \\leq 30 \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\nPDFFewerFewerBetween\n\n\n\n\nProbability Density Function (PDF)\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait fewer than 7 minutes?\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait fewer than 11 minutes?\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait between 7 and 11 minutes?"
  },
  {
    "objectID": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#cumulative-density-function",
    "href": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#cumulative-density-function",
    "title": "11: Continuous Distributions",
    "section": "Cumulative Density Function",
    "text": "Cumulative Density Function\n\nThere are no nonzero probabilities to the left. The CDF “starts with zero” probability. Here, \\(F(0) = 0\\)\nSince all probabilities add up to 100%, the CDF ends at one”. Here, \\(F(30) = 1\\)\n\n\n\n\nCDF"
  },
  {
    "objectID": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#looking-ahead",
    "href": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#looking-ahead",
    "title": "11: Continuous Distributions",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 7:\n\nWHW5\nLHW5\nMid-Semester Survey\n\nExam 2 will be on Tues., Nov. 1"
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#inequalities",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#inequalities",
    "title": "12: Exponential Distribution",
    "section": "Inequalities",
    "text": "Inequalities\n\n\n\n\n\n\nDiscrete Probability Distributions\n\n\n\nThe inclusive versus exclusive variation in inequalities matter in discrete probability distributions. With a random variable \\(X\\) defined over a support of \\(k = \\{0, 1, 2, 3, ... \\}\\) number of trials,\n\\[P(X < k) = \\displaystyle\\sum_{i=0}^{k-1} P(i), \\quad P(X \\leq k) = \\displaystyle\\sum_{i=0}^{k} P(i)\\]\n\\[P(X < 32) = P(k = 0) + P(k = 1) + \\cdots + P(k = 31)\\]\n\\[P(X \\leq 32) = P(k = 0) + P(k = 1) + \\cdots + P(k = 31) + P(k = 32)\\]\n\n\n\n\n\n\n\n\nContinuous Probability Distributions\n\n\n\nThe inclusive versus exclusive variation in inequalities do not matter with continuous probability distributions.\nClaim: For a random variable \\(X\\) with probability density function \\(f\\),\n\\[P(X < b) = P(X \\leq b)\\]"
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#motivation",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#motivation",
    "title": "12: Exponential Distribution",
    "section": "Motivation",
    "text": "Motivation\n\n\nThere are many situations that are better modeled with an open set \\([a, \\infty)\\) of support, so we can look at the mother function\n\\[f(x) = e^{-x}, \\quad a < x\\]\nwhich has a horizontal asymptote. Next, a rate parameter \\(\\lambda\\) (“lambda”) gives flexibility in models (i.e. the number we plug in for \\(\\lambda\\) depends on the word problem).\n\\[f(x) = e^{-\\lambda x}, \\quad a < x\\]"
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#normalization",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#normalization",
    "title": "12: Exponential Distribution",
    "section": "Normalization",
    "text": "Normalization\nWithout much loss of generality, we can shift to a support of \\([0, \\infty)\\). Our next goal is to rescale the function values so that the area under the curve is equal to 100 percent.\n\nFind the value of \\(k\\) so that \\(f\\) is a probability density function."
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#probability-density-function",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#probability-density-function",
    "title": "12: Exponential Distribution",
    "section": "Probability Density Function",
    "text": "Probability Density Function\n\n\n\n\n\n\nProbability Density Function\n\n\n\nHaving found the value of the scalar \\(k\\), our probability density function (PDF) is\n\\[\\text{PDF: } f(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x > 0 \\\\ 0, & \\text{otherwise} \\end{cases}\\]"
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#cumulative-distribution-function",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#cumulative-distribution-function",
    "title": "12: Exponential Distribution",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function\n\n\n\n\n\n\nCumulative Distribution Function\n\n\n\nTo continue to think of probabilities as area under a curve, we derive the cumulative distribution function (CDF) as the integral of the probability density function\n\\[F(x) = \\displaystyle\\int_{-\\infty}^{x} \\! f(t) \\, dt = \\displaystyle\\int_{0}^{x} \\! \\lambda e^{-\\lambda t} \\, dt = 1 - e^{-\\lambda x}, \\quad x > 0\\]\nThat is, the CDF of an exponential distribution is\n\\[F(x) = \\begin{cases} 1 - e^{-\\lambda x}, & x > 0 \\\\ 0, & x < 0 \\end{cases}\\]\n\\[~\\]\nThe properties of probability include\n\nWe start with zero probability\n\\[\\displaystyle\\lim_{x \\to -\\infty} F(x) = 0\\]\nWe end with all probability\n\\[\\displaystyle\\lim_{x \\to \\infty} F(x) = 1\\]"
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#conventions",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#conventions",
    "title": "12: Exponential Distribution",
    "section": "Conventions",
    "text": "Conventions\n\n\n\n\n\n\nConventions\n\n\n\n\nThe models are related with \\(\\lambda = \\displaystyle\\frac{1}{\\beta}\\). Here in Math 32, we will use the model with the rate parameter \\(\\lambda\\) (lambda) to match the convention used by the R programming language"
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#sample-statistics",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#sample-statistics",
    "title": "12: Exponential Distribution",
    "section": "Sample Statistics",
    "text": "Sample Statistics\n\nMeanVarianceMedian?Median\n\n\n\n\n\n\n\n\nMean\n\n\n\nRecall that the mean and the expected value are synonymous.\n\\[\\mu = \\text{E}[{\\color{blue}X}] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! {\\color{blue}x} \\cdot f(x) \\, dx = \\lambda\\displaystyle\\int_{0}^{\\infty} \\! {\\color{blue}x} \\cdot e^{-\\lambda x}\\, dx = \\displaystyle\\frac{1}{\\lambda}\\]\nWe have shown that the expected value for \\(X \\sim Exp(\\lambda)\\) is \\(\\text{E}[X] = \\displaystyle\\frac{1}{\\lambda}\\).\n\n\n\n\n\n\n\n\n\n\nVariance\n\n\n\nFurther analogues to the formulas used for discrete probability distributions include the the second moment\n\\[\\text{E}[{\\color{blue}X^{2}}] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! {\\color{blue}x^{2}} \\cdot f(x) \\, dx = \\lambda\\displaystyle\\int_{0}^{\\infty} \\! {\\color{blue}x^{2}} \\cdot e^{-\\lambda x}\\, dx = \\displaystyle\\frac{2}{\\lambda^{2}}\\]\nand it follows that the variance for an exponential distribution with rate parameter \\(\\lambda\\) is\n\\[\\sigma^{2} = \\text{Var}(X) = \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2} = \\displaystyle\\frac{2}{\\lambda^{2}} - \\left(\\displaystyle\\frac{1}{\\lambda}\\right)^{2} = \\displaystyle\\frac{1}{\\lambda^{2}}\\]\nAs usual, the standard deviation is the square root of the variance.\n\\[\\sigma = \\sqrt{ \\displaystyle\\frac{1}{\\lambda^{2}} } = \\displaystyle\\frac{1}{\\lambda}\\]\n\n\n\n\n\n\n\n\n\n\nMedian\n\n\n\nBack in discrete data, if we had an ordered list of data,\n\\[\\{ x_{(1)}, x_{(2)}, x_{(3)}, ...\\}\\]\nwe said that the median is the value in the ``middle’’ of the list.\nQuery: How do you think we define a median here in the setting of continuous distributions?\n\n\n\n\n\n\n\n\n\n\nMedian\n\n\n\nBack in discrete data, if we had an ordered list of data,\n\\[\\{ x_{(1)}, x_{(2)}, x_{(3)}, ...\\}\\]\nwe said that the median is the value in the ``middle’’ of the list."
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#waiting-times",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#waiting-times",
    "title": "12: Exponential Distribution",
    "section": "Waiting Times",
    "text": "Waiting Times\nLet us now return to the notion of waiting times. Suppose that a friend of yours is going to pick you up for a carpool, and you estimate that he tends to arrive with a mean time of 30 minutes. Assume an exponential distribution.\n\nNormalizationLessMore\n\n\n\n\n\n\n\n\nCareful!\n\n\n\n\\[\\text{Since } \\mu = \\displaystyle\\frac{1}{\\lambda} \\quad\\Rightarrow\\quad 30 = \\displaystyle\\frac{1}{\\lambda} \\quad\\Rightarrow\\quad \\lambda = \\displaystyle\\frac{1}{30}\\]\n\n\n\n\nCompute the probability that your friend will arrive in less than 25 minutes.\n\n\n\n\n\nCompute the probability that your friend will take more than 40 minutes to arrive."
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#memoryless-property",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#memoryless-property",
    "title": "12: Exponential Distribution",
    "section": "Memoryless Property",
    "text": "Memoryless Property\n\nExampleProof\n\n\nYou inherit an exquisite cabin in the woods, but on one condition: you must stay in the cabin overnight on the witch’s sabbath—Halloween. The cabin is notorious for housing the ghost of Cal Kulas, and he strikes sometime after the stroke of midnight with a mean time of 60 minutes.\n\nGiven that you have already waited 32 minutes to see the ghost, what is the probability that you will have to wait at least another 10 minutes?\nGiven that you have already waited 181 minutes to see the ghost, what is the probability that you will have to wait at least another 10 minutes?\n\n\n\nClaim: An exponential distribution has the memoryless property\n\\[P(T > a + b | T > b) = P(T > a)\\]\nProof:\n\\[\\begin{array}{rcl}\n      P(T > a + b | T > b) & = & \\displaystyle\\frac{ P(T > a + b \\text{ and } T > b) }{ P(T > b) } \\\\\n      ~ & = & \\displaystyle\\frac{ P(T > a+b) }{ P(T > b) } \\\\\n      ~ & = & \\displaystyle\\frac{ e^{-\\lambda(a+b)} }{ e^{-\\lambda b} } \\\\\n      ~ & = & e^{-\\lambda a} \\\\\n      ~ & = & P(T > a) \\\\\n    \\end{array}\\]\nClaim: The exponential distribution is the continuous distribution with the memoryless property.\nProof: (See Math 181)"
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#looking-ahead",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#looking-ahead",
    "title": "12: Exponential Distribution",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 7:\n\nWHW5\nLHW5\nMid-Semester Survey\n\nExam 2 will be on Tues., Nov. 1\n\n\n\n\n\nart by @38mo1"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#development",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#development",
    "title": "13: Normal Distribution",
    "section": "Development",
    "text": "Development\n\n\nLet us start with the mother function \\(y = e^{-x^{2}}\\)\n\n\n\nsymmetric graph and function\nhorizontal asymptote\n\n\n\nNext, most of the scientific community accepts placing a horizontal scaling factor of 1/2 in the exponent\n\\[y = e^{-x^{2}/2} = \\text{exp}\\left(-\\displaystyle\\frac{x^{2}}{2}\\right)\\]\nto ensure that later calculations in the standard normal distribution have unit variance: \\(\\sigma^{2} = 1\\)"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#bell-curves",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#bell-curves",
    "title": "13: Normal Distribution",
    "section": "Bell Curves",
    "text": "Bell Curves\n\n\n\n\n\n\nSample Size Consideration\n\n\n\nFor elementary statistics courses, teachers say that using the normal distribution is a good idea when the sample size \\(n > 30\\).\n\n\nDepending on the situation, modeling with a normal distribution may need a transformation (moving the graph horizontally, stretching, etc.).\n\n\n\nbell curves"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#probability-density-function",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#probability-density-function",
    "title": "13: Normal Distribution",
    "section": "Probability Density Function",
    "text": "Probability Density Function\n\n\n\n\n\n\nStandard Normal Distribution\n\n\n\nHistorically, it was a good practice to pick one bell curve for calculations.\n\n\n\nFind the value of \\(k\\) so that \\(f(x) = ke^{-x^{2}/2}\\) is a probability density function."
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#probability-density-function-1",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#probability-density-function-1",
    "title": "13: Normal Distribution",
    "section": "Probability Density Function",
    "text": "Probability Density Function\n\n\n\n\n\n\nProbability Density Function\n\n\n\nAt this point, we have the probability density function (PDF) of the standard normal distribution, denoted by lower-case Greek letter phi:\n\\[\\text{PDF: } \\phi(z) = \\displaystyle\\frac{1}{\\sqrt{2\\pi}}e^{-z^{2}/2}\\]\n\n\n\n\n\nstandard normal distribution"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#parameters",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#parameters",
    "title": "13: Normal Distribution",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\nParameters\n\n\n\nWe will now find the parameters—mean and variance—for the standard normal distribution. We find the expected value with\n\\[\\text{E}[Z] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! z \\cdot \\phi(z) \\, dz = \\displaystyle\\frac{1}{\\sqrt{2\\pi}}\\displaystyle\\int_{-\\infty}^{\\infty} \\! z e^{-z^{2}/2} \\, dz = 0\\]\nThe second moment is\n\\[\\text{E}[Z^{2}] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! z^{2} \\cdot \\phi(z) \\, dz = \\displaystyle\\frac{1}{\\sqrt{2\\pi}}\\displaystyle\\int_{-\\infty}^{\\infty} \\! z^{2} e^{-z^{2}/2} \\, dz = \\displaystyle\\frac{ \\sqrt{2\\pi} }{ \\sqrt{2\\pi} } = 1\\]\nIt follows that the variance is also one unit, so the parameters of the standard normal distribution are \\(\\mu = 0\\) and \\(\\sigma^{2} = 1\\).\n\n\n\n\n\n\n\n\nNotation\n\n\n\nThe notation \\(X \\sim N(\\mu, \\sigma^{2})\\) says that random variable \\(X\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). For example, the standard normal distribution is denoted as \\(Z \\sim N(0,1)\\)\n\n\n\nWhat is the median of the standard normal distribution?\n\n\n\n\nmedian?"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#cumulative-distribution-function",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#cumulative-distribution-function",
    "title": "13: Normal Distribution",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function\n\n\n\n\n\n\nCDF\n\n\n\nThe cumulative distribution function (CDF) for the standard normal distribution is defined as the following integral function and denoted by upper-case Greek letter Phi:\n\\[\\Phi(z) = \\displaystyle\\frac{1}{\\sqrt{2\\pi}} \\displaystyle\\int_{-\\infty}^{z} \\! e^{-t^{2}/2} \\, dt\\]\n\n\n\n\nRecall: for continuous probability distributions, probabilities are the areas under the curve\n\n\n\narea under the curve\n\n\n\nHere in Math 32, instead of doing the integral (with the ``polar trick’’) or referring to a textbook standard normal distribution table, we will perform calculations for the normal distribution in terms of CDF \\(\\Phi\\)"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#empirical-rule",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#empirical-rule",
    "title": "13: Normal Distribution",
    "section": "Empirical Rule",
    "text": "Empirical Rule\nStatistics instructors like to make the following statements to guide intuition about the normal distribution and standard deviations.\n\n1 SD2 SDs3 SDs\n\n\nAbout 67% of data falls within one standard deviation of the mean.\n\n\n\nAbout 95% of data falls within 2 standard deviations of the mean.\n\n\n\nAbout 99% of data falls within 3 standard deviations of the mean."
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#optional-error-function",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#optional-error-function",
    "title": "13: Normal Distribution",
    "section": "(optional) Error Function",
    "text": "(optional) Error Function\n\n\n\n\n\n\n(optional) Error Function\n\n\n\n\n\nSome scientific literature refers to the area under the curve of the probability density function of the \\(X \\sim N(0, 1/2)\\) normal distribution as the \n\\[\\text{erf}(x) = \\displaystyle\\frac{2}{\\sqrt{\\pi}}\\int_{0}^{x} \\! e^{-t^{2}/2} \\, dt\\]\nand we can recover the CDF of the standard normal distribution with\n\\[\\Phi(x) = \\displaystyle\\frac{1}{2}\\left[  1 + \\text{erf}\\left(\\displaystyle\\frac{x}{\\sqrt{2}}\\right) \\right]\\]"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#general-normal-distribution",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#general-normal-distribution",
    "title": "13: Normal Distribution",
    "section": "General Normal Distribution",
    "text": "General Normal Distribution\n\n\n\n\n\n\nGeneral Normal Distribution\n\n\n\nWhen we model applications with \\(X \\sim N(\\mu, \\sigma^{2})\\), by applying the \\(z\\)-score transformation\n\\[z = \\displaystyle\\frac{x - \\mu}{\\sigma}\\]\nthe normal distribution has probability density function\n\\[\\text{PDF: } f(x; \\mu, \\sigma) = \\displaystyle\\frac{1}{\\sigma\\sqrt{2\\pi}} \\cdot e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}\\]\nand cumulative distribution function\n\\[F(x) = \\Phi\\left(\\displaystyle\\frac{x-\\mu}{\\sigma}\\right) = \\displaystyle\\frac{1}{2}\\left[1 + \\text{erf}\\left(\\displaystyle\\frac{x-\\mu}{\\sigma\\sqrt{2}}\\right)\\right]\\]\n\\[~\\]\nR code: pnorm(x, mu, sd)"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#applications-of-the-normal-distribution",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#applications-of-the-normal-distribution",
    "title": "13: Normal Distribution",
    "section": "Applications of the Normal Distribution",
    "text": "Applications of the Normal Distribution\n\nFewerMoreBetweenCharacterize\n\n\nSuppose that the incubation period—that is, the time between being infected with the virus and showing symptoms—for Covid-19 is normally distributed with a mean of 8 days and a standard deviation of 3 days. Find the probability that a randomly selected case demonstrated symptoms in fewer than 7 days.\n\n\n\nGirl Scout Thin Mint cookies have a mean size of 0.25 ounces. Find the probability that one randomly selected cookie has a size of more than 0.27 ounces if the standard deviation is 0.03 ounces. Assume a normal distribution.\n\n\n\nThe cones in the eye detect light. The absorption rate of cones is normally distributed. In particular, the “green” cones have a mean of 535 nanometers and a standard deviation of 65 nanometers. If an incoming ray of light has wavelengths between 550 and 575 nanometers, calculate the percentage of that ray of light that will be absorbed by the green cones.\n\n\n\nSuppose that the number of french fries in the batches at In-n-Out are normally distributed with a mean of 42 french fries and a standard deviation of 3.7 french fries. Your friend tells you that the In-n-Out employee is flirting with you if you end up with a french fry count in the top 5 percent. How should we characterize the top 5 percent of french fries?"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#looking-ahead",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#looking-ahead",
    "title": "13: Normal Distribution",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 14:\n\nWHW6\nLHW6\nInternet Connection Check (survey)\n\nExam 2 will be on Tues., Nov. 1\n\n\n\n\n\n\ntweet source"
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#joint-probability-mass-function",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#joint-probability-mass-function",
    "title": "14: Discrete Joint Distributions",
    "section": "Joint Probability Mass Function",
    "text": "Joint Probability Mass Function\n\n\n\n\n\n\nJoint Probability Mass Function\n\n\n\nThe joint probability mass function (joint PMF) to handle simultaneous calculations of random variables \\(X\\) and \\(Y\\) can be expressed as\n\n\n\\(X = \\{a_{1}, a_{2}, ..., a_{m}\\}\\)\n\\(Y = \\{b_{1}, b_{2}, ..., b_{n}\\}\\)\n\\(p_{ij} = P(X = a_{i}, Y = b_{j})\\)\n\n\n\n\n\n\n\n\n\nProperties\n\n\n\n\nEach probability is between zero and one inclusively\n\\[0 \\leq p_{ij} \\leq 1\\]\nAll probabilities add up to 100 percent\n\\[\\displaystyle\\sum_{i = 1}^{m}\\sum_{j = 1}^{n} p_{ij} = 1\\]\nAside: it is okay if the total is 0.99 or 1.01 (artifact of rounding errors)"
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#setting",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#setting",
    "title": "14: Discrete Joint Distributions",
    "section": "Setting",
    "text": "Setting\nThe setting for the examples in this lecture is The Lantern—our beloved coffee shop.\n\n\n\n\n\nLantern\n\n\n\n\n\\(X\\): number of beverages purchased by a customer\n\\(Y\\): number of snacks purchased by a customer"
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#joint-probability",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#joint-probability",
    "title": "14: Discrete Joint Distributions",
    "section": "Joint Probability",
    "text": "Joint Probability\n\nWhat is the probability that a randomly selected customer purchased one beverage and one snack?"
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#marginal-probability-mass-functions",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#marginal-probability-mass-functions",
    "title": "14: Discrete Joint Distributions",
    "section": "Marginal Probability Mass Functions",
    "text": "Marginal Probability Mass Functions\n\n\n\n\n\n\nMarginal Probability Mass Functions\n\n\n\nThe marginal probability mass functions with respect to \\(X\\) and \\(Y\\) respectively are\n\\[{\\color{blue}p_{X}(a_{i}) = \\displaystyle\\sum_{j = 1}^{n} p(a_{i}, b_{j})}, \\quad {\\color{red}p_{Y}(b_{j}) = \\displaystyle\\sum_{i = 1}^{m} p(a_{i}, b_{j})}\\]\n\n\nIn our example setting, we have the following joint PMF with marginal probabilities:\n\n\n\n\n\n\n\nMarginal Probability Mass Functions\n\n\n\n\n\nMore succinctly, the marginal probability mass function of \\(X\\) is\n\nand the marginal probability mass function of \\(Y\\) is\n\n\n\n\nWhat is the probability that a randomly selected customer purchased one beverage or one snack?"
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#conditional-probability",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#conditional-probability",
    "title": "14: Discrete Joint Distributions",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\nCompute the probability that a randomly selected customer purchases one snack given that the customer purchased zero beverages.\nCompute the probability that a randomly selected customer purchases a beverage given that the customer purchased two snacks."
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#conditional-expectation",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#conditional-expectation",
    "title": "14: Discrete Joint Distributions",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\n\n\n\n\n\n\nConditional Expectation\n\n\n\nThe concept of conditional probability can be extended into the concept of the expected value.\n\\[\\text{E}[{\\color{blue}A}| B = b_{j}] = \\displaystyle\\sum_{i = 1}^{m} {\\color{blue}a_{i}} \\cdot {\\color{red}P(a_{i} | B = b_{j})} = \\displaystyle\\sum_{i = 1}^{m} {\\color{blue}a_{i}} \\cdot {\\color{red}\\displaystyle\\frac{P(A = a_{i}, B = b_{j})}{P(B = b_{j})}}\\]\n\n\n\n\n\n\nWhat is the expected number of snacks purchased given that a customer purchases one beverage?"
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#joint-cumulative-distribution-function",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#joint-cumulative-distribution-function",
    "title": "14: Discrete Joint Distributions",
    "section": "Joint Cumulative Distribution Function",
    "text": "Joint Cumulative Distribution Function\n\n\n\n\n\n\nJoint Cumulative Distribution Function\n\n\n\nAs in the univariate case, the multivariate joint cumulative distribution function (joint CDF) is defined similarly as\n\\[F(a, b) = P(X \\leq a, Y \\leq b)\\]"
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#looking-ahead",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#looking-ahead",
    "title": "14: Discrete Joint Distributions",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 14:\n\nWHW6\nLHW6\nInternet Connection Check (survey)\n\nExam 2 will be on Tues., Nov. 1"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#joint-probability-density-function",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#joint-probability-density-function",
    "title": "15: Continuous Joint Distributions",
    "section": "Joint Probability Density Function",
    "text": "Joint Probability Density Function\n\n\n\n\n\n\nJoint Probability Density Function\n\n\n\nThe joint probability density function \\(f(x,y)\\) to handle simultaneous calculations of random variables \\(X\\) and \\(Y\\) can be expressed as\n\\[P({\\color{blue}a_{1} < X < a_{2}}, {\\color{red}b_{1} < Y < b_{2}}) = \\displaystyle{\\color{blue}\\int_{a_{1}}^{a_{2}}}{\\color{red}\\int_{b_{1}}^{b_{2}}} \\! {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx}\\]\n\n\n\n\n\n\n\n\nProperties\n\n\n\n\nEach probability is between zero and one inclusively\n\\[0 \\leq {\\color{purple}f(x,y)} \\leq 1$ \\text{ for all } {\\color{blue}$x$}, {\\color{red}$y$}\\]\nAll probabilities add up to 100 percent\n\\[{\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}}{\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{purple}f(x,y)} \\, {\\color{blue}dx} \\, {\\color{red}dy} = 1\\]"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#setting",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#setting",
    "title": "15: Continuous Joint Distributions",
    "section": "Setting",
    "text": "Setting\nFor the examples in this lecture session, we will model the queues at In-n-Out with random variables\n\n\n\n\n\nIn-n-Out\n\n\n\n\n\\(X\\): wait time to order food\n\\(Y\\): wait time to receive food\n\nand a function of the form\n\\[{\\color{purple}f(x,y)} = k{\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}},\\]\n\\[{\\color{blue}x > 0}, \\quad {\\color{red}y > 0}\\]"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#normalization",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#normalization",
    "title": "15: Continuous Joint Distributions",
    "section": "Normalization",
    "text": "Normalization\nFind the value of \\(k\\) so that \\(f\\) is a probability density function.\n\\[{\\color{purple}f(x,y)} = k{\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}}, \\quad {\\color{blue}x > 0}, \\quad {\\color{red}y > 0}\\]"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#joint-probability",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#joint-probability",
    "title": "15: Continuous Joint Distributions",
    "section": "Joint Probability",
    "text": "Joint Probability\n\\[{\\color{purple}f(x,y)} = \\displaystyle\\frac{1}{25}{\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}}, \\quad {\\color{blue}x > 0}, \\quad {\\color{red}y > 0}\\]\nCompute the probability that you will take {between 1 and 2 minutes to order} and wait {between 3 and 4 minutes to receive} your food."
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#joint-cumulative-distribution-function",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#joint-cumulative-distribution-function",
    "title": "15: Continuous Joint Distributions",
    "section": "Joint Cumulative Distribution Function",
    "text": "Joint Cumulative Distribution Function\n\n\n\n\n\n\nJoint Cumulative Distribution Function\n\n\n\nIn general, we handle the probability calculations with the joint cumulative distribution function \\(F(a,b)\\)\n% joint cumulative distribution function\n\\[{\\color{purple}F(a,b)} = P({\\color{blue}X \\leq a}, {\\color{red}Y \\leq b}) = {\\color{blue}\\displaystyle\\int_{-\\infty}^{a}}{\\color{red}\\displaystyle\\int_{-\\infty}^{b}} \\! {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx} \\]\n\n\n\n\n\n\n\n\nProperties\n\n\n\nWe can verify that the joint CDF starts at zero\n\\[{\\color{purple}F(0,0)} = 0\\]\nand that the joint CDF collects all probabilities\n\\[\\displaystyle\\lim_{a \\to \\infty, b \\to \\infty} F(a,b) = 1\\]\nIf need be, we can recover the joint PDF from the joint CDF as the mixed second-order partial derivatives\n\\[{\\color{purple}f(x,y)} = \\displaystyle\\frac{{\\color{purple}\\partial^{2}}}{{\\color{blue}\\partial x} {\\color{red}\\partial y}} {\\color{purple}F(x,y)}\\]\n\n\nWhat is the joint CDF for the In-n-Out setting?"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#marginal-probabilities",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#marginal-probabilities",
    "title": "15: Continuous Joint Distributions",
    "section": "Marginal Probabilities",
    "text": "Marginal Probabilities\n\n\n\n\n\n\nMarginal Cumulative Distribution Functions\n\n\n\nThe marginal cumulative distribution functions can be computed as\n% marginal CDF\n\\[\\begin{array}{rcl}\n  {\\color{blue}F_{X}(a)} & = & \\displaystyle\\lim_{{\\color{red}b \\to \\infty}} {\\color{purple}F(a,b)} \\\\\n  {\\color{red}F_{Y}(b)} & = & \\displaystyle\\lim_{{\\color{blue}a \\to \\infty}} {\\color{purple}F(a,b)} \\\\\n\\end{array}\\]\n\n\n\n\n\n\n\n\nProperties\n\n\n\nIntuition: the marginal CDF is seeking to analyze the probabilities in just one variable regardless of the other variables, so ``eliminate’’ the other varibles by taking their limits to infinity.\nWe can verify that the marginal CDFs start at zero\n\\[{\\color{blue}F_{X}(0)} = 0 \\text{ and } {\\color{red}F_{Y}(0)} = 0\\]\nand that the marginal CDFs collect all probabilities\n\\[\\displaystyle\\lim_{{\\color{blue}a \\to \\infty}} {\\color{blue}F_{X}(a)} = 1 \\text{ and } \\displaystyle\\lim_{{\\color{red}b \\to \\infty}} {\\color{red}F_{Y}(b)} = 1\\]\n\n\nWhat are the marginal CDFs for the In-n-Out setting?"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#marginal-probabilities-1",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#marginal-probabilities-1",
    "title": "15: Continuous Joint Distributions",
    "section": "Marginal Probabilities",
    "text": "Marginal Probabilities\n\n\n\n\n\n\nMarginal Cumulative Distribution Functions\n\n\n\nThe marginal probability density functions can be computed as\n\\[\\begin{array}{rcl}\n  {\\color{blue}f_{X}(x)} & = & {\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\\\\n  {\\color{red}f_{Y}(y)} & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{purple}f(x,y)} \\, {\\color{blue}dx} \\\\\n\\end{array}\\]\n\n\n\n\n\n\n\n\nIntuition\n\n\n\nIntuition: the marginal PDF is seeking to analyze the probabilities in just one variable regardless of the other variables, so ``integrate out’’ the other variables.\nAlternatively,\n\\[{\\color{blue}f_{X}(x) = \\displaystyle\\frac{d}{dx} F_{X}(x)} \\text{ and } {\\color{red}f_{Y}(y) = \\displaystyle\\frac{d}{dy} F_{Y}(y)} \\]\n\n\nWhat are the marginal PDFs for the In-n-Out setting?"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#marginal-expectation",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#marginal-expectation",
    "title": "15: Continuous Joint Distributions",
    "section": "Marginal Expectation",
    "text": "Marginal Expectation\n\n\n\n\n\nIn-N-Out\n\n\n\n\nWhat is the expected wait time to order food?\nWhat is the expected wait time to receive food?"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#independence",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#independence",
    "title": "15: Continuous Joint Distributions",
    "section": "Independence",
    "text": "Independence\n\n\n\n\n\n\nIndependence\n\n\n\nRecall that two events \\(A\\) and \\(B\\) are independent if\n\\[{\\color{purple}P(AB)} = {\\color{blue}P(A)} \\cdot {\\color{red}P(B)}\\]\n\n\nHere, the variables \\(X\\) and \\(Y\\) in the In-n-Out example were independent, which can be easily verified by noting that the integrals were separable.\n\\[\\begin{array}{rcl}\n  {\\color{purple}f(x,y)} & = & {\\color{blue}f_{X}(x)} \\cdot {\\color{red}f_{Y}(y)} \\\\\n  {\\color{red}\\displaystyle\\frac{1}{25}} {\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}} & = & {\\color{blue}xe^{-x}} \\cdot {\\color{red}\\displaystyle\\frac{y}{25}e^{-y/5}} \\\\\n\\end{array}\\]\nand\n\\[\\begin{array}{rcl}\n      {\\color{purple}F(a,b)} & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{a}} {\\color{red}\\displaystyle\\int_{-\\infty}^{b}} \\! {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx} \\\\\n      ~ & = & {\\color{red}\\displaystyle\\frac{1}{25}} {\\color{blue}\\displaystyle\\int_{0}^{a}} {\\color{red}\\displaystyle\\int_{0}^{b}} \\! {\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}} \\, {\\color{red}dy} \\, {\\color{blue}dx}\\\\\n      ~ & = & {\\color{red}\\displaystyle\\frac{1}{25}} {\\color{blue}\\left(\\displaystyle\\int_{0}^{a} xe^{-x} \\, dx \\right)} {\\color{red}\\left(\\displaystyle\\int_{0}^{b} \\! ye^{-y/5} \\, dy \\right)} \\\\\n      \\end{array}\\]\n\n\n\n\n\n\nDependence\n\n\n\nUpcoming lectures: dependent variables"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#looking-ahead",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#looking-ahead",
    "title": "15: Continuous Joint Distributions",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 21:\n\nWHW7\nLHW7\n\nExam 2 will be on Tues., Nov. 1\n\n\n\n\n\n\ntweet source"
  },
  {
    "objectID": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#linear-operators",
    "href": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#linear-operators",
    "title": "16: Linear Operators",
    "section": "Linear Operators",
    "text": "Linear Operators\n\n\n\n\n\n\nLinear Operators\n\n\n\nWe say that \\(L\\) is a linear operator if\n\\[\\begin{array}{rcl}\n  L(a{\\color{blue}f(x)}) & = & aL({\\color{blue}f(x)}) \\\\\n  L({\\color{blue}f(x)} + {\\color{red}g(x)}) & = & L({\\color{blue}f(x)}) + L({\\color{red}g(x)}) \\\\\n\\end{array}\\]\n\n\n\n\n\n\n\n\nLinear Operators\n\n\n\nLoosely translated, \\(L\\) is a linear operator if\n\nwe can factor out a scalar multiple\nwe can split the operator across a sum or difference"
  },
  {
    "objectID": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#calculus-review",
    "href": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#calculus-review",
    "title": "16: Linear Operators",
    "section": "Calculus Review",
    "text": "Calculus Review\n\n\n\nShow that the derivative operator is a linear operator.\n\nShow that the integral operator is a linear operator.\n\n\n\n\n\n\n\n\n\n\n\nClaim: the derivative operator is a linear operator\n\n\n\n\n\nProof\n\\[\\begin{array}{rcl}\n      \\displaystyle\\frac{d}{dx}(a{\\color{blue}f(x)} + b{\\color{red}g(x)}) & = & \\displaystyle\\frac{d}{dx} a{\\color{blue}f(x)} + \\displaystyle\\frac{d}{dx} b{\\color{red}g(x)} \\\\\n      ~ & = &  a\\displaystyle\\frac{d}{dx} {\\color{blue}f(x)} + b\\displaystyle\\frac{d}{dx} {\\color{red}g(x)} \\\\\n    \\end{array}\\]\n\\[\\displaystyle\\frac{d}{dx}(a{\\color{blue}f(x)} + b{\\color{red}g(x)}) = a\\displaystyle\\frac{d}{dx} {\\color{blue}f(x)} + b\\displaystyle\\frac{d}{dx} {\\color{red}g(x)},\\]\nso \\(\\displaystyle\\frac{d}{dx}\\) is a linear operator.\n\n\n\n\n\n\n\n\n\nClaim: the integral operator is a linear operator\n\n\n\n\n\nProof\n\\[\\begin{array}{rcl}\n      \\displaystyle\\int \\! (a{\\color{blue}f(x)} + b{\\color{red}g(x)}) \\, dx & = & \\displaystyle\\int \\! a{\\color{blue}f(x)} \\, dx + \\displaystyle\\int \\! b{\\color{red}g(x)} \\, dx \\\\\n      ~ & = & a\\displaystyle\\int \\! {\\color{blue}f(x)} \\, dx + b\\displaystyle\\int \\! {\\color{red}g(x)} \\, dx \\\\\n    \\end{array}\\]\n\\[\\displaystyle\\int \\! (a{\\color{blue}f(x)} + b{\\color{red}g(x)}) \\, dx = a\\displaystyle\\int \\! {\\color{blue}f(x)} \\, dx + b\\displaystyle\\int \\! {\\color{red}g(x)} \\, dx,\\]\nso \\(\\displaystyle\\int\\) is a linear operator."
  },
  {
    "objectID": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#expected-value",
    "href": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#expected-value",
    "title": "16: Linear Operators",
    "section": "Expected Value",
    "text": "Expected Value\nIs the expectation operator \\(\\text{E}\\) a linear operator?\n\n\n\n\n\n\nClaim: \\(\\text{E}[aX] = a\\text{E}[X]\\)\n\n\n\n\n\nProof\n\\[\\text{E}[aX] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! ax \\cdot f(x) \\, dx = a{\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty} \\! x \\cdot f(x) \\, dx} = a{\\color{blue}\\text{E}[X]}\\]\nWe have shown that we can factor out a scalar multiple across the expectation operator.\n\n\n\n\n\n\n\n\n\nClaim: \\(\\text{E}[X + c] = \\text{E}[X] + c\\)\n\n\n\n\n\nProof\n\\[\\begin{array}{rcl}\n          \\text{E}[X + c] & = & \\displaystyle\\int_{-\\infty}^{\\infty} \\! (x + c) \\cdot f(x) \\, dx \\\\\n          ~ & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty} \\! x \\cdot f(x) \\, dx} + \\displaystyle\\int_{-\\infty}^{\\infty} \\! c \\cdot f(x) \\, dx\\\\\n          ~ & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty} \\! x \\cdot f(x) \\, dx} + c\\displaystyle\\int_{-\\infty}^{\\infty} \\! f(x) \\, dx \\\\\n          ~ & = & {\\color{blue}\\text{E}[X]} + c \\\\\n        \\end{array}\\]\nWe have shown that a horizontal shift of \\(c\\) units in the data also affects the expected value by \\(c\\) units\n\n\n\n\n\n\n\n\n\nClaim: \\(\\text{E}[X + Y] = \\text{E}[X] + \\text{E}[Y]\\)\n\n\n\n\n\nProof\n\\[\\begin{array}{rcl}\n          {\\color{purple}\\text{E}[X + Y]} & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}}{\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{purple}(x + y) \\cdot f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx} \\\\\n          ~ & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}}{\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{blue}x} \\cdot {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx}  + {\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}}{\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{red}y} \\cdot {\\color{purple}f(x,y)} \\, {\\color{blue}dx} \\, {\\color{red}dy} \\\\\n          ~ & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}\\! x \\cdot f_{X}(x) \\, dx}  + {\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty} \\! y \\cdot f_{Y}(y) \\, dy} \\\\\n          ~ & = & {\\color{blue}\\text{E}[X]} + {\\color{red}\\text{E}[Y]} \\\\\n        \\end{array}\\]\nWe have shown that the expected value of a sum \\ is the sum of the expected values.\n\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nCombining the above results, since\n\\[\\text{E}[aX + bY] = a\\text{E}[X] + b\\text{E}[Y]\\]\nwe have shown that the expectation operator \\(\\text{E}\\) is a linear operator.\nAlso,\n\\[\\text{E}[aX + bY + c] = a\\text{E}[X] + b\\text{E}[Y] + c\\]"
  },
  {
    "objectID": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#variance",
    "href": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#variance",
    "title": "16: Linear Operators",
    "section": "Variance",
    "text": "Variance\nIs the variance \\(\\text{Var}(X)\\) function a linear operator?\n\n\n\n\n\n\nClaim: \\(\\text{Var}(aX) = a\\text{Var}(X)\\)\n\n\n\n\n\nCounterpoint:\nRecall the ``practical formula for variance’’\n\\[\\text{Var}(X) = \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2}\\]\nand tracking the scaling factor proceeds as follows\n\\[\\begin{array}{rcl}\n          \\text{Var}(aX) & = & \\text{E}[(aX)^{2}] - \\left(\\text{E}[aX]\\right)^{2} \\\\\n          ~ & = & \\displaystyle\\int_{-\\infty}^{\\infty}\\! (ax)^{2} \\cdot f(x) \\, dx - \\left(a\\text{E}[X]\\right)^{2} \\\\\n          ~ & = & \\displaystyle\\int_{-\\infty}^{\\infty}\\! a^{2}x^{2} \\cdot f(x) \\, dx - a^{2}\\left(\\text{E}[X]\\right)^{2} \\\\\n          ~ & = & a^{2} \\left[ \\displaystyle\\int_{-\\infty}^{\\infty}\\! x^{2} \\cdot f(x) \\, dx - \\left(\\text{E}[X]\\right)^{2} \\right] \\\\\n          ~ & = & a^{2} \\left( \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2} \\right) \\\\\n          ~ & = & a^{2} \\text{Var}(X) \\\\\n        \\end{array}\\]\nWhen factoring out a scalar from the variance function, the factor is squared.\nFurthermore, since \\(\\text{Var}(aX) \\neq a\\text{Var}(X)\\), we have shown that the variance function is not a linear operator.\n\n\n\n\n\n\n\n\n\nClaim: \\(\\text{Var}(X + c) = \\text{Var}(X) + c\\)\n\n\n\n\n\nCounterpoint:\nRecall the ``practical formula for variance’’\n\\[\\begin{array}{rcl}\n          \\text{Var}(X + c) & = & \\text{E}[(X + c)^{2}] - \\left(\\text{E}[X + c]\\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2} + 2cX + c^{2}] - \\left(\\text{E}[X] + c \\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] + \\text{E}[2cX] + \\text{E}[c^{2}] - \\left(\\text{E}[X]\\right)^{2} - 2c\\text{E}[X] - c^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] + 2c\\text{E}[X] + c^{2} - \\left(\\text{E}[X]\\right)^{2} - 2c\\text{E}[X] - c^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2} \\\\\n          ~ & = & \\text{Var}(X) \\\\\n        \\end{array}\\]\nWe have shown that \\(\\text{Var}(X + c) = \\text{Var}(X)\\). That is, variance is not affected by a horizontal shift (phase shift)!\nFurthermore, since \\(\\text{Var}(X + c) \\neq \\text{Var}(X) + c\\), we have shown that the variance function is not a linear operator.\n\n\n\n\n\n\n\n\n\nClaim: \\(\\text{Var}(X + Y) = {\\color{blue}\\text{Var}(X)} + {\\color{red}\\text{Var}(Y)}\\)\n\n\n\n\n\nCounterpoint:\n\\[\\begin{array}{rcl}\n          \\text{Var}(X + Y) & = & \\text{E}[(X + Y)^{2}] - \\left(\\text{E}[X + Y]\\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2} + 2XY + Y^{2}] - \\left(\\text{E}[X] + \\text{E}[Y]\\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] + \\text{E}[2XY] + \\text{E}[Y^{2}] - \\left(\\text{E}[X]\\right)^{2}+ 2\\text{E}[X]\\text{E}[Y] + \\left(\\text{E}[Y]\\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2}  + \\text{E}[Y^{2}] - \\left(\\text{E}[Y]\\right)^{2} + 2\\text{E}[XY] - 2\\text{E}[X]\\text{E}[Y] \\\\\n          ~ & = & {\\color{blue}\\text{Var}(X)} + {\\color{red}\\text{Var}(Y)} + 2\\left( {\\color{purple}\\text{E}[XY] - \\text{E}[X]\\text{E}[Y] } \\right) \\\\\n        \\end{array}\\]\nWe have shown that \\(\\text{Var}(X + Y) \\neq \\text{Var}(X) + \\text{Var}(Y)\\). That is, the variance of the sum is not the sum of the variances (unless …?)\nFurthermore, since \\(\\text{Var}(X + Y) \\neq \\text{Var}(X) + \\text{Var}(Y)\\), we have shown that the variance function is not a linear operator.\n\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nWe have shown that the variance function is not a linear operator.\nNext time: working with\n\\[{\\color{purple}\\text{E}[XY] - \\text{E}[X]\\text{E}[Y]}\\]\nwhich is called the covariance!"
  },
  {
    "objectID": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#looking-ahead",
    "href": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#looking-ahead",
    "title": "16: Linear Operators",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 21:\n\nWHW7\nLHW7\n\nExam 2 will be on Tues., Nov. 1\n\n\n\ntweet source"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#setting",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#setting",
    "title": "17: Covariance and Correlation",
    "section": "Setting",
    "text": "Setting\n\n\nWe will once again visualize the act of ordering food at In-n-Out.\n\n\\(X\\): number of fries orders\n\\(Y\\): number of beef patties ordered\n\n\n\n\njoint PMF\n\n\n\n\n\n\nIn-n-Out"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#independence",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#independence",
    "title": "17: Covariance and Correlation",
    "section": "Independence",
    "text": "Independence\n\nAre \\(X\\) and \\(Y\\) independent?"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#covariance",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#covariance",
    "title": "17: Covariance and Correlation",
    "section": "Covariance",
    "text": "Covariance\n\n\n\n\n\n\nTrue or False? \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)\n\n\n\n\n\nFalse. In general,\n\\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\left( \\text{E}[XY] - \\text{E}[X]\\text{E}[Y] \\right)\\]\n\n\n\n\n\n\n\n\n\nMotivation for Independence\n\n\n\n\n\nAs you probably suspected, \\(\\text{Var}(X + Y)\\) does equal \\(\\text{Var}(X) + \\text{Var}(Y)\\) if \\(X\\) and \\(Y\\) are independent (exercise left to reader).\n\n\n\n\n\n\n\n\n\nCovariance\n\n\n\nWe define the covariance of random variables as\n\\[\\text{Cov}(X,Y) = \\text{E}[XY] - \\text{E}[X]\\text{E}[Y]\\]\n\n\n\n\n\n\n\n\nDerek’s Intuition\n\n\n\n\n\nAs an analogy, the random variables somewhat act like waves in that they can work together and grow or somewhat cancel each other out.\n\n\nImage source: https://www.physics-and-radio-electronics.com/physics/waveinterference.html\n\n\n\n\n\n\n\n\n\n\nThe Pythagorean Theorem of Statistics\n\n\n\n\n\n\n\nImage credit: Bioinformatics professor Dr. David Ardell"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#covariance-2",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#covariance-2",
    "title": "17: Covariance and Correlation",
    "section": "Covariance",
    "text": "Covariance\n\n\nCompute the covariance in the In-n-Out setting\nWhat are the units of the answer in this example?"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#correlation",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#correlation",
    "title": "17: Covariance and Correlation",
    "section": "Correlation",
    "text": "Correlation\n\n\n\n\n\n\nCorrelation\n\n\n\nJust like how the \\(z\\)-score is a standardized and unitless measure, the correlation was designed to be standardized and unitless (i.e. units cancel out).\n\\[r = \\text{Corr}(X,Y) = \\displaystyle\\frac{ \\text{Cov}(X,Y) }{ \\sqrt{ \\text{Var}(X) \\cdot \\text{Var}(Y)} }\\]\n\nIf \\(\\text{Var}(X) = 0\\), the data \\(X\\) are constant, and simply return \\(r = 0\\)\nIf \\(\\text{Var}(Y) = 0\\), the data \\(Y\\) are constant, and simply return \\(r = 0\\)\n\n\n\n\n\nCompute the correlation in the In-n-Out setting"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#interpretation-of-correlation",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#interpretation-of-correlation",
    "title": "17: Covariance and Correlation",
    "section": "Interpretation of Correlation",
    "text": "Interpretation of Correlation\n\n\n\n\n\n\nRanges\n\n\n\n\n\n\n\n\nRanges\n\n\nAside: the infinity-sized expected values might happen in continuous distributions.\n\n\n\n\n\n\n\n\n\nInterpretation of Correlation\n\n\n\nEarly development of the concept of correlation was done by Karl Pearson. Pearson suggested the following interpretations of the correlation (but there is no strict rule for this):\n\n\\(|r| < 0.4\\): virtually uncorrelated\n\\(0.4 \\leq |r| < 0.7\\): slightly correlated\n\\(0.7 \\leq |r| \\leq 1.0\\): strongly correlated"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#continuous-joint-probability-distribution-functions",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#continuous-joint-probability-distribution-functions",
    "title": "17: Covariance and Correlation",
    "section": "Continuous Joint Probability Distribution Functions",
    "text": "Continuous Joint Probability Distribution Functions\n\n\nWe will once again visualize the act of ordering food at In-n-Out.\n\n\\(X\\): number of fries orders\n\\(Y\\): number of beef patties ordered\n\nwith joint PDF\n\\[f(x,y) = \\frac{1}{30}(x + y)e^{-x}e^{-y/5}\\]\n\nAre \\(X\\) and \\(Y\\) independent?\nAre \\(X\\) and \\(Y\\) correlated?\n\n\n\n\n\nIn-n-Out"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#looking-ahead",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#looking-ahead",
    "title": "17: Covariance and Correlation",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 28:\n\nWHW8\nLHW8\n\nExam 2 will be on Tues., Nov. 1\n\nmore info in weekly announcement\n\n\n\n tweet source"
  },
  {
    "objectID": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#linear-conversion",
    "href": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#linear-conversion",
    "title": "18: Change of Variables",
    "section": "Linear Conversion",
    "text": "Linear Conversion\nLet \\(F\\) be the daily high temperature in Fahrenheit in Merced, California, with a mean of 76 degrees and a standard deviation of 15 degrees. Compute those sample statistics in Celsius.\n\n\n\n\n\n\nTemperature Conversion\n\n\n\nWe know that the conversion formula is\n\\[C = \\displaystyle\\frac{5}{9}(F - 32)\\]"
  },
  {
    "objectID": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#range-rule-of-thumb",
    "href": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#range-rule-of-thumb",
    "title": "18: Change of Variables",
    "section": "Range Rule of Thumb",
    "text": "Range Rule of Thumb\n\n\n\n\n\n\nRange Rule of Thumb\n\n\n\nRecall\n\nAbout 67 percent of data falls within one standard deviation of the mean\nAbout 95 percent of data falls within two standard deviations of the mean\n\n\\[\\left( \\mu - 2\\sigma, \\mu + 2\\sigma \\right)\\]\n\n\nWe had computed\n\n\\(\\mu_{F} \\approx 76\\) and \\(\\sigma_{F} \\approx 15\\) degrees Fahrenheit\n\\(\\mu_{C} \\approx 24.4444\\) and \\(\\sigma_{C} \\approx 8.3333\\) degrees Celsius\n\nBuild range-rule-of-thumb intervals for the Merced high temperatures in Fahrenheit and in Celsius."
  },
  {
    "objectID": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#distributions",
    "href": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#distributions",
    "title": "18: Change of Variables",
    "section": "Distributions",
    "text": "Distributions\nDetermine the distribution and density functions for\n\\[Y = \\displaystyle\\frac{5}{9}(X - 32)\\]"
  },
  {
    "objectID": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#change-of-coordinates",
    "href": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#change-of-coordinates",
    "title": "18: Change of Variables",
    "section": "Change of Coordinates",
    "text": "Change of Coordinates\n\n\n\n\n\n\nChange of Coordinates\n\n\n\nLet \\(X\\) be a continuous random variable with distribution function \\(F_{X}\\) and density function \\(f_{X}\\). If we apply a linear transformation\n\\[Y = aX + c\\]\nwhere \\(a >0\\) and \\(c\\) are constants, then\n\\[F_{Y}(y) = F_{X}\\left(\\displaystyle\\frac{y - c}{a}\\right) \\text{ and } f_{Y}(y) = \\displaystyle\\frac{1}{a}f_{X}\\left(\\displaystyle\\frac{y - c}{a}\\right)\\]\n\n\nIf \\(X \\sim Exp(1/2)\\), then what kind of distribution does \\(Y = 32X\\) have?"
  },
  {
    "objectID": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#nonlinear-transformations",
    "href": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#nonlinear-transformations",
    "title": "18: Change of Variables",
    "section": "Nonlinear Transformations",
    "text": "Nonlinear Transformations\n\nConcaveConvex\n\n\nLet \\(X \\sim U\\left(0, \\displaystyle\\frac{\\pi}{2}\\right)\\) and \\(Y = \\sin(X)\\).\nCompare \\(\\text{E}[\\sin X]\\) and \\(\\sin(\\text{E}[X])\\)\n\n\n\nSuppose that a disease outbreak can be modeled where \\(X\\) is the population density of a city and \\(Y\\) is the number of diagnosed cases with\n\\[X \\sim U(0,100), \\quad Y = X^{3.2}\\]\nCompare \\(\\text{E}[X^{3.2}]\\) and \\(\\left(\\text{E}[X]\\right)^{3.2}\\)\n\n\n\n\n\n\n\n\n\n\nJensen’s Inequality\n\n\n\nThe previous two examples were demonstrations of , which states that\n\nIf \\(g\\) is a convex function of random variable \\(X\\), then\n\n\\[g(\\text{E}[X]) \\leq \\text{E}[g(X)]\\]\n\nIf \\(g\\) is a concave function of random variable \\(X\\), then\n\n\\[g(\\text{E}[X]) \\geq \\text{E}[g(X)]\\]\nwhere the equal signs are not included when the function \\(g\\) is strictly convex or strictly concave."
  },
  {
    "objectID": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#looking-ahead",
    "href": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#looking-ahead",
    "title": "18: Change of Variables",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 28:\n\nWHW8\nLHW8\n\nExam 2 will be on Tues., Nov. 1\n\nmore info in weekly announcement\n\nMath 32 discussion sections on Oct. 31 are optional\n\ncanceled for Nov. 2"
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#today-law-of-large-numbers",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#today-law-of-large-numbers",
    "title": "20: Law of Large Numbers",
    "section": "Today: Law of Large Numbers",
    "text": "Today: Law of Large Numbers\nGoal: start to understand error as it relates to sample size\nObjectives:\n\nDistribution of the mean \\(\\bar{X}_{n}\\)\nChebyshev’s Inequality\nLaw of Large Numbers"
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#i.i.d.",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#i.i.d.",
    "title": "20: Law of Large Numbers",
    "section": "i.i.d.",
    "text": "i.i.d.\n\n\n\n\n\n\ni.i.d.\n\n\n\nLet \\(X_{1}, X_{2}, X_{3}, ...\\) be an independent and identically distributed sequence of random variables (denoted ``i.i.d’’)\n\neach \\(X_{i}\\) is independent of each other\neach \\(X_{i}\\) has mean \\(\\mu\\)\neach \\(X_{i}\\) has variance \\(\\sigma^{2}\\)\n\n\n\nLet us now seek the distribution of the mean\n\\[\\bar{X}_{n} = \\displaystyle\\frac{X_{1} + X_{2} + ... + X_{n}}{n}\\]\n\nexpected value\nvariance"
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#distribution-of-mean",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#distribution-of-mean",
    "title": "20: Law of Large Numbers",
    "section": "Distribution of Mean",
    "text": "Distribution of Mean\n\n\n\n\n\n\nDistribution of Mean\n\n\n\nIf \\(\\bar{X}_{n}\\) is the mean of \\(n\\) independent and identical random variables, each with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\), then we can describe the distribution of \\(\\bar{X}_{n}\\) with\n\\[\\text{E}[\\bar{X}_{n}] = \\mu \\quad\\text{and}\\quad \\text{Var}(\\bar{X}_{n}) = \\displaystyle\\frac{\\sigma^{2}}{n}\\]"
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#far-from-the-mean",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#far-from-the-mean",
    "title": "20: Law of Large Numbers",
    "section": "Far from the Mean",
    "text": "Far from the Mean\n\n\n\n\n\n\nFar from the Mean\n\n\n\nIdea: we can get a sense of the probability that, for a particular boundary location \\(a\\), an observation lies outside of the interval\n\\[(\\mu - a, \\mu + a)\\]\n\n\n\n\\(\\mu\\): population average\n\\(a\\): tolerance\n\nClaim: \\(P(|X - \\mu| \\geq a) \\leq \\displaystyle\\frac{\\text{Var}(X)}{a^{2}}\\)"
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#chebyshevs-inequality",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#chebyshevs-inequality",
    "title": "20: Law of Large Numbers",
    "section": "Chebyshev’s Inequality",
    "text": "Chebyshev’s Inequality\n\n\n\n\n\n\nChebyshev’s Inequality\n\n\n\nFor a random variable \\(X\\) and boundary location \\(a\\),\n\\[P(|X - \\mu| \\geq a) \\leq \\displaystyle\\frac{\\text{Var}(X)}{a^{2}}\\]\n\n\nThat is, if we know the variance of a distribution, we can compute an upper bound for the probability of rare events!"
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#law-of-large-numbers",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#law-of-large-numbers",
    "title": "20: Law of Large Numbers",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\nThe Law of Large Numbers basically combines Chebyshev’s Inequality with the earlier work for the distribution of the mean:\n\n\\(\\text{E}[\\bar{X}_{n}] = \\mu \\quad\\text{and}\\quad \\text{Var}(\\bar{X}_{n}) = \\displaystyle\\frac{\\sigma^{2}}{n}\\)\n\\(P(|X - \\mu| \\geq a) \\leq \\displaystyle\\frac{\\text{Var}(X)}{a^{2}}\\)\n\nIdea: What happens when we observe a lot of data?\n\n\n\n\n\n\nLaw of Large Numbers\n\n\n\nTaking the limit as \\(n\\) goes to infinity, we arrive at the Law of Large Numbers:\n\\[\\displaystyle\\lim_{n \\to \\infty} P(|\\bar{X}_{n} - \\mu| \\geq a) \\leq \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{\\sigma^{2}}{a^{2} n} = 0\\]\n\n\nThat is, the probability that the mean of a sample of random variables is ``far’’ from the inherent expected value eventually goes to zero."
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#nerdy-example",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#nerdy-example",
    "title": "20: Law of Large Numbers",
    "section": "Nerdy Example",
    "text": "Nerdy Example\nHow many numbers between zero and one do we have to add up to have a sum that is greater than one?\n\nLet \\(X_{i} \\sim U(0,1)\\) be i.i.d.\nLet \\(Y\\) be the amount of \\(X_{i}\\) added together to get a sum greater than one\nFor a conservative estimate, suppose \\(Y \\sim U(2,6)\\), then\n\n\\[\\text{Var}(Y) = \\displaystyle\\frac{4}{3}\\]\n\nEmpirically (i.e. computer simulation), we saw convergence toward\n\n\\[\\bar{Y}_{n} = e \\approx 2.7183\\]\nHow many trials are needed so that the simulations converge to a mean within 0.01 of the true answer with at least 95 percent probability?"
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#looking-ahead",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#looking-ahead",
    "title": "20: Law of Large Numbers",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\nWHW9\n\nFinal Exam will be on Dec. 8\n\n\ntweet source"
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#today-estimators",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#today-estimators",
    "title": "21: Estimators",
    "section": "Today: Estimators",
    "text": "Today: Estimators\nGoal: Explore generalization from samples to populations\nObjectives: Show the biased or unbiased estimation via\n\nsample mean \\(\\bar{x}\\)\nsample variance \\(s^{2}\\)\nsample standard deviation \\(s\\)"
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#demographics-example",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#demographics-example",
    "title": "21: Estimators",
    "section": "Demographics Example",
    "text": "Demographics Example\nFrom our Demographics Survey data of Math 32 students, suppose that the following is a sample of observations of heights (in inches):\n\\[\\{x_{11} = 72, x_{12} = 61, x_{13} = 60, x_{14} = 75, x_{15} = 69\\}\\]\n\nthen \\(t_{1} = 67.4\\) inches is the sample mean.\n\nSuppose that the following is another sample of heights:\n\\[\\{x_{21} = 66, x_{22} = 78, x_{23} = 78, x_{24} = 77, x_{25} = 64\\}\\]\n\nthen \\(t_{2} = 72.6\\) inches is the sample mean.\n\nSuppose that the following is another sample of heights:\n\\[\\{x_{31} = 61, x_{32} = 59, x_{33} = 70, x_{34} = 61, x_{35} = 65\\}\\]\n\nthen \\(t_{3} = 63.2\\) inches is the sample mean.\n\n\n\n\n\n\n\nSampling Itself is Probabilistic\n\n\n\n\n\nObserve: the sample mean (usually) changes upon a new set of observations\n\n\n\n\nCan we calculate the average height of UC Merced students?\nHow can we calculate the average height of UC Merced students?\n\nThought: what if we take a mean of the sample means?"
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#estimators",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#estimators",
    "title": "21: Estimators",
    "section": "Estimators",
    "text": "Estimators\n\n\n\n\n\n\nEstimators\n\n\n\nLet \\(T\\) be a random variable and \\(f\\) be some calculation\n\\[T = f(x_{1}, x_{2}, x_{3}, ...)\\]\nIf we are trying to estimate a population parameter \\(\\theta\\), we say that \\(T\\) is an unbiased estimator of \\(\\theta\\) if\n\\[\\text{E}[T] = \\theta\\]\n\n\nToday, we will look at situations where \\(f\\) is calculating the\n\nmean\nvariance\nstandard deviation"
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#mean",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#mean",
    "title": "21: Estimators",
    "section": "Mean",
    "text": "Mean\n\nSetupCodeSimulationProof\n\n\nWe will run simulations with \\(X \\sim U(0,1)\\) because we know what the answers should be. The population mean is\n\\[\\mu = \\displaystyle\\frac{a + b}{2} = \\displaystyle\\frac{1}{2}\\]\n\n\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- mean(these_numbers) #record average\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = 1/2, color = \"red\", size = 3) +\n  labs(title = \"Simulation Sample Mean\",\n       subtitle = paste(\"black: sample distribution\\nred: true population mean\\nmean of sample means: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLoosely speaking, since the sampling distribution “lines up” with the population mean, we say that the sample median is an unbiased estimator of the population mean.\n\n\n\\[\\begin{array}{rcl}\n      \\text{E}[\\bar{X}_{n}] & = & \\text{E}\\left( \\displaystyle\\frac{X_{1} + X_{2} + ... + X_{n}}{n} \\right) \\\\\n      ~ & = & \\displaystyle\\frac{1}{n}\\text{E}\\left( X_{1} + X_{2} + ... + X_{n} \\right) \\\\\n      ~ & = & \\displaystyle\\frac{1}{n}\\left(\\text{E}[X_{1}] + \\text{E}[X_{2}] + ... + \\text{E}[X_{n}] \\right) \\\\\n      ~ & = & \\displaystyle\\frac{1}{n}\\left(\\mu + \\mu + ... + \\mu \\right) \\\\\n      ~ & = & \\displaystyle\\frac{1}{n}\\left(n \\mu \\right) \\\\\n    \\end{array}\\]\nTherefore \\(\\text{E}[\\bar{X}_{n}] = \\mu\\)"
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#population-variance",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#population-variance",
    "title": "21: Estimators",
    "section": "Population Variance",
    "text": "Population Variance\n\nSetupCodeSimulation\n\n\nWe will run simulations with \\(X \\sim U(0,1)\\) because we know what the answers should be. The population variance is\n\\[\\sigma^{2} = \\displaystyle\\frac{(b-a)^{2}}{12} = \\displaystyle\\frac{1}{12}\\]\nWe will explore what happens if we apply the population variance formula\n\\[\\sigma^{2} = \\frac{1}{N}\\displaystyle\\sum_{i=1}^{N} (x_{i} - \\mu)^{2}\\]\nto samples.\n\n\n\n# user-defined function\npop_var <- function(x){\n  N <- length(!is.na(x)) #population size\n  mu <- mean(x, na.rm = TRUE) #population mean\n  \n  # return population mean (note use of \"N\")\n  sum( (x - mu)^2 ) / N\n}\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- pop_var(these_numbers) #record population variance\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = 1/12, color = \"red\", size = 3) +\n  labs(title = \"Simulation of Population Variances\",\n       subtitle = paste(\"black: sample distribution\\nred: true population variance\\nmean of population variances: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLoosely speaking, since the sampling distribution tends to underestimate the population variance, we say that the population variance (with \\(N\\)) is a biased estimator of the population variance."
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#bessels-correction",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#bessels-correction",
    "title": "21: Estimators",
    "section": "Bessel’s Correction",
    "text": "Bessel’s Correction\nCan we rescale the process for computing variance so that the operation is an unbiased estimator for the population variance?\nLet \\(X_{i}\\) be a set of \\(n\\) i.i.d. random variables from the same distribution with the same population variance \\(\\sigma^{2}\\). By independence, there is zero covariance.\nWe will compute the value of \\(k\\) so that\n\\[\\text{E}\\left[k \\cdot \\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X}_{n})^{2}}{n}\\right] = \\sigma^{2}\\]\nLemma: \\(\\text{Var}(X_{i} - \\bar{X}_{n}) = \\displaystyle\\frac{n-1}{n} \\cdot \\sigma^{2}\\)\n\n\n\n\n\n\nBessel’s Correction\n\n\n\n\n\nWe have derived the formula for the sample variance\n\\[S_{n}^{2} = \\displaystyle\\frac{1}{n-1}\\displaystyle\\sum_{i=1}^{n}(X_{i} - \\bar{X}_{n})^{2}\\]\nThat is, the “\\(n-1\\)” (Bessel’s correction) is in place so that the sample variance \\(s^{2}\\) is an unbiased estimator of the population variance \\(\\sigma^{2}\\)"
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#sample-variance",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#sample-variance",
    "title": "21: Estimators",
    "section": "Sample Variance",
    "text": "Sample Variance\n\nSetupCodeSimulation\n\n\nWe will run simulations with \\(X \\sim U(0,1)\\) because we know what the answers should be. The population variance is\n\\[\\sigma^{2} = \\displaystyle\\frac{(b-a)^{2}}{12} = \\displaystyle\\frac{1}{12}\\]\nWe will explore what happens if we apply the sample variance formula\n\\[s^{2} = \\frac{1}{n-1}\\displaystyle\\sum_{i=1}^{n} (x_{i} - \\mu)^{2}\\]\nto samples.\n\n\n\n# user-defined function\nsamp_var <- function(x){\n  n  <- length(!is.na(x)) #sample size\n  xbar <- mean(x, na.rm = TRUE) #sample mean\n  \n  # return population mean (note use of \"n-1\")\n  sum( (x - xbar)^2 ) / (n-1)\n}\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- samp_var(these_numbers) #record sample variance\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = 1/12, color = \"red\", size = 3) +\n  labs(title = \"Simulation of Sample Variances\",\n       subtitle = paste(\"black: sample distribution\\nred: true population variance\\nmean of sample variances: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLoosely speaking, since the sampling distribution “lines up” with the population variance, we say that the sample variance (with \\(n-1\\)) is an unbiased estimator of the population variance."
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#sample-standard-deviation",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#sample-standard-deviation",
    "title": "21: Estimators",
    "section": "Sample Standard Deviation",
    "text": "Sample Standard Deviation\n\nSetupCodeSimulationCommentary\n\n\nWe will run simulations with \\(X \\sim U(0,1)\\) because we know what the answers should be. The population standard deviation is\n\\[\\sigma = \\sqrt{\\displaystyle\\frac{(b-a)^{2}}{12}} = \\sqrt{\\displaystyle\\frac{1}{12}}\\]\nWe will explore what happens if we apply the sample variance formula\n\\[s = \\sqrt{\\frac{1}{n-1}\\displaystyle\\sum_{i=1}^{n} (x_{i} - \\mu)^{2}}\\]\nto samples.\n\n\n\n# user-defined function\nsamp_var <- function(x){\n  n  <- length(!is.na(x)) #sample size\n  xbar <- mean(x, na.rm = TRUE) #sample mean\n  \n  # return population mean (note use of \"n-1\")\n  sum( (x - xbar)^2 ) / (n-1)\n}\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- sqrt(samp_var(these_numbers)) #record sample standard deviation\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = sqrt(1/12), color = \"red\", size = 3) +\n  labs(title = \"Simulation of Sample Variances\",\n       subtitle = paste(\"black: sample distribution\\nred: true population variance\\nmean of sample standard deviations: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nLet \\(X_{i}\\) be a set of \\(n\\) i.i.d. random variables from the same distribution with the same population standard deviation \\(\\sigma\\). To avoid trivial situations, assume non-zero variance, so \\(\\sigma \\neq 0\\).\nIf \\(s = \\sqrt{ \\displaystyle\\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X}_{n})^{2}}{n-1} }\\) was an unbiased estimator, then \\(\\text{E}[s] = \\sigma\\)\nHowever, by Jensen’s Inequality, since \\(g(x) = x^{2}\\) is a convex function,\n\\[\\sigma^{2}  = \\text{E}[S_{n}^{2}] > \\left(\\text{E}[S_{n}]\\right)^{2}\\]\nand it follows that \\(\\text{E}[S_{n}] < \\sigma\\). Due to the underestimation, sample standard deviation \\(s\\) is a biased estimator of population standard deviation \\(\\sigma\\).\n\\[~\\]\nHowever, in practice, the discrepancy is usually so small that it is ignored."
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#looking-ahead",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#looking-ahead",
    "title": "21: Estimators",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\nWHW9\nWHW10\n(next LHW assignments will be given after Thanksgiving)\n\nFinal Exam will be on Dec. 8"
  }
]