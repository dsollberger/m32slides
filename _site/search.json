[
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html",
    "title": "Introduction",
    "section": "",
    "text": "library(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#introducting-the-presenter",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#introducting-the-presenter",
    "title": "Introduction",
    "section": "Introducting the Presenter",
    "text": "Introducting the Presenter\n\n\n\nLecturer: Derek Sollberger\n\nI go by “Derek” or “teacher”\n\nBA in Applied Mathematics, UC Berkeley\nMS in Applied Mathematics, CSULB\nMS in Applied Mathematics, UC Merced"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#introducting-the-presenter-1",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#introducting-the-presenter-1",
    "title": "Introduction",
    "section": "Introducting the Presenter",
    "text": "Introducting the Presenter\n\n\n\n\n\nContinuing Lecturer in Applied Mathematics\n10+ years of teaching at UC Merced\nCourses:\n\nBio 18: Data Science\nBio 175: Biostatistics\nBio 184: Python for Bioinformatics\nMath 32: Probability and Statistics"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#current-research-in-pedagogy",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#current-research-in-pedagogy",
    "title": "Introduction",
    "section": "Current Research in Pedagogy",
    "text": "Current Research in Pedagogy\n\n\n\n\n\nactive learning\ncomputer programming\naugmented reality"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#on-notetaking",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#on-notetaking",
    "title": "Introduction",
    "section": "On Notetaking",
    "text": "On Notetaking\n\n\n\nDo not write all of the information from the slides\nDo write along with what Derek writes on the whiteboard\nMake a few notes for main ideas and computer programming\nRetain placement notes, such as “Example #2” or “Survey#1”\nNo need to copy computer code from lecture (code will be provided)"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#why-probability",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#why-probability",
    "title": "Introduction",
    "section": "Why Probability?",
    "text": "Why Probability?\n\nThe Classic Birthday Problem\nHow many students have to enter the classroom until there are two students that share a birthday?"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#deterministic-vs-probabilistic",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#deterministic-vs-probabilistic",
    "title": "Introduction",
    "section": "Deterministic vs Probabilistic",
    "text": "Deterministic vs Probabilistic\nDeterministic: a situation that can be solved with equation solving and/or an algorithm\n\nExample: If water boils at 100 degrees Celsius, what is that threshold in Fahrenheit?\n\nProbabilistic: a situation that cannot be completely solved due to an element of chance\n\nExample: What is the chance that it will rain tomorrow?"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#probability-and-you",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#probability-and-you",
    "title": "Introduction",
    "section": "Probability and You",
    "text": "Probability and You\n\nApplied MathematicsBioengineeringChemical SciencesComputer Science and EngineeringEnvironmental EngineeringMaterials Science and EngineeringMechanical EngineeringPhysics\n\n\nDoes a probabilistic sequence converge or diverge?\n\n\nWhat percentage of lyme disease patients would be cured with the current but experimental treatments?\n\n\nWhat proportion of reactants undergo a reaction early in the reaction?\n\n\nHow many computers in a network would be affected after a virus infection?\n\n\nHow many of a certain species of plants are in the Vernal Pools Reserve?\n\n\nWhat percentage of a semiconductor is made of impurities?\n\n\nFor a commercial passenger airplane, what is the probability that at least two engines fail during a flight?\n\n\nHow many stars are in the Milky Way?"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#ugh-the-syllabus",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#ugh-the-syllabus",
    "title": "Introduction",
    "section": "Ugh, the Syllabus",
    "text": "Ugh, the Syllabus\n\nDescriptionCLOsPLOs\n\n\nConcepts of probability and statistics. Conditional probability, independence, random variables, distribution functions, descriptive statistics, transformations, sampling errors, confidence intervals, least squares and maximum likelihood. Exploratory data analysis and interactive computing.\n\n\n\nDevelop probabilistic models of random phenomena.\nInfer statistical models from real data.\nApply mathematical methods to probabilistic/statistical models to\n\n\nMake predictions and\nQuantify the uncertainty in these predictions.\n\n\nWrite and run “simple” R programs for the purposes of data analysis, modeling, and visualization.\n\n\n\n\nSolve mathematical problems using analytical methods.\nSolve mathematical problems using computational methods.\nRecognize the relationships between different areas of mathematics and the connections between mathematics and other disciplines.\nGive clear and organized written and verbal explanations of mathematical ideas to a variety of audiences\nModel real-world problems mathematically and analyze those models using their mastery of the core concepts."
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#assessment",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#assessment",
    "title": "Introduction",
    "section": "Assessment",
    "text": "Assessment\n\nBefore-LectureComputer ProgrammingWritten AssignmentsDiscussionSurveys and ReadingsExams\n\n\n\n10 percent of semester grade\nquizzes due before lecture\n\nno extensions\n\n5 to 10 minutes per quiz\n\nreview concepts and formulas\npreview thought exercises\n\n\n\n\n\n15 percent of semester grade\nlanguage: R\nplatform: JupyterHub\nanswers to frequently asked questions\n\nno, work may not be done in another language (e.g. Python)\nno, work may not be done in another IDE (e.g. VS Code)\n\n10 to 20 minutes per week\n\n\n\n\n20 percent of semester grade\nclassical math textbook homework\nadvice: do most of the work during your discussion section\n\n\n\n\n10 percent of semester grade for discussion section participation\nTA will track attendance\nadvised to work on written and computer assignments during discussion sections\n\n\n\n\n5 percent of semester grade\ngraded quickly on effort and completion\n5 to 10 minutes per survey/reading\n\n\n\n\nExam 1: 10 percent of semester grade (Wed., Mar. 1)\nExam 2: 15 percent of semester grade (Mon., Apr. 10)\nFinal Exam: 15 percent of semester grade (Sat., May 6)\nbased on the written assignments (i.e. no computer code)"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#student-accessibility-services",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#student-accessibility-services",
    "title": "Introduction",
    "section": "Student Accessibility Services",
    "text": "Student Accessibility Services\nSpecial Accommodations: University of California, Merced is committed to creating learning environments that are accessible to all. If you anticipate or experience physical or academic barriers based on a disability, please feel welcome to contact me privately so we can discuss options. In addition, please contact Student Accessibility Services (SAS) at (209) 228-6996 or disabilityservices@ucmerced.edu as soon as possible to explore reasonable accommodations. All accommodations must have prior approval from Student Accessibility Services on the basis of appropriate documentation. If you anticipate or experience barriers due to pregnancy, temporary medical condition, or injury,please feel welcome to contact me so we can discuss options. You are encouraged to contact the Dean of Students for support and resources at (209) 228-3633 or https://studentaffairs.ucmerced.edu/dean-students."
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#academic-integrity",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#academic-integrity",
    "title": "Introduction",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAcademic integrity is the foundation of an academic community and without it none of the educational or research goals of the university can be achieved. All members of the community are responsible for its academic integrity. Existing policies forbid cheating on examinations, plagiarism and other forms of academic dishonesty. The UC Merced Academic Honesty Policy The UC Merced Academic Honesty Policy can be found on the Student Conduct website. Infractions against academic integrity will incur consequences such as an “F” on the assignment/exam and/or a report to the Academic Senate."
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#nerdy-example",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#nerdy-example",
    "title": "Introduction",
    "section": "Nerdy Example",
    "text": "Nerdy Example\nHow many numbers between zero and one do we have to add up to have a sum that is greater than one?\n\nAssume selection from a uniform distribution"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#cumulative-summation",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#cumulative-summation",
    "title": "Introduction",
    "section": "Cumulative Summation",
    "text": "Cumulative Summation\nLet us start with the natural numbers\n\\[i = \\{1, 2, 3, ...\\}\\] Then cumulative summation takes place with\n\\[F(n) = \\sum_{i = 1}^{n} i\\]"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#cumulative-summation-1",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#cumulative-summation-1",
    "title": "Introduction",
    "section": "Cumulative Summation",
    "text": "Cumulative Summation\nIn R, we can define a sequence of natural numbers\n\nnatural_numbers <- 1:10\nprint(natural_numbers)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nand then employ the cumsum() function to perform the cumulative summation.\n\ncumsum(natural_numbers)\n\n [1]  1  3  6 10 15 21 28 36 45 55"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#random-number-generation",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#random-number-generation",
    "title": "Introduction",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nIn R, we generate a random number between zero and one (here: assumed from a uniform distribution) with the runif function.\n\nrunif(1)\n\n[1] 0.3763247\n\n\nFrom there, we can (for example) produce a sample of \\(n = 32\\) such random numbers\n\nrunif(32)\n\n [1] 0.808496253 0.530255245 0.575500263 0.925266754 0.218109261 0.948042712\n [7] 0.082536464 0.895080403 0.030557328 0.600224842 0.908927160 0.995998403\n[13] 0.506766883 0.232773983 0.097109339 0.935733052 0.008453035 0.821926254\n[19] 0.446838635 0.138450059 0.236597474 0.374993653 0.606477449 0.855093764\n[25] 0.869073509 0.978764013 0.936657508 0.248463629 0.316659372 0.007899186\n[31] 0.148523741 0.414026578"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#one-iteration",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#one-iteration",
    "title": "Introduction",
    "section": "One Iteration",
    "text": "One Iteration\nNext, we employ function composition to apply cumulative summation to our vector of random numbers\n\nX <- cumsum(runif(32))\nprint(X)\n\n [1]  0.7794607  1.1846579  2.1206889  3.0557736  3.3456139  3.9987518\n [7]  4.0430161  4.9879634  5.4354227  6.0101752  6.8707419  7.3198442\n[13]  7.6528439  8.3454212  8.3885242  9.1884093 10.0467503 10.9859400\n[19] 11.2546798 11.3066498 11.7020358 12.0357989 12.5634824 12.8123160\n[25] 12.8256348 12.9948029 13.3551376 13.6047064 13.7093181 13.9621456\n[31] 14.6875053 15.0901570\n\n\nand then we can check when our cumulative summation first exceeded 1.0\n\nwhich.max(X > 1.0)\n\n[1] 2"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#simulation",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#simulation",
    "title": "Introduction",
    "section": "Simulation",
    "text": "Simulation\nTo try to understand the randomness, we can repeat the procedure for many iterations (here, \\(N = 10000\\)).\n\nN <- 1e5 #number of iterations\nour_results <- rep(NA, N) #initialize space for results\nfor(i in 1:N){\n  this_vector <- cumsum(runif(10))\n  this_result <- which.max(this_vector > 1.0)\n  our_results[i] <- this_result\n}"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#visualization",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#visualization",
    "title": "Introduction",
    "section": "Visualization",
    "text": "Visualization\nTo understand a distribution of a probabilistic setting, we can visualize the results.\n\nCodeGraph\n\n\n\ndf <- data.frame(our_results)\ndf |>\n  ggplot() +\n  geom_histogram(aes(x = our_results), binwidth = 1,\n                 color = \"black\", fill = \"blue\") +\n  labs(title = \"Histogram of Results\",\n       subtitle = \"How would you describe the distribution?\",\n       caption = \"Math 32\",\n       x = \"number of numbers needed\",\n       y = \"count\") +\n  scale_x_continuous(breaks = seq(2,8))"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#measure-of-centrality",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#measure-of-centrality",
    "title": "Introduction",
    "section": "Measure of Centrality",
    "text": "Measure of Centrality\nTo hone in on our understanding of the distribution, let us take the mean() of our_results\n\nmean(our_results, na.rm = TRUE)\n\n[1] 2.71241\n\n\n\nNote: R stops execution upon evaluating a missing value. For our intents and purposes, we will suppress that exception with na.rm = TRUE"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#nerdy-example-1",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#nerdy-example-1",
    "title": "Introduction",
    "section": "Nerdy Example",
    "text": "Nerdy Example\nHow many numbers between zero and one do we have to add up to have a sum that is greater than one?\n\\[ e \\approx 2.718282\\]\n\n# theoretical answer\nexp(1)\n\n[1] 2.718282\n\n\nThought questions:\n\nhow do we know that the answer converges?\nhow many iterations did we need for a sufficient answer?"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#looking-ahead",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#looking-ahead",
    "title": "Introduction",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\nBe mindful of before-lecture quizzes\n\ndue Fri., Jan. 20:\n\nPerceptions of Probability (survey)\nWHW1\n\nIdentity Statement (essay)\n\nExam 1 will be on Wed., Mar. 1"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 32",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "m32slides",
    "section": "",
    "text": "35: Introduction to Machine Learning (2)\n\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n34: Introduction to Machine Learning\n\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nConfidence Intervals\n\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n30: Regression Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n29: Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n28: Beta Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n27: Maximum Likelihood\n\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n26: Likelihood\n\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n25: Central Limit Theorem\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n24: Estimators\n\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n23: Law of Large Numbers\n\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n22: Poisson Process\n\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n21: Change of Variables\n\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n20: Correlation\n\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n19: Covariance\n\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n18: Linear Operators\n\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n17: Continuous Joint Distributions\n\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n16: Discrete Joint Distributions\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n15: Normal Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n14: Exponential Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n13: Continuous Distributions\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n12: Geometric Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n11: Cumulative Computation\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n10: Binomial Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n09: Expectation\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n08: Variance\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n07: Measures of Centrality\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n6: Bayes’ Rule (Examples)\n\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n5: Bayes’ Rule (Concepts)\n\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n4: Conditional Probability\n\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n03_-_Complements\n\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n2: Inclusion-Exclusion\n\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study: Yawning\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n[your name]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nDerek Sollberger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html",
    "title": "02_-_Independence",
    "section": "",
    "text": "library(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#starter-examples",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#starter-examples",
    "title": "02_-_Independence",
    "section": "Starter Examples",
    "text": "Starter Examples\nIn order to introduce probability concepts, you will notice that many sections of this book start out by talking about simple situations such as flipping a coin or rolling a die (or rolling a pair of dice). This is to ease the reader into more complicated examples.\n\n\n\n\n\n\nNote\n\n\n\nA possibility space is a set of all of the possible outcomes."
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-1-one-die",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-1-one-die",
    "title": "02_-_Independence",
    "section": "Example 1: One Die",
    "text": "Example 1: One Die\n\n\nConsider rolling one six-sided die. For each of the following events, list their possible ways, and then find their probabilities:\n\nA: rolling an even number\nB: rolling a number greater than 3\nC: rolling a double-digit number\n\n\n\n\n\none, six-sided die"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#starting-to-combine",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#starting-to-combine",
    "title": "02_-_Independence",
    "section": "Starting to Combine",
    "text": "Starting to Combine\nAt first, we consider simply adding up the probabilities to compute the probability of the union of the probabilities:\n\\[P(A \\cup B) = P(A) + P(B) = ?\\]"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#inconsistent",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#inconsistent",
    "title": "02_-_Independence",
    "section": "Inconsistent?",
    "text": "Inconsistent?\nSo far,\n\\[P(A \\cup B) = P(A) + P(B) = 100\\%\\]\nWhat went wrong? (Discuss with a neighbor)"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#some-set-notation",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#some-set-notation",
    "title": "02_-_Independence",
    "section": "Some Set Notation",
    "text": "Some Set Notation\n\n\n\n\n\n\nTip\n\n\n\nThe union of sets A and B is the set of all elements that appear either in set A OR set B\n\\[A \\cup B = \\{x : x \\in A \\text{ OR } x \\in B\\}\\]\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe intersection of sets A and B is the set of all elements that appear both in set A AND set B\n\\[A \\cap B = \\{x : x \\in A \\text{ AND } x \\in B\\}\\]"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#inclusion-exclusion-principle",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#inclusion-exclusion-principle",
    "title": "02_-_Independence",
    "section": "Inclusion-Exclusion Principle",
    "text": "Inclusion-Exclusion Principle\nWe observe that there was an overlap in the calculation. Since some elements were counted twice, we can compensate by subtracting one copy of that overlap. This notion is called the Inclusion-Exclusion Principle\n\n\n\n\n\n\nNote\n\n\n\nTo compute the probability of a set union, we need to consider the overlapping portions. For two sets A and B, the probability of observing the union is\n\\[P(A \\cup B) = {\\color{red}P(A)} + {\\color{blue}P(B)} - {\\color{purple}P(A \\cap B)}\\]"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-2-two-dice",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-2-two-dice",
    "title": "02_-_Independence",
    "section": "Example 2: Two dice",
    "text": "Example 2: Two dice\n\n\nConsider rolling two six-sided dice. Find the probability that their total is 8 or the second die shows a number greater or equal to 5.\n\n\n\n\ntwo dice"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#subtlety-in-assumptions",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#subtlety-in-assumptions",
    "title": "02_-_Independence",
    "section": "Subtlety in Assumptions",
    "text": "Subtlety in Assumptions\nIn the previous example, we assumed a notion of sampling with replacement. That is, when rolling dice, we know that a number can be repeated. In other situations were observations cannot repeat, we are sampling without replacement."
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-3-faculty-matters",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-3-faculty-matters",
    "title": "02_-_Independence",
    "section": "Example 3: Faculty Matters",
    "text": "Example 3: Faculty Matters\n\nPromptSample Space 1Sample Space 2\n\n\nConsider a subset of the UC Merced Applied Math department with 6 faculty members—Blanchette, Buvoli, Sandoval, Stepanian, Thompson, Yatskar—must select two of its members to serve on a personnel review committee. Because the work will be time-consuming, no one is anxious to serve, so it is decided that the representatives will be selected by putting 6 slips of paper in a box, mixing them, and selecting two without replacement.\n\nWhat is the probability that both Thompson and Yatskar will be selected?\nWhat is the probability that at least one of the two members whose name begins with ‘B’ is selected?\nIf the 6 faculty members have taught for 15, 3, 5, 9, 4, and 12 years, respectively, at the university, what is the probability that the two chosen representatives have at least a combined 10 years’ teaching experience at the university?"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#assuming-a-distribution",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#assuming-a-distribution",
    "title": "02_-_Independence",
    "section": "Assuming a Distribution",
    "text": "Assuming a Distribution\nUnless otherwise noted, the coin flip has two disjoint outcomes—heads or tails—with probabilities\n\\[P(\\text{heads}) = 0.5, \\quad P(\\text{tails}) = 0.5\\]\n\nIn the early formation of the field of statistics, there were considerations such as the following. How should the possibility space for a trial of flipping two coins be represented?\n\n3 elements: \\(\\{\\)two heads, mixed result, two tails\\(\\}\\), OR\n4 elements: \\(\\{\\)HH, HT, TH, TT\\(\\}\\)"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#complements",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#complements",
    "title": "02_-_Independence",
    "section": "Complements",
    "text": "Complements\n\n\n\n\n\n\nNote\n\n\n\nTo sets are disjoint if\n\\[P(A \\cap B) = 0\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nA possibility space is a set of all of the possible outcomes for an event. It is usually denoted by the Greek letter capital omega.\n\n\nFor example, the set of all outcomes for two coin flips of a fair coin turns out to be\n\\[\\Omega = \\{HH, HT, TH, TT\\}\\]\n\n\n\n\n\n\nNote\n\n\n\nIf \\(A\\) is a set (and a subset of the possibility space), then the complement of A, denoted \\(A^{c}\\), is the set of outcomes that is in the universal set but not in the set A\n\\[A \\subseteq \\Omega \\quad\\Rightarrow\\quad A^{c} = \\Omega - A\\]"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-4-one-die",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-4-one-die",
    "title": "02_-_Independence",
    "section": "Example 4: One Die",
    "text": "Example 4: One Die\n\n\n\n\nFor example, if we think of our roll of a six-sided die, the possibility space was\n\\[\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\]\nIf we have a subset consisting of the even numbers\n\\[E = \\{2, 4, 6\\}\\]\nwhat do you think the complement \\(E^{c}\\) will be?"
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#empty-set",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#empty-set",
    "title": "02_-_Independence",
    "section": "Empty Set",
    "text": "Empty Set\n\n\n\n\n\n\nNote\n\n\n\nThe empty set \\(\\{\\}\\) literally has zero elements in the set\n\n\nClaim: Set \\(A\\) and its complement are disjoint."
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-5-replacement",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#example-5-replacement",
    "title": "02_-_Independence",
    "section": "Example 5: Replacement",
    "text": "Example 5: Replacement\nThere may be situations where we need to be careful about whether selections from a set were done with replacement or without replacement.\nIn the wardrobe, there are 8 blue socks and 6 red socks.\n\\[B, B, B, B, B, B, B, B\\]\n\\[R, R, R, R, R, R\\]\nCompute the following probabilities\n\nSelecting 3 red socks with replacement\nSelecting 3 red socks without replacement\nSelecting 4 blue socks with replacement\nSelecting 4 blue socks without replacement\n\n\n\n\n\n\n\nTip\n\n\n\nNotice how when we sample with replacement, each iteration is independent of the previous iterations. When we sample without replacement, each iteration depends on the previous iterations."
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#de-morgans-law",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#de-morgans-law",
    "title": "02_-_Independence",
    "section": "De Morgan’s Law",
    "text": "De Morgan’s Law\nOne relationship between the notions of complements, intersections, and unions is as follows.\n\n\n\n\n\n\nNote\n\n\n\n\\[(A \\cup B)^{c} = A^{c} \\cap B^{c}\\]\nThe complement of the union is the intersection of the complements."
  },
  {
    "objectID": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#looking-ahead",
    "href": "posts/02_-_Independence/Math_32_-_02_-_Independence.html#looking-ahead",
    "title": "02_-_Independence",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 2:\n\nWHW1\nLHW1\nCLO (survey)\n\nBe mindful of before-lecture quizzes\n\nExam 1 will be on Tues., Sept. 27\n\n\n\n\nRafa Moral\n\n\n40-second song"
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#example-subsetting",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#example-subsetting",
    "title": "Dependence",
    "section": "Example: Subsetting",
    "text": "Example: Subsetting\nConsider the months of the year\n\\[M = \\{\\text{Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec} \\}\\]\nLet us say that a month is ``long’’ if it has 31 days. What is the probability that we have a long month given that we are in the Fall semester?\n\\[F = \\{ \\text{Aug, Sep, Oct, Nov, Dec} \\}\\]"
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#conditional-probability",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#conditional-probability",
    "title": "Dependence",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nWe can condense this process into a formula for conditional probability:\n\n\n\n\n\n\nNote\n\n\n\nThe conditional probability of observing event \\(A\\) given event \\(B\\) has already taken place is\n\\[P(A|B) = \\displaystyle\\frac{P(A \\cap B)}{P(B)}\\]"
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#example-contigency-tables",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#example-contigency-tables",
    "title": "Dependence",
    "section": "Example: Contigency Tables",
    "text": "Example: Contigency Tables\nIn this hypothetical example, suppose that we are following an epidemiologist who is testing patients at a hospital in for the novel strain of coronavirus.\n\nBuild a contingency table with the following data\n\n\n175 true positives\n32 false negatives\n18 false positives\n2019 true negatives\n\n\nCompute the probability that a randomly selected patient is disease free given that the drug test is positive."
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#prosecutors-fallacy",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#prosecutors-fallacy",
    "title": "Dependence",
    "section": "Prosecutor’s Fallacy",
    "text": "Prosecutor’s Fallacy\n\nUsing the same counts as the previous example, compute the probability that for a randomly selected patient the test returns positive given that the patient is disease free.\n\n\n\n\n\n\n\nWarning\n\n\n\nConverses for conditional probability are almost never equal.\n\\[P(A|B) \\neq P(B|A)\\]"
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#bayes-rule",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#bayes-rule",
    "title": "Dependence",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\nIn the previous section, we studied conditional probability\n\\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\]\nand we talked about how the inverse probabilities \\(P(A|B)\\) and \\(P(B|A)\\) are almost never equal. In this section, we discuss how to properly think and calculate that inverse probability.\n\n\n\n\n\n\nTip\n\n\n\nAnother look at conditional probability is\n\\[P(A \\text{ and } B) = P(B|A) \\cdot P(A)\\]\nThis is read as “The probability of the intersection \\(A\\) and \\(B\\) is the probability of event \\(B\\) conditioned on event \\(A\\).”\n\n\n\n\n\n\n\n\nTip\n\n\n\nMoreover, if we consider how if event \\(B\\) is dependent on event \\(A\\), then sometimes \\(B\\) happens when \\(A\\) happens and sometimes when \\(A\\) does not occur. More succinctly, the total probability of event \\(B\\) is\n\\[P(B) = P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nStaring with the conditional probability formula\n\\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\]\nBayes’ Rule combines the ideas of conditioned probability and total probability as\n\\[P(A|B) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(B)} = \\displaystyle\\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})}\\]"
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#a-deep-dive",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#a-deep-dive",
    "title": "Dependence",
    "section": "A Deep Dive",
    "text": "A Deep Dive\n\nExampleTree DiagramNumeratorDenominator\n\n\nAn executive has their blood tested for boneitis. Let \\(B\\) be the event that an executive has the disease, and let \\(T\\) be the event that the test returns positive. Laboratory trials yielded the following information:\n\\[P(T|B) = 0.70 \\quad\\text{and}\\quad P(T|B^{c}) = 0.10\\]\nAssume a prior probability of \\(P(B) = 0.0032\\). Compute \\(P(B|T)\\)\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram"
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#more-practice",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#more-practice",
    "title": "Dependence",
    "section": "More Practice",
    "text": "More Practice\n\nExampleTree DiagramNumeratorDenominator\n\n\nAn executive has their blood tested for boneitis. Let \\(B\\) be the event that an executive has the disease, and let \\(T\\) be the event that the test returns positive. Laboratory trials yielded the following information:\n\\[P(T|B) = 0.70 \\quad\\text{and}\\quad P(T|B^{c}) = 0.10\\]\nAssume a prior probability of \\(P(B) = 0.0032\\). Compute \\(P(B|T^{c})\\)\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram"
  },
  {
    "objectID": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#looking-ahead",
    "href": "posts/03_-_Dependence/Math_32_-_03_-_Dependence.html#looking-ahead",
    "title": "Dependence",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 2:\n\nWHW1\nLHW1\nCLO (survey)\n\nBe mindful of before-lecture quizzes\nNo discussions next week for Math 32 (Sept. 5-7)\nExam 1 will be on Tues., Sept. 27\n\n\n\n\n\nsong about Bayes’ Rule"
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-spam-filtering",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-spam-filtering",
    "title": "Bayes’ Rule (Examples)",
    "section": "Example: Spam Filtering",
    "text": "Example: Spam Filtering\nIn 2002, Paul Graham used Bayes’ Rule as part of his algorithms to greatly decrease false positive rates of unwanted e-mails (“spam”). Let \\(H^{c}\\) be the event that an e-mail is “spam”. Let \\(W\\) be the event that an e-mail contains a trigger word such as “watches”. Suppose that\n\nthe probability that an e-mail contains that word given that it is spam is 17%\nthe probability that an e-mail contains that word given that it is not spam is 9%\nthe probability that a randomly selected e-mail message is spam is 80%\n\nFind the probability that an e-mail message is spam, given that the trigger word appears."
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-quality-control",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-quality-control",
    "title": "Bayes’ Rule (Examples)",
    "section": "Example: Quality Control",
    "text": "Example: Quality Control\nA manufacturing process produces integrated circuit chips. Over the long run the fraction of bad chips produced by the process is around 20%. Thoroughly testing a chip to determine whether it is good or bad is rather expensive, so a cheap test is tried. All good chips will pass the cheap test, but so will 10% of the bad chips. Given that a chip passes the test, what is the probability that the chip was defective?"
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-monty-hall-problem",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-monty-hall-problem",
    "title": "Bayes’ Rule (Examples)",
    "section": "Example: Monty Hall Problem",
    "text": "Example: Monty Hall Problem\n\n\n\n\nMonty Hall asks you to choose one of three doors. One of the doors hides a prize and the other two doors have no prize. You state out loud which door you pick, but you don’t open it right away.\n“Monty opens one of the other two doors, and there is no prize behind it.\n“At this moment, there are two closed doors, one of which you picked. The prize is behind one of the closed doors, but you don’t know which one. Monty asks you, ‘Do you want to switch doors?’”\n\nswitch doors\ndo not switch doors"
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-dui-checkpoint",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-dui-checkpoint",
    "title": "Bayes’ Rule (Examples)",
    "section": "Example: DUI Checkpoint",
    "text": "Example: DUI Checkpoint\nA breath analyzer, used by the police to test whether drivers exceed the legal limit set for the blood alcohol percentage while driving, is known to satisfy\n\\[P(A|B) = P(A^{c}|B^{c}) = x\\]\nwhere \\(A\\) is the event “breath analyzer indicates that legal limit is exceeded” and \\(B\\) “driver’s blood alcohol percentage exceeds legal limit.” On Saturday nights, about 4% of the drivers are known to exceed the limit.\n\nDescribe in words the meaning of \\(P(B|A)\\)\nDetermine \\(P(B|A)\\) if \\(x = 0.90\\)\nHow big should \\(x\\) be so that \\(P(B|A) \\geq 0.95\\)?"
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-disease-outbreak",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#example-disease-outbreak",
    "title": "Bayes’ Rule (Examples)",
    "section": "Example: Disease Outbreak",
    "text": "Example: Disease Outbreak\nSuppose that at UC Merced, there is a two percent chance that a freshman has herpes at the end of the school year. Let \\(H\\) be the event of having the virus, while \\(C\\) represents the event that the freshman is from the Cathedral dorm. Among the herpes carriers, the probably of being a Cathedral resident is 32%. Among those free of disease, the probably of being a Cathedral resident is 13%. What is the probability that a freshman has herpes, given that you know that he or she lived in the Cathedral dorm?"
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#generalized-bayes-rule",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#generalized-bayes-rule",
    "title": "Bayes’ Rule (Examples)",
    "section": "Generalized Bayes’ Rule",
    "text": "Generalized Bayes’ Rule\nIf we are conditioning \\(B\\) on an event \\(A\\), where the latter can be partitioned into several subsets,\n\\[A = \\{ A_{1}, A_{2}, ..., A_{j} \\}\\]\nthen the total probability is\n\\[P(B) = P(B|A_{1}) \\cdot P(A_{1}) + P(B|A_{2}) \\cdot P(A_{2}) + ... + P(B|A_{n}) \\cdot P(A_{n})\\]\nand Bayes Rule for computing the probability of \\(A_{i}\\) given \\(B\\) becomes\n\\[P(A_{i}|B) = \\displaystyle\\frac{ P(B|A_{i}) \\cdot P(A_{i}) }{ P(B|A_{1}) \\cdot P(A_{1}) + P(B|A_{2}) \\cdot P(A_{2}) + ... + P(B|A_{n}) \\cdot P(A_{n}) }\\]"
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#bayesian-odds",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#bayesian-odds",
    "title": "Bayes’ Rule (Examples)",
    "section": "Bayesian Odds",
    "text": "Bayesian Odds\n\n\n\n\n\n\nNote\n\n\n\nThe Bayesian odds of event \\(A\\) to event \\(B\\) given that event \\(C\\) has already taken place is\n\\[\\displaystyle\\frac{ P(A|C) }{ P(B|C) }\\]"
  },
  {
    "objectID": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#looking-ahead",
    "href": "posts/04_-_Bayes Rule/Math_32_-_04_-_Dependence.html#looking-ahead",
    "title": "Bayes’ Rule (Examples)",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 9:\n\nWHW2\nLHW2\nPerceptions of Probability (survey)\n\nBe mindful of before-lecture quizzes\nNo discussions this week for Math 32 (Sept. 5-7)\nExam 1 will be on Tues., Sept. 27\n\n\n\nsource"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#review-summation-notation",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#review-summation-notation",
    "title": "05: Measures of Centrality",
    "section": "Review: Summation Notation",
    "text": "Review: Summation Notation\nThe Fibonnaci Sequence \\(\\{1, 1, 2, 3, 5, 8, 13, ...\\}\\) is an example of a sequence where\n\\(F_{1} = 1, F_{2} = 1, F_{n} = F_{n-1} + F_{n-2}\\)\nCompute\n\n\\(\\displaystyle\\sum_{i=1}^{1} a_{i}\\)\n\\(\\displaystyle\\sum_{i=1}^{2} a_{i}\\)\n\\(\\displaystyle\\sum_{i=1}^{3} a_{i}\\)\n\\(\\displaystyle\\sum_{i=1}^{4} a_{i}\\)"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#populations-versus-samples",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#populations-versus-samples",
    "title": "05: Measures of Centrality",
    "section": "Populations versus Samples",
    "text": "Populations versus Samples\nWe tend to study a relatively small sample to understand the behavior of a much larger population."
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#mean-average",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#mean-average",
    "title": "05: Measures of Centrality",
    "section": "Mean (Average)",
    "text": "Mean (Average)\n\n“mean” and “average” are synonymous and will be used interchangably\nThe mean of \\(\\{ x_{1}, x_{2}, ..., x_{n} \\}\\) is denoted by\n\nGreek letter \\(\\mu\\) (“mu”) for a population mean (where we know all of the elements)\nAnglicized \\(\\bar{x}\\) (“x bar”) for a sample mean (where we are working with a sample of data)\n\nTo calculate the mean\n\nAdd up all of the numbers\nDivide by the amount of numbers\n\n\n\\[\\mu = \\displaystyle\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N} x_{i} \\quad\\text{or}\\quad \\bar{x} = \\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} x_{i}\\]"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#example-one-die",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#example-one-die",
    "title": "05: Measures of Centrality",
    "section": "Example: One Die",
    "text": "Example: One Die\n\nFind the mean of the roll of one six-sided die.\nFind the mean of the sample \\(\\{21, 22, 23, 32\\}\\)."
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#median",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#median",
    "title": "05: Measures of Centrality",
    "section": "Median",
    "text": "Median\nThe median of an ordered, discrete set of numbers is the number in the middle. If there are an even amount of data, then the median is the average of the middle two numbers in the ordered data set.\n\nCompute the median of \\(\\{1, 2, 1, 5, 3\\}\\)\nCompute the median of \\(\\{1, 1, 2, 3, 5, 8\\}\\)"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#weighted-mean",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#weighted-mean",
    "title": "05: Measures of Centrality",
    "section": "Weighted Mean",
    "text": "Weighted Mean\n\\[\\bar{x} = \\displaystyle\\frac{ \\displaystyle\\sum_{i=1}^{n} {\\color{red}w_{i}} \\cdot {\\color{blue}x_{i}}  }{ \\displaystyle\\sum_{i=1}^{n} {\\color{red}w_{i}} }\\]"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#example-cal-kulas",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#example-cal-kulas",
    "title": "05: Measures of Centrality",
    "section": "Example: Cal Kulas",
    "text": "Example: Cal Kulas\n\n\n\nSetting\nGoing into the final exam for a Statistics course, Cal Kulas had earned the following marks in the other categories.\n\n\n\n\n\nCategory\n\n\nWeight\n\n\nCal Kulas\n\n\n\n\n\n\nattendance\n\n\n10%\n\n\n95%\n\n\n\n\nquizzes\n\n\n20%\n\n\n75%\n\n\n\n\nmidterms\n\n\n25%\n\n\n60%\n\n\n\n\nproject\n\n\n20%\n\n\n90%\n\n\n\n\n\n\n\n\nTasks\n\nWhat is his current grade in the course?\nWhat does Cal Kulas need on the final exam so that he earns at least 80% in the course?"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#review-summation-notation-1",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#review-summation-notation-1",
    "title": "05: Measures of Centrality",
    "section": "Review: Summation Notation",
    "text": "Review: Summation Notation\nThe Fibonnaci Sequence \\(\\{1, 1, 2, 3, 5, 8, 13, ...\\}\\) is an example of a sequence where\n\\(F_{1} = 1, F_{2} = 1, F_{n} = F_{n-1} + F_{n-2}\\)\nCompute\n\n\\(\\displaystyle\\prod_{i=1}^{1} a_{i}\\)\n\\(\\displaystyle\\prod_{i=1}^{2} a_{i}\\)\n\\(\\displaystyle\\prod_{i=1}^{3} a_{i}\\)\n\\(\\displaystyle\\prod_{i=1}^{4} a_{i}\\)"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#for-formulas",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#for-formulas",
    "title": "05: Measures of Centrality",
    "section": "For Formulas",
    "text": "For Formulas\nIn particular, sigma and product notation will allow us to express probability formulas more efficiently. For example, the independence formula for two events \\(A\\) and \\(B\\),\n\\[P(AB) = P(A) \\cdot P(B)\\]\nbecomes the following for \\(n\\) independent events:\n\\[P\\left(  \\{X_{i}\\}_{i=1}^{n} \\right) = \\displaystyle\\prod_{i=1}^{n} P(X_{i})\\]"
  },
  {
    "objectID": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#looking-ahead",
    "href": "posts/05_-_Measures of Centrality/Math_32_05_Centrality.html#looking-ahead",
    "title": "05: Measures of Centrality",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 9:\n\nWHW2\nLHW2\nPerceptions of Probability (survey)\n\nBe mindful of before-lecture quizzes\nExam 1 will be on Tues., Sept. 27\n\n\n\n\n\nEven NASA makes mistakes!\n\n\nsource"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#overview",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#overview",
    "title": "06: Variance",
    "section": "Overview",
    "text": "Overview\n\\[s^{2} = \\displaystyle\\frac{1}{n-1}\\displaystyle\\sum_{i = 1}^{n} (x_{i} - \\bar{x})^{2}\\]\nToday’s main questions are “What is variance and what is a standard deviation?” We will go through\n\nthe formulas and calculations\ndemostrations\napplications (word problems)"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#notation",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#notation",
    "title": "06: Variance",
    "section": "Notation",
    "text": "Notation\n\n\n\n\nWe tend to study a relatively small sample to understand the behavior of a much larger population."
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#example-nathans-hot-dog-eating-contest",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#example-nathans-hot-dog-eating-contest",
    "title": "06: Variance",
    "section": "Example: Nathan’s Hot Dog Eating Contest",
    "text": "Example: Nathan’s Hot Dog Eating Contest\n\n\nEach year on July 4, the Nathan’s Hot Dog Eating Contest takes place on Coney Island in New York. The rule is simple: eat as many hot dogs (and buns) as you can in 10 minutes. The past 5 winning amounts were: 63, 70, 72, 74, 71. Compute the variance.\n\n\n\nsource\n\n\n\n\n\n\nwinners Joey Chestnut and Miki Sudo"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#units",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#units",
    "title": "06: Variance",
    "section": "Units?",
    "text": "Units?\n\n\n\nsquare hot dogs?"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#demostrations",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#demostrations",
    "title": "06: Variance",
    "section": "Demostrations",
    "text": "Demostrations\n\nSetupRA to BC to D\n\n\nFor each of the following sets\n\n\\(A = \\{1, 2, 3, 4, 5, 6, 7\\}\\)\n\\(B = \\{3, 4, 5, 6, 7, 8, 9\\}\\)\n\\(C = \\{-3, -2, -1, 0, 1, 2, 3\\}\\)\n\\(D = \\{-9, -6, -3, 0, 3, 6, 9\\}\\)\n\nwe will compute the sample mean, sample median, and sample standard deviation.\n\n\n\nA <- seq(1, 7, 1)\nB <- A + 2\nC <- seq(-3, 3, 3)\nD <- 3*C\n\n\n\n\nmean(A)\n\n[1] 4\n\nmean(B)\n\n[1] 6\n\n\n\nmedian(A)\n\n[1] 4\n\nmedian(B)\n\n[1] 6\n\n\n\nsd(A)\n\n[1] 2.160247\n\nsd(B)\n\n[1] 2.160247\n\n\n\n\n\nmean(C)\n\n[1] 0\n\nmean(D)\n\n[1] 0\n\n\n\nmedian(C)\n\n[1] 0\n\nmedian(D)\n\n[1] 0\n\n\n\nsd(C)\n\n[1] 3\n\nsd(D)\n\n[1] 9"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#shiny",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#shiny",
    "title": "06: Variance",
    "section": "Shiny",
    "text": "Shiny\n\n# https://quarto.org/docs/interactive/shiny/\nsliderInput(\"mu\", \"mean:\", \n            min = 1, max = 10, value = 5)\nsliderInput(\"s\", \"deviation:\", \n            min = 1, max = 5, value = 2)\nplotOutput(\"distPlot\")\n\n\noutput$distPlot <- renderPlot({\n  x <- rnorm(100, mean = mu, sd = s)\n  df <- data.frame(x = x)\n  \n  df |>\n    ggplot(aes(x = x)) +\n    geom_density()\n})"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#standardization",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#standardization",
    "title": "06: Variance",
    "section": "Standardization",
    "text": "Standardization\n\\[z = \\displaystyle\\frac{x - \\mu}{\\sigma}\\]\nTo standardize data, compute a z-score by\n\nsubtracting by the mean\nthen dividing by the standard deviation\n\nThis calculation is considered to be “unitless”, and the units are usually said as “[number of] standard deviations above/below the mean”\nMost data falls within two standard deviations of the mean,\n\\[\\text{usually } z \\in (-2, 2)\\]\nbut \\(z \\in (-\\infty, \\infty)\\)"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#example-dating-website-data",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#example-dating-website-data",
    "title": "06: Variance",
    "section": "Example: Dating Website Data",
    "text": "Example: Dating Website Data\n\nScenario 1Scenario 2\n\n\n\nAccording to OkCupid data, if men rate women on a scale from 1 = “least attractive” to 7 = “most attractive”, the average score is 3.99 with a sample standard deviation of 1.6401.\n\nWhat is the \\(z\\)-score of a woman rated a “6”?\nWhat is the attractiveness score of a woman at a \\(z\\)-score of 1.5?\n\n\n\n\nAccording to OkCupid data, if women rate men on a scale from 1 = “least attractive” to 7 = “most attractive”, the average score is 2.43 with a sample standard deviation of 1.2510.\n\nWhat is the \\(z\\)-score of a woman rated a “6”?\nWhat is the attractiveness score of a woman at a \\(z\\)-score of 1.5?"
  },
  {
    "objectID": "posts/06_-_Variance/Math_32_06_Variance.html#looking-ahead",
    "href": "posts/06_-_Variance/Math_32_06_Variance.html#looking-ahead",
    "title": "06: Variance",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 16:\n\nWHW3\nLHW3\nDemographics Part 1 (survey)\n\nBe mindful of before-lecture quizzes\nExam 1 will be on Tues., Sept. 27\n\nmore information in weekly announcements\n\n\n\n\nsource"
  },
  {
    "objectID": "posts/07_-_Expectation/Math_32_07_Expectation.html#example-demographics",
    "href": "posts/07_-_Expectation/Math_32_07_Expectation.html#example-demographics",
    "title": "07: Expectation",
    "section": "Example: Demographics",
    "text": "Example: Demographics\n\nSuppose that all of the students in Math 32 are between ages 19 and 21 inclusively with the following distribution:\n\nAge 19: 35%\nAge 20: 45%\nAge 21: 20%\n\nRewrite the data as a discrete mass function."
  },
  {
    "objectID": "posts/07_-_Expectation/Math_32_07_Expectation.html#discrete-probability-distributions",
    "href": "posts/07_-_Expectation/Math_32_07_Expectation.html#discrete-probability-distributions",
    "title": "07: Expectation",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\n\n\n\n\n\n\nNote\n\n\n\nA discrete probability distribution is a population where we can list the possible values\n\\[X = \\{ x_{1}, x_{2}, x_{3}, ... x_{n} \\}\\]\nand measure the respective probabilities\n\\[P(X = x_{1}), P(X = x_{2}), P(X = x_{3}), ..., P(X = x_{n})\\]\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs usual, the probabilities rules include that each probability is between zero and one inclusively\n\\[0 \\leq P(X = x_{i}) \\leq 1\\]\nand that all probabilities add up to 100 percent\n\\[\\displaystyle\\sum_{x \\in X} P(X = x) = 1\\]"
  },
  {
    "objectID": "posts/07_-_Expectation/Math_32_07_Expectation.html#expectation",
    "href": "posts/07_-_Expectation/Math_32_07_Expectation.html#expectation",
    "title": "07: Expectation",
    "section": "Expectation",
    "text": "Expectation\n\n\n\n\n\n\nNote\n\n\n\nFor a random variable \\(X\\) (understood through a discrete probability distribution), its expected value is\n\\[\\mu = \\text{E}[{\\color{blue}X}] = \\displaystyle\\sum_{x \\in X} {\\color{blue}x} \\cdot {\\color{red}P(X = x)}\\]\n\n\n\nCompute the expected value of the roll of one six-sided die."
  },
  {
    "objectID": "posts/07_-_Expectation/Math_32_07_Expectation.html#variance",
    "href": "posts/07_-_Expectation/Math_32_07_Expectation.html#variance",
    "title": "07: Expectation",
    "section": "Variance",
    "text": "Variance\n\n\n\n\n\n\nNote\n\n\n\nThe variance of a random variable \\(X\\) is defined as the expected squared deviation from the mean\n\\[\\text{Var}(X) = \\text{E}[(X - \\text{E}[X])^{2}]\\]\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe above theoretical formula is built from first principles and is good for building the math foundation. However, the following practical formula is better for hand calculations and computer calculations.\n\n\nClaim:\n\\[\\text{Var}(X) = \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2}\\]"
  },
  {
    "objectID": "posts/07_-_Expectation/Math_32_07_Expectation.html#example-boxing-bets",
    "href": "posts/07_-_Expectation/Math_32_07_Expectation.html#example-boxing-bets",
    "title": "07: Expectation",
    "section": "Example: Boxing Bets",
    "text": "Example: Boxing Bets\n\n\nBefore watching a boxing match, my friends and I made bets over which round the fight would end. A boxing match lasts up to 12 rounds. Each gambler pays $5 and is assigned a round randomly. The winner garners the whole pot of money. What is the expected value of the bet? What is the variance of the bet?"
  },
  {
    "objectID": "posts/07_-_Expectation/Math_32_07_Expectation.html#looking-ahead",
    "href": "posts/07_-_Expectation/Math_32_07_Expectation.html#looking-ahead",
    "title": "07: Expectation",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 16:\n\nWHW3\nLHW3\nDemographics Part 1 (survey)\n\nBe mindful of before-lecture quizzes\nExam 1 will be on Tues., Sept. 27\n\nmore information in weekly announcements\n\n\n\n\nsource"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#bernoulli-trials",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#bernoulli-trials",
    "title": "08: Binomial Distribution",
    "section": "Bernoulli Trials",
    "text": "Bernoulli Trials\nTo continue our exploration of discrete distributions, we will look at situations that have two disjoint possibilities.\n\n\n\n\n\n\nBernoulli Trials\n\n\n\nFor math symbols to represent a Bernoulli trial, the events \\(\\{1, 0\\}\\) have respective probabilities \\(p\\) and \\(1-p\\).\n\n\nFor example, for one flip of a coin\n\\[P(\\text{heads}) = p, \\quad P(\\text{tails}) = 1-p\\]\n\n\n\none coin, but not necessarily fair"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#arrangements",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#arrangements",
    "title": "08: Binomial Distribution",
    "section": "Arrangements",
    "text": "Arrangements\n\n\n\n\n\n\nPermutations\n\n\n\nPermutations (and the number of permutations) are the arrangements when order matters\n\n\n\n\n\n\n\n\nCombinations\n\n\n\nCombinations (and the number of combinations) are the arrangements when order does not matter\n\n\nFlipping 3 fair coins, what is the probability that heads will be observed exactly twice?"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#choose",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#choose",
    "title": "08: Binomial Distribution",
    "section": "Choose",
    "text": "Choose\n\n\n\\[\\binom{n}{k} = \\displaystyle\\frac{n!}{k!(n-k)!}\\]\n\nsaid ``n choose k’’\nThis choose operator keeps track of the number of permutations in a certain combination\nnote \\(0! = 1\\) (to avoid dividing by zero)\n\n\n\n\n\nfrom The Simpsons"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#binomial-distribution",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#binomial-distribution",
    "title": "08: Binomial Distribution",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\n\n\n\n\nBinomial Distribution\n\n\n\n\\[P(x = k) = \\binom{n}{k} p^{k}(1-p)^{n-k}\\]\n\n\\(0 \\leq k \\leq n\\), where \\(n\\) and \\(k\\) are whole numbers\n\\(0 \\leq p \\leq 1\\)"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#example-squirtle",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#example-squirtle",
    "title": "08: Binomial Distribution",
    "section": "Example: Squirtle",
    "text": "Example: Squirtle\n\n\n\n\n\n\n\n\nBinomial Distribution\n\n\n\n\\[P(x = k) = \\binom{n}{k} p^{k}(1-p)^{n-k}\\]\n\n\\(0 \\leq k \\leq n\\), where \\(n\\) and \\(k\\) are whole numbers\n\\(0 \\leq p \\leq 1\\)\n\n\n\n\nHistorically, Squirtle defeats Charizard 32% of the time. If there are 5 battles, what is the probability that Squirtle wins exactly 2 times?"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#example-charizard",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#example-charizard",
    "title": "08: Binomial Distribution",
    "section": "Example: Charizard",
    "text": "Example: Charizard\n\n\n\n\n\nCharizard\n\n\n\nHistorically, Charizard defeats Squirtle 68% of the time. If there are 5 battles, what is the probability that Charizard wins exactly 3 times?"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#symmetry",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#symmetry",
    "title": "08: Binomial Distribution",
    "section": "Symmetry",
    "text": "Symmetry\n\nPropertySquirtleCharizardR code\n\n\nThe previous two examples had the same answer, which is true due to a symmetry property in the choose operator:\n\\[\\binom{n}{k} = \\binom{n}{n-k}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk <- 0:5\npk <- dbinom(k, 5, 0.32)\nk_bool <- k == 2\ndf <- data.frame(k, pk, k_bool)\n\ndf |>\n  ggplot(aes(x = k, y = pk, \n             color = k_bool, fill = k_bool)) +\n  geom_bar(stat = \"identity\") +\n  geom_label(aes(x = k, y = pk, \n                 label = round(pk, 4)),\n             color = \"black\", fill = \"white\") +\n  labs(title = \"2 Squirtle Wins\",\n       subtitle = \"n = 5, k = 2, p = 0.32, P(k = 2) = 0.3220\",\n       caption = \"Math 32\",\n       x = \"wins\",\n       y = \"probability\") +\n  scale_color_manual(values = c(\"black\", \"#ca7721\")) +\n  scale_fill_manual(values = c(\"gray70\", \"#297383\")) +\n  theme(\n    legend.position = \"none\",\n    panel.background = element_blank()\n  )\n\n\n\n# plotly::ggplotly(ex2_plot)\n\n\nk <- 0:5\npk <- dbinom(k, 5, 0.68)\nk_bool <- k == 3\ndf <- data.frame(k, pk, k_bool)\n\ndf |>\n  ggplot(aes(x = k, y = pk, \n             color = k_bool, fill = k_bool)) +\n  geom_bar(stat = \"identity\") +\n  geom_label(aes(x = k, y = pk, \n                 label = round(pk, 4)),\n             color = \"black\", fill = \"white\") +\n  labs(title = \"3 Charizard Wins\",\n       subtitle = \"n = 5, k = 3, p = 0.68, P(k = 3) = 0.3220\",\n       caption = \"Math 32\",\n       x = \"wins\",\n       y = \"probability\") +\n  scale_color_manual(values = c(\"black\", \"#de5138\")) +\n  scale_fill_manual(values = c(\"gray70\", \"#e53800\")) +\n  theme(\n    legend.position = \"none\",\n    panel.background = element_blank()\n  )\n\n\n\n# plotly::ggplotly(ex2_plot)\n\n\n\n\n\n\n\n\n\n\nHow do we pick between \\(p\\) and \\(1-p\\)?\n\n\n\nAt first, it does not matter how you define the binomial setting for what corresponds to \\(p\\) and what corresponds to \\(1-p\\), but you need to be consistent in the rest of the task for how you defined your variables and use the value(s) for \\(k\\)."
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#parameters",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#parameters",
    "title": "08: Binomial Distribution",
    "section": "Parameters",
    "text": "Parameters\nThe notation \\(X \\sim Ber(p)\\) is read as “random variable \\(X\\) has a Bernoulli distribution with parameter \\(p\\)”. Compute the expected value and variance for a Bernoulli trial."
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#parameters-1",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#parameters-1",
    "title": "08: Binomial Distribution",
    "section": "Parameters",
    "text": "Parameters\nThe notation \\(X \\sim Bin(n,p)\\) is read as ``random variable \\(X\\) has a binomial distribution with parameters \\(n\\) and \\(p\\)’’. Compute the expected value and variance for a binomial distribution.\nWe are assuming that the \\(n\\) trials are from each other, where independence in probability means that\n\\[P\\left(  \\{X_{i}\\}_{i=1}^{n} \\right) = \\displaystyle\\prod_{i=1}^{n} P(X_{i})\\]\nIn other words, we are sampling the Bernoulli trial \\(n\\) times with replacement, so we can simply multiply the results from the previous example by \\(n\\).\n\\[\\begin{array}{|c|c|c|}\n\\hline\n\\textbf{mean}               & \\mu & np \\\\ \\hline\n\\textbf{variance}           & \\sigma^{2} & np(1-p) \\\\ \\hline\n\\textbf{standard deviation} & \\sigma & \\sqrt{np(1-p)} \\\\ \\hline\n\\end{array}\\]"
  },
  {
    "objectID": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#looking-ahead",
    "href": "posts/08_-_Binomial Distribution/Math_32_08_Binomial_Distribution.html#looking-ahead",
    "title": "08: Binomial Distribution",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 23:\n\nWHW4\nLHW4\nDemographics Part 2 (survey)\n\nExam 1 will be on Tues., Sept. 27\n\nmore information in weekly announcements\n\n\n\n\nsource"
  },
  {
    "objectID": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#probability-mass-function",
    "href": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#probability-mass-function",
    "title": "9: Cumulative Computation",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\nLast time, we developed the probability mass function for the binomial distribution. The probability of choosing \\(k\\) observations among a sample size of \\(n\\), each observation with prior probability \\(p\\), is given by\n\\[P(k) = \\binom{n}{k}p^{k}(1-p)^{n-k}\\]\nNote the usual properties of probability:\n\neach probability is between zero and one (inclusively)\n\n\\[0 \\leq P(k) \\leq 1 \\quad\\text{for each } k\\]\n\nall probabilities add up to 100%\n\n\\[1 = \\displaystyle\\sum_{k = 0}^{n} \\binom{n}{k}p^{k}(1-p)^{n-k}\\]"
  },
  {
    "objectID": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#from-one-to-many",
    "href": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#from-one-to-many",
    "title": "9: Cumulative Computation",
    "section": "From One to Many",
    "text": "From One to Many\n\nExactlyAt MostExactlyMore Than\n\n\nThere are 4 parking spaces in front of the boba place. Suppose that each parking space tends to be occupied about 57 percent of the time. What is the probability that exactly 3 of the parking spaces are open?\n\n\n\nboba!\n\n\n\n\nThere are 4 parking spaces in front of the boba place. Suppose that each parking space tends to be occupied about 57 percent of the time. What is the probability that at most 2 of the parking spaces are open?\n\n\n\nboba!\n\n\n\n\nThere are 32 parking spaces in a row in a UC Merced parking lot. Suppose that each parking space tends to be occupied about 81 percent of the time. What is the probability that exactly 4 of the parking spaces are open?\n\n\n\nparking\n\n\n\n\nThere are 32 parking spaces in a row in a UC Merced parking lot. Suppose that each parking space tends to be occupied about 81 percent of the time. What is the probability that more than 5 of the parking spaces are open?\n\n\n\nparking"
  },
  {
    "objectID": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#leveraging-complements",
    "href": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#leveraging-complements",
    "title": "9: Cumulative Computation",
    "section": "Leveraging Complements",
    "text": "Leveraging Complements\nThere are 32 parking spaces in a row in a UC Merced parking lot. Suppose that each parking space tends to be occupied about 97 percent of the time. What is the probability that at least one of the parking spaces is open?\n\n\n\nparking"
  },
  {
    "objectID": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#looking-ahead",
    "href": "posts/09_-_Cumulative Calculations/Math_32_09_Cumulative_Computation.html#looking-ahead",
    "title": "9: Cumulative Computation",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Sept. 23:\n\nWHW4\nLHW4\nDemographics Part 2 (survey)\n\nExam 1 will be on Tues., Sept. 27\n\nmore information in weekly announcements\n\n\n\n\n\n\nsome found sign in Sausalito"
  },
  {
    "objectID": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#infinite-support",
    "href": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#infinite-support",
    "title": "10: Geometric Distribution",
    "section": "Infinite Support",
    "text": "Infinite Support\n\n\nHere let us assume an endless box of chocolates with random selection with replacement of\nWrite out some of the sample space. Let \\(F\\) be the event of choosing a favorite chocolate, so \\(F^{c}\\) is the event of choosing an average chocolate, \\(P(F) = p\\) and \\(P(F^{c}) = 1-p\\). Let \\(k\\) be the amount of chocolates chosen reaching a favorite chocolate\n\n\n\n\nForest Gump"
  },
  {
    "objectID": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#probability-mass-function",
    "href": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#probability-mass-function",
    "title": "10: Geometric Distribution",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\n\n\n\n\n\n\nGeometric Distribution Probability Mass Function\n\n\n\nA geometric distribution is a discrete probability distribution with\n\nprobability \\(p\\) for “success”\nprobability \\(1-p\\) for “failure”\nfor \\(k = 0, 1, 2, 3, ...\\)\n“success” on \\((k+1)^{\\text{th}}\\) trial\n\nThe probability mass function (PMF) is\n\\[f(X = k) = (1-p)^{k}p\\]\nThe cumulative mass function (CMF) is\n\\[F(X \\leq k) = 1 - (1-p)^{k+1}\\]"
  },
  {
    "objectID": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#cumulative-calculations",
    "href": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#cumulative-calculations",
    "title": "10: Geometric Distribution",
    "section": "Cumulative Calculations",
    "text": "Cumulative Calculations\n\n\n\n\n\nTed Mosby\n\n\n\nTed Mosby is setting up his weekend plans. Barney convinced him to try out an app called Tinder. “Ted Mosby, Architect” has a 2/3 chance of setting up a date among those he “swipes right”. Suppose that Ted stops using Tinder this session once he sets up a date.\n\nCompute the probability that Ted needs exactly 4 ``swipes right’’ to set up a date.\nCompute the probability that Ted needs at most 4 ``swipes right’’ to set up a date."
  },
  {
    "objectID": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#complementary-cumulative-mass-function",
    "href": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#complementary-cumulative-mass-function",
    "title": "10: Geometric Distribution",
    "section": "Complementary Cumulative Mass Function",
    "text": "Complementary Cumulative Mass Function\n\n\n\n\n\n\nComplementary Cumulative Mass Function\n\n\n\nWhen modeling with a geometric distribution—i.e. \\(X \\sim Geo(p)\\)—the probability that “success” takes more than \\(k\\) trials is\n\\[P(X > k) = (1-p)^{k+1}\\]\n\n\n\nMore ThanConditional ProbabilityConditional Probability 2\n\n\n\n\n\n\n\nRebecca Bunch\n\n\n\nRebecca Bunch follows Josh Chan, whom she briefly dated as a teenager, by moving to West Covina, California. Suppose that it may take a while for Rebecca and Josh to reunite and there is a 29 percent chance of them meeting during any particular week. What is the probability that it will take Rebecca more than 3 weeks to reunite with Josh?\n\n\n\n\n\n\n\n\n\nRebecca Bunch\n\n\n\nWhat is the probability that it will take Rebecca more than 9 weeks to reunite with Josh given that she has already spent more than 5 weeks in West Covina?\n\n\n\n\n\n\n\n\n\nRebecca Bunch\n\n\n\nStill holding on to that 29 percent chance of reuniting with Josh, compute the probability that it will take Rebecca more than 36 weeks to reunite with Josh given that she has already spent more than 32 weeks in West Covina."
  },
  {
    "objectID": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#memoryless-property",
    "href": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#memoryless-property",
    "title": "10: Geometric Distribution",
    "section": "Memoryless Property",
    "text": "Memoryless Property\nThe previous two examples demonstrated the memoryless property.\n\n\n\n\n\n\nMemoryless Property\n\n\n\nThe geometric distribution is the only discrete probability distribution that has the memoryless property:\n\\[P(X \\geq a + b | X \\geq b) = P(X \\geq a)\\]"
  },
  {
    "objectID": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#looking-ahead",
    "href": "posts/10_-_Geometric Distribution/Math_32_10_Geometric_Distribution.html#looking-ahead",
    "title": "10: Geometric Distribution",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 7:\n\nWHW5\nLHW5\nMid-Semester Survey\n\nExam 1 will be on Tues., Nov. 1"
  },
  {
    "objectID": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#continuous-variables",
    "href": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#continuous-variables",
    "title": "11: Continuous Distributions",
    "section": "Continuous Variables",
    "text": "Continuous Variables\n\n\n\nImage Credit: G2 Learing Hub\n\n\n\nA discrete variable is “countable” (has values that can be in a list)\nA continuous variable has values that cannot be written as a list (“uncountable”)"
  },
  {
    "objectID": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#uniform-distribution",
    "href": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#uniform-distribution",
    "title": "11: Continuous Distributions",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\nYou arrive at a bus stop at 10 o’clock, knowing that the bus will arrive at some time uniformly distributed between 10:00 and 10:30.\n\nNormalizationFewerMoreConditional\n\n\n\n\nWe build a probability density function (PDF) by ensuring that the area under the curve equals 100 percent (i.e. one square unit).\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait fewer than 12 minutes?\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait longer than 10 minutes?\n\n\n\n\n\n\n\n\nIf at 10:15 the bus has not yet arrived, what is the probability that you will have to wait at least an additional 10 minutes?"
  },
  {
    "objectID": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#linear-distribution",
    "href": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#linear-distribution",
    "title": "11: Continuous Distributions",
    "section": "Linear Distribution",
    "text": "Linear Distribution\n\nYou arrive at a bus stop at 10 o’clock, knowing that the bus will arrive at some time linearly distributed between 10:00 and 10:30. The probability density function (PDF) is\n\\[f(x) = \\begin{cases} -\\displaystyle\\frac{1}{450}x + \\displaystyle\\frac{1}{15} & 0 \\leq x \\leq 30 \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\nPDFFewerFewerBetween\n\n\n\n\nProbability Density Function (PDF)\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait fewer than 7 minutes?\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait fewer than 11 minutes?\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait between 7 and 11 minutes?"
  },
  {
    "objectID": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#cumulative-density-function",
    "href": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#cumulative-density-function",
    "title": "11: Continuous Distributions",
    "section": "Cumulative Density Function",
    "text": "Cumulative Density Function\n\nThere are no nonzero probabilities to the left. The CDF “starts with zero” probability. Here, \\(F(0) = 0\\)\nSince all probabilities add up to 100%, the CDF ends at one”. Here, \\(F(30) = 1\\)\n\n\n\n\nCDF"
  },
  {
    "objectID": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#looking-ahead",
    "href": "posts/11_-_Continuous Distributions/Math_32_11_Geometric_Distribution.html#looking-ahead",
    "title": "11: Continuous Distributions",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 7:\n\nWHW5\nLHW5\nMid-Semester Survey\n\nExam 2 will be on Tues., Nov. 1"
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#inequalities",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#inequalities",
    "title": "12: Exponential Distribution",
    "section": "Inequalities",
    "text": "Inequalities\n\n\n\n\n\n\nDiscrete Probability Distributions\n\n\n\nThe inclusive versus exclusive variation in inequalities matter in discrete probability distributions. With a random variable \\(X\\) defined over a support of \\(k = \\{0, 1, 2, 3, ... \\}\\) number of trials,\n\\[P(X < k) = \\displaystyle\\sum_{i=0}^{k-1} P(i), \\quad P(X \\leq k) = \\displaystyle\\sum_{i=0}^{k} P(i)\\]\n\\[P(X < 32) = P(k = 0) + P(k = 1) + \\cdots + P(k = 31)\\]\n\\[P(X \\leq 32) = P(k = 0) + P(k = 1) + \\cdots + P(k = 31) + P(k = 32)\\]\n\n\n\n\n\n\n\n\nContinuous Probability Distributions\n\n\n\nThe inclusive versus exclusive variation in inequalities do not matter with continuous probability distributions.\nClaim: For a random variable \\(X\\) with probability density function \\(f\\),\n\\[P(X < b) = P(X \\leq b)\\]"
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#motivation",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#motivation",
    "title": "12: Exponential Distribution",
    "section": "Motivation",
    "text": "Motivation\n\n\nThere are many situations that are better modeled with an open set \\([a, \\infty)\\) of support, so we can look at the mother function\n\\[f(x) = e^{-x}, \\quad a < x\\]\nwhich has a horizontal asymptote. Next, a rate parameter \\(\\lambda\\) (“lambda”) gives flexibility in models (i.e. the number we plug in for \\(\\lambda\\) depends on the word problem).\n\\[f(x) = e^{-\\lambda x}, \\quad a < x\\]"
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#normalization",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#normalization",
    "title": "12: Exponential Distribution",
    "section": "Normalization",
    "text": "Normalization\nWithout much loss of generality, we can shift to a support of \\([0, \\infty)\\). Our next goal is to rescale the function values so that the area under the curve is equal to 100 percent.\n\nFind the value of \\(k\\) so that \\(f\\) is a probability density function."
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#probability-density-function",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#probability-density-function",
    "title": "12: Exponential Distribution",
    "section": "Probability Density Function",
    "text": "Probability Density Function\n\n\n\n\n\n\nProbability Density Function\n\n\n\nHaving found the value of the scalar \\(k\\), our probability density function (PDF) is\n\\[\\text{PDF: } f(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x > 0 \\\\ 0, & \\text{otherwise} \\end{cases}\\]"
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#cumulative-distribution-function",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#cumulative-distribution-function",
    "title": "12: Exponential Distribution",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function\n\n\n\n\n\n\nCumulative Distribution Function\n\n\n\nTo continue to think of probabilities as area under a curve, we derive the cumulative distribution function (CDF) as the integral of the probability density function\n\\[F(x) = \\displaystyle\\int_{-\\infty}^{x} \\! f(t) \\, dt = \\displaystyle\\int_{0}^{x} \\! \\lambda e^{-\\lambda t} \\, dt = 1 - e^{-\\lambda x}, \\quad x > 0\\]\nThat is, the CDF of an exponential distribution is\n\\[F(x) = \\begin{cases} 1 - e^{-\\lambda x}, & x > 0 \\\\ 0, & x < 0 \\end{cases}\\]\n\\[~\\]\nThe properties of probability include\n\nWe start with zero probability\n\\[\\displaystyle\\lim_{x \\to -\\infty} F(x) = 0\\]\nWe end with all probability\n\\[\\displaystyle\\lim_{x \\to \\infty} F(x) = 1\\]"
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#conventions",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#conventions",
    "title": "12: Exponential Distribution",
    "section": "Conventions",
    "text": "Conventions\n\n\n\n\n\n\nConventions\n\n\n\n\nThe models are related with \\(\\lambda = \\displaystyle\\frac{1}{\\beta}\\). Here in Math 32, we will use the model with the rate parameter \\(\\lambda\\) (lambda) to match the convention used by the R programming language"
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#sample-statistics",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#sample-statistics",
    "title": "12: Exponential Distribution",
    "section": "Sample Statistics",
    "text": "Sample Statistics\n\nMeanVarianceMedian?Median\n\n\n\n\n\n\n\n\nMean\n\n\n\nRecall that the mean and the expected value are synonymous.\n\\[\\mu = \\text{E}[{\\color{blue}X}] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! {\\color{blue}x} \\cdot f(x) \\, dx = \\lambda\\displaystyle\\int_{0}^{\\infty} \\! {\\color{blue}x} \\cdot e^{-\\lambda x}\\, dx = \\displaystyle\\frac{1}{\\lambda}\\]\nWe have shown that the expected value for \\(X \\sim Exp(\\lambda)\\) is \\(\\text{E}[X] = \\displaystyle\\frac{1}{\\lambda}\\).\n\n\n\n\n\n\n\n\n\n\nVariance\n\n\n\nFurther analogues to the formulas used for discrete probability distributions include the the second moment\n\\[\\text{E}[{\\color{blue}X^{2}}] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! {\\color{blue}x^{2}} \\cdot f(x) \\, dx = \\lambda\\displaystyle\\int_{0}^{\\infty} \\! {\\color{blue}x^{2}} \\cdot e^{-\\lambda x}\\, dx = \\displaystyle\\frac{2}{\\lambda^{2}}\\]\nand it follows that the variance for an exponential distribution with rate parameter \\(\\lambda\\) is\n\\[\\sigma^{2} = \\text{Var}(X) = \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2} = \\displaystyle\\frac{2}{\\lambda^{2}} - \\left(\\displaystyle\\frac{1}{\\lambda}\\right)^{2} = \\displaystyle\\frac{1}{\\lambda^{2}}\\]\nAs usual, the standard deviation is the square root of the variance.\n\\[\\sigma = \\sqrt{ \\displaystyle\\frac{1}{\\lambda^{2}} } = \\displaystyle\\frac{1}{\\lambda}\\]\n\n\n\n\n\n\n\n\n\n\nMedian\n\n\n\nBack in discrete data, if we had an ordered list of data,\n\\[\\{ x_{(1)}, x_{(2)}, x_{(3)}, ...\\}\\]\nwe said that the median is the value in the ``middle’’ of the list.\nQuery: How do you think we define a median here in the setting of continuous distributions?\n\n\n\n\n\n\n\n\n\n\nMedian\n\n\n\nBack in discrete data, if we had an ordered list of data,\n\\[\\{ x_{(1)}, x_{(2)}, x_{(3)}, ...\\}\\]\nwe said that the median is the value in the ``middle’’ of the list."
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#waiting-times",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#waiting-times",
    "title": "12: Exponential Distribution",
    "section": "Waiting Times",
    "text": "Waiting Times\nLet us now return to the notion of waiting times. Suppose that a friend of yours is going to pick you up for a carpool, and you estimate that he tends to arrive with a mean time of 30 minutes. Assume an exponential distribution.\n\nNormalizationLessMore\n\n\n\n\n\n\n\n\nCareful!\n\n\n\n\\[\\text{Since } \\mu = \\displaystyle\\frac{1}{\\lambda} \\quad\\Rightarrow\\quad 30 = \\displaystyle\\frac{1}{\\lambda} \\quad\\Rightarrow\\quad \\lambda = \\displaystyle\\frac{1}{30}\\]\n\n\n\n\nCompute the probability that your friend will arrive in less than 25 minutes.\n\n\n\n\n\nCompute the probability that your friend will take more than 40 minutes to arrive."
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#memoryless-property",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#memoryless-property",
    "title": "12: Exponential Distribution",
    "section": "Memoryless Property",
    "text": "Memoryless Property\n\nExampleProof\n\n\nYou inherit an exquisite cabin in the woods, but on one condition: you must stay in the cabin overnight on the witch’s sabbath—Halloween. The cabin is notorious for housing the ghost of Cal Kulas, and he strikes sometime after the stroke of midnight with a mean time of 60 minutes.\n\nGiven that you have already waited 32 minutes to see the ghost, what is the probability that you will have to wait at least another 10 minutes?\nGiven that you have already waited 181 minutes to see the ghost, what is the probability that you will have to wait at least another 10 minutes?\n\n\n\nClaim: An exponential distribution has the memoryless property\n\\[P(T > a + b | T > b) = P(T > a)\\]\nProof:\n\\[\\begin{array}{rcl}\n      P(T > a + b | T > b) & = & \\displaystyle\\frac{ P(T > a + b \\text{ and } T > b) }{ P(T > b) } \\\\\n      ~ & = & \\displaystyle\\frac{ P(T > a+b) }{ P(T > b) } \\\\\n      ~ & = & \\displaystyle\\frac{ e^{-\\lambda(a+b)} }{ e^{-\\lambda b} } \\\\\n      ~ & = & e^{-\\lambda a} \\\\\n      ~ & = & P(T > a) \\\\\n    \\end{array}\\]\nClaim: The exponential distribution is the continuous distribution with the memoryless property.\nProof: (See Math 181)"
  },
  {
    "objectID": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#looking-ahead",
    "href": "posts/12_-_Exponential Distribution/Math_32_12_Exponential_Distribution.html#looking-ahead",
    "title": "12: Exponential Distribution",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 7:\n\nWHW5\nLHW5\nMid-Semester Survey\n\nExam 2 will be on Tues., Nov. 1\n\n\n\n\n\nart by @38mo1"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#development",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#development",
    "title": "13: Normal Distribution",
    "section": "Development",
    "text": "Development\n\n\nLet us start with the mother function \\(y = e^{-x^{2}}\\)\n\n\n\nsymmetric graph and function\nhorizontal asymptote\n\n\n\nNext, most of the scientific community accepts placing a horizontal scaling factor of 1/2 in the exponent\n\\[y = e^{-x^{2}/2} = \\text{exp}\\left(-\\displaystyle\\frac{x^{2}}{2}\\right)\\]\nto ensure that later calculations in the standard normal distribution have unit variance: \\(\\sigma^{2} = 1\\)"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#bell-curves",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#bell-curves",
    "title": "13: Normal Distribution",
    "section": "Bell Curves",
    "text": "Bell Curves\n\n\n\n\n\n\nSample Size Consideration\n\n\n\nFor elementary statistics courses, teachers say that using the normal distribution is a good idea when the sample size \\(n > 30\\).\n\n\nDepending on the situation, modeling with a normal distribution may need a transformation (moving the graph horizontally, stretching, etc.).\n\n\n\nbell curves"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#probability-density-function",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#probability-density-function",
    "title": "13: Normal Distribution",
    "section": "Probability Density Function",
    "text": "Probability Density Function\n\n\n\n\n\n\nStandard Normal Distribution\n\n\n\nHistorically, it was a good practice to pick one bell curve for calculations.\n\n\n\nFind the value of \\(k\\) so that \\(f(x) = ke^{-x^{2}/2}\\) is a probability density function."
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#probability-density-function-1",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#probability-density-function-1",
    "title": "13: Normal Distribution",
    "section": "Probability Density Function",
    "text": "Probability Density Function\n\n\n\n\n\n\nProbability Density Function\n\n\n\nAt this point, we have the probability density function (PDF) of the standard normal distribution, denoted by lower-case Greek letter phi:\n\\[\\text{PDF: } \\phi(z) = \\displaystyle\\frac{1}{\\sqrt{2\\pi}}e^{-z^{2}/2}\\]\n\n\n\n\n\nstandard normal distribution"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#parameters",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#parameters",
    "title": "13: Normal Distribution",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\nParameters\n\n\n\nWe will now find the parameters—mean and variance—for the standard normal distribution. We find the expected value with\n\\[\\text{E}[Z] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! z \\cdot \\phi(z) \\, dz = \\displaystyle\\frac{1}{\\sqrt{2\\pi}}\\displaystyle\\int_{-\\infty}^{\\infty} \\! z e^{-z^{2}/2} \\, dz = 0\\]\nThe second moment is\n\\[\\text{E}[Z^{2}] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! z^{2} \\cdot \\phi(z) \\, dz = \\displaystyle\\frac{1}{\\sqrt{2\\pi}}\\displaystyle\\int_{-\\infty}^{\\infty} \\! z^{2} e^{-z^{2}/2} \\, dz = \\displaystyle\\frac{ \\sqrt{2\\pi} }{ \\sqrt{2\\pi} } = 1\\]\nIt follows that the variance is also one unit, so the parameters of the standard normal distribution are \\(\\mu = 0\\) and \\(\\sigma^{2} = 1\\).\n\n\n\n\n\n\n\n\nNotation\n\n\n\nThe notation \\(X \\sim N(\\mu, \\sigma^{2})\\) says that random variable \\(X\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). For example, the standard normal distribution is denoted as \\(Z \\sim N(0,1)\\)\n\n\n\nWhat is the median of the standard normal distribution?\n\n\n\n\nmedian?"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#cumulative-distribution-function",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#cumulative-distribution-function",
    "title": "13: Normal Distribution",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function\n\n\n\n\n\n\nCDF\n\n\n\nThe cumulative distribution function (CDF) for the standard normal distribution is defined as the following integral function and denoted by upper-case Greek letter Phi:\n\\[\\Phi(z) = \\displaystyle\\frac{1}{\\sqrt{2\\pi}} \\displaystyle\\int_{-\\infty}^{z} \\! e^{-t^{2}/2} \\, dt\\]\n\n\n\n\nRecall: for continuous probability distributions, probabilities are the areas under the curve\n\n\n\narea under the curve\n\n\n\nHere in Math 32, instead of doing the integral (with the ``polar trick’’) or referring to a textbook standard normal distribution table, we will perform calculations for the normal distribution in terms of CDF \\(\\Phi\\)"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#empirical-rule",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#empirical-rule",
    "title": "13: Normal Distribution",
    "section": "Empirical Rule",
    "text": "Empirical Rule\nStatistics instructors like to make the following statements to guide intuition about the normal distribution and standard deviations.\n\n1 SD2 SDs3 SDs\n\n\nAbout 67% of data falls within one standard deviation of the mean.\n\n\n\nAbout 95% of data falls within 2 standard deviations of the mean.\n\n\n\nAbout 99% of data falls within 3 standard deviations of the mean."
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#optional-error-function",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#optional-error-function",
    "title": "13: Normal Distribution",
    "section": "(optional) Error Function",
    "text": "(optional) Error Function\n\n\n\n\n\n\n(optional) Error Function\n\n\n\n\n\nSome scientific literature refers to the area under the curve of the probability density function of the \\(X \\sim N(0, 1/2)\\) normal distribution as the \n\\[\\text{erf}(x) = \\displaystyle\\frac{2}{\\sqrt{\\pi}}\\int_{0}^{x} \\! e^{-t^{2}/2} \\, dt\\]\nand we can recover the CDF of the standard normal distribution with\n\\[\\Phi(x) = \\displaystyle\\frac{1}{2}\\left[  1 + \\text{erf}\\left(\\displaystyle\\frac{x}{\\sqrt{2}}\\right) \\right]\\]"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#general-normal-distribution",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#general-normal-distribution",
    "title": "13: Normal Distribution",
    "section": "General Normal Distribution",
    "text": "General Normal Distribution\n\n\n\n\n\n\nGeneral Normal Distribution\n\n\n\nWhen we model applications with \\(X \\sim N(\\mu, \\sigma^{2})\\), by applying the \\(z\\)-score transformation\n\\[z = \\displaystyle\\frac{x - \\mu}{\\sigma}\\]\nthe normal distribution has probability density function\n\\[\\text{PDF: } f(x; \\mu, \\sigma) = \\displaystyle\\frac{1}{\\sigma\\sqrt{2\\pi}} \\cdot e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}\\]\nand cumulative distribution function\n\\[F(x) = \\Phi\\left(\\displaystyle\\frac{x-\\mu}{\\sigma}\\right) = \\displaystyle\\frac{1}{2}\\left[1 + \\text{erf}\\left(\\displaystyle\\frac{x-\\mu}{\\sigma\\sqrt{2}}\\right)\\right]\\]\n\\[~\\]\nR code: pnorm(x, mu, sd)"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#applications-of-the-normal-distribution",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#applications-of-the-normal-distribution",
    "title": "13: Normal Distribution",
    "section": "Applications of the Normal Distribution",
    "text": "Applications of the Normal Distribution\n\nFewerMoreBetweenCharacterize\n\n\nSuppose that the incubation period—that is, the time between being infected with the virus and showing symptoms—for Covid-19 is normally distributed with a mean of 8 days and a standard deviation of 3 days. Find the probability that a randomly selected case demonstrated symptoms in fewer than 7 days.\n\n\n\nGirl Scout Thin Mint cookies have a mean size of 0.25 ounces. Find the probability that one randomly selected cookie has a size of more than 0.27 ounces if the standard deviation is 0.03 ounces. Assume a normal distribution.\n\n\n\nThe cones in the eye detect light. The absorption rate of cones is normally distributed. In particular, the “green” cones have a mean of 535 nanometers and a standard deviation of 65 nanometers. If an incoming ray of light has wavelengths between 550 and 575 nanometers, calculate the percentage of that ray of light that will be absorbed by the green cones.\n\n\n\nSuppose that the number of french fries in the batches at In-n-Out are normally distributed with a mean of 42 french fries and a standard deviation of 3.7 french fries. Your friend tells you that the In-n-Out employee is flirting with you if you end up with a french fry count in the top 5 percent. How should we characterize the top 5 percent of french fries?"
  },
  {
    "objectID": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#looking-ahead",
    "href": "posts/13_-_Normal Distribution/Math_32_13_Normal_Distribution.html#looking-ahead",
    "title": "13: Normal Distribution",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 14:\n\nWHW6\nLHW6\nInternet Connection Check (survey)\n\nExam 2 will be on Tues., Nov. 1\n\n\n\n\n\n\ntweet source"
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#joint-probability-mass-function",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#joint-probability-mass-function",
    "title": "14: Discrete Joint Distributions",
    "section": "Joint Probability Mass Function",
    "text": "Joint Probability Mass Function\n\n\n\n\n\n\nJoint Probability Mass Function\n\n\n\nThe joint probability mass function (joint PMF) to handle simultaneous calculations of random variables \\(X\\) and \\(Y\\) can be expressed as\n\n\n\\(X = \\{a_{1}, a_{2}, ..., a_{m}\\}\\)\n\\(Y = \\{b_{1}, b_{2}, ..., b_{n}\\}\\)\n\\(p_{ij} = P(X = a_{i}, Y = b_{j})\\)\n\n\n\n\n\n\n\n\n\nProperties\n\n\n\n\nEach probability is between zero and one inclusively\n\\[0 \\leq p_{ij} \\leq 1\\]\nAll probabilities add up to 100 percent\n\\[\\displaystyle\\sum_{i = 1}^{m}\\sum_{j = 1}^{n} p_{ij} = 1\\]\nAside: it is okay if the total is 0.99 or 1.01 (artifact of rounding errors)"
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#setting",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#setting",
    "title": "14: Discrete Joint Distributions",
    "section": "Setting",
    "text": "Setting\nThe setting for the examples in this lecture is The Lantern—our beloved coffee shop.\n\n\n\n\n\nLantern\n\n\n\n\n\\(X\\): number of beverages purchased by a customer\n\\(Y\\): number of snacks purchased by a customer"
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#joint-probability",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#joint-probability",
    "title": "14: Discrete Joint Distributions",
    "section": "Joint Probability",
    "text": "Joint Probability\n\nWhat is the probability that a randomly selected customer purchased one beverage and one snack?"
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#marginal-probability-mass-functions",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#marginal-probability-mass-functions",
    "title": "14: Discrete Joint Distributions",
    "section": "Marginal Probability Mass Functions",
    "text": "Marginal Probability Mass Functions\n\n\n\n\n\n\nMarginal Probability Mass Functions\n\n\n\nThe marginal probability mass functions with respect to \\(X\\) and \\(Y\\) respectively are\n\\[{\\color{blue}p_{X}(a_{i}) = \\displaystyle\\sum_{j = 1}^{n} p(a_{i}, b_{j})}, \\quad {\\color{red}p_{Y}(b_{j}) = \\displaystyle\\sum_{i = 1}^{m} p(a_{i}, b_{j})}\\]\n\n\nIn our example setting, we have the following joint PMF with marginal probabilities:\n\n\n\n\n\n\n\nMarginal Probability Mass Functions\n\n\n\n\n\nMore succinctly, the marginal probability mass function of \\(X\\) is\n\nand the marginal probability mass function of \\(Y\\) is\n\n\n\n\nWhat is the probability that a randomly selected customer purchased one beverage or one snack?"
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#conditional-probability",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#conditional-probability",
    "title": "14: Discrete Joint Distributions",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\nCompute the probability that a randomly selected customer purchases one snack given that the customer purchased zero beverages.\nCompute the probability that a randomly selected customer purchases a beverage given that the customer purchased two snacks."
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#conditional-expectation",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#conditional-expectation",
    "title": "14: Discrete Joint Distributions",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\n\n\n\n\n\n\nConditional Expectation\n\n\n\nThe concept of conditional probability can be extended into the concept of the expected value.\n\\[\\text{E}[{\\color{blue}A}| B = b_{j}] = \\displaystyle\\sum_{i = 1}^{m} {\\color{blue}a_{i}} \\cdot {\\color{red}P(a_{i} | B = b_{j})} = \\displaystyle\\sum_{i = 1}^{m} {\\color{blue}a_{i}} \\cdot {\\color{red}\\displaystyle\\frac{P(A = a_{i}, B = b_{j})}{P(B = b_{j})}}\\]\n\n\n\n\n\n\nWhat is the expected number of snacks purchased given that a customer purchases one beverage?"
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#joint-cumulative-distribution-function",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#joint-cumulative-distribution-function",
    "title": "14: Discrete Joint Distributions",
    "section": "Joint Cumulative Distribution Function",
    "text": "Joint Cumulative Distribution Function\n\n\n\n\n\n\nJoint Cumulative Distribution Function\n\n\n\nAs in the univariate case, the multivariate joint cumulative distribution function (joint CDF) is defined similarly as\n\\[F(a, b) = P(X \\leq a, Y \\leq b)\\]"
  },
  {
    "objectID": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#looking-ahead",
    "href": "posts/14_-_Discrete Joint Distributions/Math_32_14_Discrete_Joint_Distributions.html#looking-ahead",
    "title": "14: Discrete Joint Distributions",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 14:\n\nWHW6\nLHW6\nInternet Connection Check (survey)\n\nExam 2 will be on Tues., Nov. 1"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#joint-probability-density-function",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#joint-probability-density-function",
    "title": "15: Continuous Joint Distributions",
    "section": "Joint Probability Density Function",
    "text": "Joint Probability Density Function\n\n\n\n\n\n\nJoint Probability Density Function\n\n\n\nThe joint probability density function \\(f(x,y)\\) to handle simultaneous calculations of random variables \\(X\\) and \\(Y\\) can be expressed as\n\\[P({\\color{blue}a_{1} < X < a_{2}}, {\\color{red}b_{1} < Y < b_{2}}) = \\displaystyle{\\color{blue}\\int_{a_{1}}^{a_{2}}}{\\color{red}\\int_{b_{1}}^{b_{2}}} \\! {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx}\\]\n\n\n\n\n\n\n\n\nProperties\n\n\n\n\nEach probability is between zero and one inclusively\n\\[0 \\leq {\\color{purple}f(x,y)} \\leq 1$ \\text{ for all } {\\color{blue}$x$}, {\\color{red}$y$}\\]\nAll probabilities add up to 100 percent\n\\[{\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}}{\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{purple}f(x,y)} \\, {\\color{blue}dx} \\, {\\color{red}dy} = 1\\]"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#setting",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#setting",
    "title": "15: Continuous Joint Distributions",
    "section": "Setting",
    "text": "Setting\nFor the examples in this lecture session, we will model the queues at In-n-Out with random variables\n\n\n\n\n\nIn-n-Out\n\n\n\n\n\\(X\\): wait time to order food\n\\(Y\\): wait time to receive food\n\nand a function of the form\n\\[{\\color{purple}f(x,y)} = k{\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}},\\]\n\\[{\\color{blue}x > 0}, \\quad {\\color{red}y > 0}\\]"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#normalization",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#normalization",
    "title": "15: Continuous Joint Distributions",
    "section": "Normalization",
    "text": "Normalization\nFind the value of \\(k\\) so that \\(f\\) is a probability density function.\n\\[{\\color{purple}f(x,y)} = k{\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}}, \\quad {\\color{blue}x > 0}, \\quad {\\color{red}y > 0}\\]"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#joint-probability",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#joint-probability",
    "title": "15: Continuous Joint Distributions",
    "section": "Joint Probability",
    "text": "Joint Probability\n\\[{\\color{purple}f(x,y)} = \\displaystyle\\frac{1}{25}{\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}}, \\quad {\\color{blue}x > 0}, \\quad {\\color{red}y > 0}\\]\nCompute the probability that you will take {between 1 and 2 minutes to order} and wait {between 3 and 4 minutes to receive} your food."
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#joint-cumulative-distribution-function",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#joint-cumulative-distribution-function",
    "title": "15: Continuous Joint Distributions",
    "section": "Joint Cumulative Distribution Function",
    "text": "Joint Cumulative Distribution Function\n\n\n\n\n\n\nJoint Cumulative Distribution Function\n\n\n\nIn general, we handle the probability calculations with the joint cumulative distribution function \\(F(a,b)\\)\n% joint cumulative distribution function\n\\[{\\color{purple}F(a,b)} = P({\\color{blue}X \\leq a}, {\\color{red}Y \\leq b}) = {\\color{blue}\\displaystyle\\int_{-\\infty}^{a}}{\\color{red}\\displaystyle\\int_{-\\infty}^{b}} \\! {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx} \\]\n\n\n\n\n\n\n\n\nProperties\n\n\n\nWe can verify that the joint CDF starts at zero\n\\[{\\color{purple}F(0,0)} = 0\\]\nand that the joint CDF collects all probabilities\n\\[\\displaystyle\\lim_{a \\to \\infty, b \\to \\infty} F(a,b) = 1\\]\nIf need be, we can recover the joint PDF from the joint CDF as the mixed second-order partial derivatives\n\\[{\\color{purple}f(x,y)} = \\displaystyle\\frac{{\\color{purple}\\partial^{2}}}{{\\color{blue}\\partial x} {\\color{red}\\partial y}} {\\color{purple}F(x,y)}\\]\n\n\nWhat is the joint CDF for the In-n-Out setting?"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#marginal-probabilities",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#marginal-probabilities",
    "title": "15: Continuous Joint Distributions",
    "section": "Marginal Probabilities",
    "text": "Marginal Probabilities\n\n\n\n\n\n\nMarginal Cumulative Distribution Functions\n\n\n\nThe marginal cumulative distribution functions can be computed as\n% marginal CDF\n\\[\\begin{array}{rcl}\n  {\\color{blue}F_{X}(a)} & = & \\displaystyle\\lim_{{\\color{red}b \\to \\infty}} {\\color{purple}F(a,b)} \\\\\n  {\\color{red}F_{Y}(b)} & = & \\displaystyle\\lim_{{\\color{blue}a \\to \\infty}} {\\color{purple}F(a,b)} \\\\\n\\end{array}\\]\n\n\n\n\n\n\n\n\nProperties\n\n\n\nIntuition: the marginal CDF is seeking to analyze the probabilities in just one variable regardless of the other variables, so ``eliminate’’ the other varibles by taking their limits to infinity.\nWe can verify that the marginal CDFs start at zero\n\\[{\\color{blue}F_{X}(0)} = 0 \\text{ and } {\\color{red}F_{Y}(0)} = 0\\]\nand that the marginal CDFs collect all probabilities\n\\[\\displaystyle\\lim_{{\\color{blue}a \\to \\infty}} {\\color{blue}F_{X}(a)} = 1 \\text{ and } \\displaystyle\\lim_{{\\color{red}b \\to \\infty}} {\\color{red}F_{Y}(b)} = 1\\]\n\n\nWhat are the marginal CDFs for the In-n-Out setting?"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#marginal-probabilities-1",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#marginal-probabilities-1",
    "title": "15: Continuous Joint Distributions",
    "section": "Marginal Probabilities",
    "text": "Marginal Probabilities\n\n\n\n\n\n\nMarginal Cumulative Distribution Functions\n\n\n\nThe marginal probability density functions can be computed as\n\\[\\begin{array}{rcl}\n  {\\color{blue}f_{X}(x)} & = & {\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\\\\n  {\\color{red}f_{Y}(y)} & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{purple}f(x,y)} \\, {\\color{blue}dx} \\\\\n\\end{array}\\]\n\n\n\n\n\n\n\n\nIntuition\n\n\n\nIntuition: the marginal PDF is seeking to analyze the probabilities in just one variable regardless of the other variables, so ``integrate out’’ the other variables.\nAlternatively,\n\\[{\\color{blue}f_{X}(x) = \\displaystyle\\frac{d}{dx} F_{X}(x)} \\text{ and } {\\color{red}f_{Y}(y) = \\displaystyle\\frac{d}{dy} F_{Y}(y)} \\]\n\n\nWhat are the marginal PDFs for the In-n-Out setting?"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#marginal-expectation",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#marginal-expectation",
    "title": "15: Continuous Joint Distributions",
    "section": "Marginal Expectation",
    "text": "Marginal Expectation\n\n\n\n\n\nIn-N-Out\n\n\n\n\nWhat is the expected wait time to order food?\nWhat is the expected wait time to receive food?"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#independence",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#independence",
    "title": "15: Continuous Joint Distributions",
    "section": "Independence",
    "text": "Independence\n\n\n\n\n\n\nIndependence\n\n\n\nRecall that two events \\(A\\) and \\(B\\) are independent if\n\\[{\\color{purple}P(AB)} = {\\color{blue}P(A)} \\cdot {\\color{red}P(B)}\\]\n\n\nHere, the variables \\(X\\) and \\(Y\\) in the In-n-Out example were independent, which can be easily verified by noting that the integrals were separable.\n\\[\\begin{array}{rcl}\n  {\\color{purple}f(x,y)} & = & {\\color{blue}f_{X}(x)} \\cdot {\\color{red}f_{Y}(y)} \\\\\n  {\\color{red}\\displaystyle\\frac{1}{25}} {\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}} & = & {\\color{blue}xe^{-x}} \\cdot {\\color{red}\\displaystyle\\frac{y}{25}e^{-y/5}} \\\\\n\\end{array}\\]\nand\n\\[\\begin{array}{rcl}\n      {\\color{purple}F(a,b)} & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{a}} {\\color{red}\\displaystyle\\int_{-\\infty}^{b}} \\! {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx} \\\\\n      ~ & = & {\\color{red}\\displaystyle\\frac{1}{25}} {\\color{blue}\\displaystyle\\int_{0}^{a}} {\\color{red}\\displaystyle\\int_{0}^{b}} \\! {\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}} \\, {\\color{red}dy} \\, {\\color{blue}dx}\\\\\n      ~ & = & {\\color{red}\\displaystyle\\frac{1}{25}} {\\color{blue}\\left(\\displaystyle\\int_{0}^{a} xe^{-x} \\, dx \\right)} {\\color{red}\\left(\\displaystyle\\int_{0}^{b} \\! ye^{-y/5} \\, dy \\right)} \\\\\n      \\end{array}\\]\n\n\n\n\n\n\nDependence\n\n\n\nUpcoming lectures: dependent variables"
  },
  {
    "objectID": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#looking-ahead",
    "href": "posts/15_-_Continuous Joint Distributions/Math_32_15_Continuous_Joint_Distributions.html#looking-ahead",
    "title": "15: Continuous Joint Distributions",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 21:\n\nWHW7\nLHW7\n\nExam 2 will be on Tues., Nov. 1\n\n\n\n\n\n\ntweet source"
  },
  {
    "objectID": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#linear-operators",
    "href": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#linear-operators",
    "title": "16: Linear Operators",
    "section": "Linear Operators",
    "text": "Linear Operators\n\n\n\n\n\n\nLinear Operators\n\n\n\nWe say that \\(L\\) is a linear operator if\n\\[\\begin{array}{rcl}\n  L(a{\\color{blue}f(x)}) & = & aL({\\color{blue}f(x)}) \\\\\n  L({\\color{blue}f(x)} + {\\color{red}g(x)}) & = & L({\\color{blue}f(x)}) + L({\\color{red}g(x)}) \\\\\n\\end{array}\\]\n\n\n\n\n\n\n\n\nLinear Operators\n\n\n\nLoosely translated, \\(L\\) is a linear operator if\n\nwe can factor out a scalar multiple\nwe can split the operator across a sum or difference"
  },
  {
    "objectID": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#calculus-review",
    "href": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#calculus-review",
    "title": "16: Linear Operators",
    "section": "Calculus Review",
    "text": "Calculus Review\n\n\n\nShow that the derivative operator is a linear operator.\n\nShow that the integral operator is a linear operator.\n\n\n\n\n\n\n\n\n\n\n\nClaim: the derivative operator is a linear operator\n\n\n\n\n\nProof\n\\[\\begin{array}{rcl}\n      \\displaystyle\\frac{d}{dx}(a{\\color{blue}f(x)} + b{\\color{red}g(x)}) & = & \\displaystyle\\frac{d}{dx} a{\\color{blue}f(x)} + \\displaystyle\\frac{d}{dx} b{\\color{red}g(x)} \\\\\n      ~ & = &  a\\displaystyle\\frac{d}{dx} {\\color{blue}f(x)} + b\\displaystyle\\frac{d}{dx} {\\color{red}g(x)} \\\\\n    \\end{array}\\]\n\\[\\displaystyle\\frac{d}{dx}(a{\\color{blue}f(x)} + b{\\color{red}g(x)}) = a\\displaystyle\\frac{d}{dx} {\\color{blue}f(x)} + b\\displaystyle\\frac{d}{dx} {\\color{red}g(x)},\\]\nso \\(\\displaystyle\\frac{d}{dx}\\) is a linear operator.\n\n\n\n\n\n\n\n\n\nClaim: the integral operator is a linear operator\n\n\n\n\n\nProof\n\\[\\begin{array}{rcl}\n      \\displaystyle\\int \\! (a{\\color{blue}f(x)} + b{\\color{red}g(x)}) \\, dx & = & \\displaystyle\\int \\! a{\\color{blue}f(x)} \\, dx + \\displaystyle\\int \\! b{\\color{red}g(x)} \\, dx \\\\\n      ~ & = & a\\displaystyle\\int \\! {\\color{blue}f(x)} \\, dx + b\\displaystyle\\int \\! {\\color{red}g(x)} \\, dx \\\\\n    \\end{array}\\]\n\\[\\displaystyle\\int \\! (a{\\color{blue}f(x)} + b{\\color{red}g(x)}) \\, dx = a\\displaystyle\\int \\! {\\color{blue}f(x)} \\, dx + b\\displaystyle\\int \\! {\\color{red}g(x)} \\, dx,\\]\nso \\(\\displaystyle\\int\\) is a linear operator."
  },
  {
    "objectID": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#expected-value",
    "href": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#expected-value",
    "title": "16: Linear Operators",
    "section": "Expected Value",
    "text": "Expected Value\nIs the expectation operator \\(\\text{E}\\) a linear operator?\n\n\n\n\n\n\nClaim: \\(\\text{E}[aX] = a\\text{E}[X]\\)\n\n\n\n\n\nProof\n\\[\\text{E}[aX] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! ax \\cdot f(x) \\, dx = a{\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty} \\! x \\cdot f(x) \\, dx} = a{\\color{blue}\\text{E}[X]}\\]\nWe have shown that we can factor out a scalar multiple across the expectation operator.\n\n\n\n\n\n\n\n\n\nClaim: \\(\\text{E}[X + c] = \\text{E}[X] + c\\)\n\n\n\n\n\nProof\n\\[\\begin{array}{rcl}\n          \\text{E}[X + c] & = & \\displaystyle\\int_{-\\infty}^{\\infty} \\! (x + c) \\cdot f(x) \\, dx \\\\\n          ~ & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty} \\! x \\cdot f(x) \\, dx} + \\displaystyle\\int_{-\\infty}^{\\infty} \\! c \\cdot f(x) \\, dx\\\\\n          ~ & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty} \\! x \\cdot f(x) \\, dx} + c\\displaystyle\\int_{-\\infty}^{\\infty} \\! f(x) \\, dx \\\\\n          ~ & = & {\\color{blue}\\text{E}[X]} + c \\\\\n        \\end{array}\\]\nWe have shown that a horizontal shift of \\(c\\) units in the data also affects the expected value by \\(c\\) units\n\n\n\n\n\n\n\n\n\nClaim: \\(\\text{E}[X + Y] = \\text{E}[X] + \\text{E}[Y]\\)\n\n\n\n\n\nProof\n\\[\\begin{array}{rcl}\n          {\\color{purple}\\text{E}[X + Y]} & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}}{\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{purple}(x + y) \\cdot f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx} \\\\\n          ~ & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}}{\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{blue}x} \\cdot {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx}  + {\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}}{\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{red}y} \\cdot {\\color{purple}f(x,y)} \\, {\\color{blue}dx} \\, {\\color{red}dy} \\\\\n          ~ & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}\\! x \\cdot f_{X}(x) \\, dx}  + {\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty} \\! y \\cdot f_{Y}(y) \\, dy} \\\\\n          ~ & = & {\\color{blue}\\text{E}[X]} + {\\color{red}\\text{E}[Y]} \\\\\n        \\end{array}\\]\nWe have shown that the expected value of a sum \\ is the sum of the expected values.\n\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nCombining the above results, since\n\\[\\text{E}[aX + bY] = a\\text{E}[X] + b\\text{E}[Y]\\]\nwe have shown that the expectation operator \\(\\text{E}\\) is a linear operator.\nAlso,\n\\[\\text{E}[aX + bY + c] = a\\text{E}[X] + b\\text{E}[Y] + c\\]"
  },
  {
    "objectID": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#variance",
    "href": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#variance",
    "title": "16: Linear Operators",
    "section": "Variance",
    "text": "Variance\nIs the variance \\(\\text{Var}(X)\\) function a linear operator?\n\n\n\n\n\n\nClaim: \\(\\text{Var}(aX) = a\\text{Var}(X)\\)\n\n\n\n\n\nCounterpoint:\nRecall the ``practical formula for variance’’\n\\[\\text{Var}(X) = \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2}\\]\nand tracking the scaling factor proceeds as follows\n\\[\\begin{array}{rcl}\n          \\text{Var}(aX) & = & \\text{E}[(aX)^{2}] - \\left(\\text{E}[aX]\\right)^{2} \\\\\n          ~ & = & \\displaystyle\\int_{-\\infty}^{\\infty}\\! (ax)^{2} \\cdot f(x) \\, dx - \\left(a\\text{E}[X]\\right)^{2} \\\\\n          ~ & = & \\displaystyle\\int_{-\\infty}^{\\infty}\\! a^{2}x^{2} \\cdot f(x) \\, dx - a^{2}\\left(\\text{E}[X]\\right)^{2} \\\\\n          ~ & = & a^{2} \\left[ \\displaystyle\\int_{-\\infty}^{\\infty}\\! x^{2} \\cdot f(x) \\, dx - \\left(\\text{E}[X]\\right)^{2} \\right] \\\\\n          ~ & = & a^{2} \\left( \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2} \\right) \\\\\n          ~ & = & a^{2} \\text{Var}(X) \\\\\n        \\end{array}\\]\nWhen factoring out a scalar from the variance function, the factor is squared.\nFurthermore, since \\(\\text{Var}(aX) \\neq a\\text{Var}(X)\\), we have shown that the variance function is not a linear operator.\n\n\n\n\n\n\n\n\n\nClaim: \\(\\text{Var}(X + c) = \\text{Var}(X) + c\\)\n\n\n\n\n\nCounterpoint:\nRecall the ``practical formula for variance’’\n\\[\\begin{array}{rcl}\n          \\text{Var}(X + c) & = & \\text{E}[(X + c)^{2}] - \\left(\\text{E}[X + c]\\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2} + 2cX + c^{2}] - \\left(\\text{E}[X] + c \\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] + \\text{E}[2cX] + \\text{E}[c^{2}] - \\left(\\text{E}[X]\\right)^{2} - 2c\\text{E}[X] - c^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] + 2c\\text{E}[X] + c^{2} - \\left(\\text{E}[X]\\right)^{2} - 2c\\text{E}[X] - c^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2} \\\\\n          ~ & = & \\text{Var}(X) \\\\\n        \\end{array}\\]\nWe have shown that \\(\\text{Var}(X + c) = \\text{Var}(X)\\). That is, variance is not affected by a horizontal shift (phase shift)!\nFurthermore, since \\(\\text{Var}(X + c) \\neq \\text{Var}(X) + c\\), we have shown that the variance function is not a linear operator.\n\n\n\n\n\n\n\n\n\nClaim: \\(\\text{Var}(X + Y) = {\\color{blue}\\text{Var}(X)} + {\\color{red}\\text{Var}(Y)}\\)\n\n\n\n\n\nCounterpoint:\n\\[\\begin{array}{rcl}\n          \\text{Var}(X + Y) & = & \\text{E}[(X + Y)^{2}] - \\left(\\text{E}[X + Y]\\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2} + 2XY + Y^{2}] - \\left(\\text{E}[X] + \\text{E}[Y]\\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] + \\text{E}[2XY] + \\text{E}[Y^{2}] - \\left(\\text{E}[X]\\right)^{2}+ 2\\text{E}[X]\\text{E}[Y] + \\left(\\text{E}[Y]\\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2}  + \\text{E}[Y^{2}] - \\left(\\text{E}[Y]\\right)^{2} + 2\\text{E}[XY] - 2\\text{E}[X]\\text{E}[Y] \\\\\n          ~ & = & {\\color{blue}\\text{Var}(X)} + {\\color{red}\\text{Var}(Y)} + 2\\left( {\\color{purple}\\text{E}[XY] - \\text{E}[X]\\text{E}[Y] } \\right) \\\\\n        \\end{array}\\]\nWe have shown that \\(\\text{Var}(X + Y) \\neq \\text{Var}(X) + \\text{Var}(Y)\\). That is, the variance of the sum is not the sum of the variances (unless …?)\nFurthermore, since \\(\\text{Var}(X + Y) \\neq \\text{Var}(X) + \\text{Var}(Y)\\), we have shown that the variance function is not a linear operator.\n\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nWe have shown that the variance function is not a linear operator.\nNext time: working with\n\\[{\\color{purple}\\text{E}[XY] - \\text{E}[X]\\text{E}[Y]}\\]\nwhich is called the covariance!"
  },
  {
    "objectID": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#looking-ahead",
    "href": "posts/16_-_Linear Operators/Math_32_16_Linear_Operators.html#looking-ahead",
    "title": "16: Linear Operators",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 21:\n\nWHW7\nLHW7\n\nExam 2 will be on Tues., Nov. 1\n\n\n\ntweet source"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#setting",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#setting",
    "title": "17: Covariance and Correlation",
    "section": "Setting",
    "text": "Setting\n\n\nWe will once again visualize the act of ordering food at In-n-Out.\n\n\\(X\\): number of fries orders\n\\(Y\\): number of beef patties ordered\n\n\n\n\njoint PMF\n\n\n\n\n\n\nIn-n-Out"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#independence",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#independence",
    "title": "17: Covariance and Correlation",
    "section": "Independence",
    "text": "Independence\n\nAre \\(X\\) and \\(Y\\) independent?"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#covariance",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#covariance",
    "title": "17: Covariance and Correlation",
    "section": "Covariance",
    "text": "Covariance\n\n\n\n\n\n\nTrue or False? \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)\n\n\n\n\n\nFalse. In general,\n\\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\left( \\text{E}[XY] - \\text{E}[X]\\text{E}[Y] \\right)\\]\n\n\n\n\n\n\n\n\n\nMotivation for Independence\n\n\n\n\n\nAs you probably suspected, \\(\\text{Var}(X + Y)\\) does equal \\(\\text{Var}(X) + \\text{Var}(Y)\\) if \\(X\\) and \\(Y\\) are independent (exercise left to reader).\n\n\n\n\n\n\n\n\n\nCovariance\n\n\n\nWe define the covariance of random variables as\n\\[\\text{Cov}(X,Y) = \\text{E}[XY] - \\text{E}[X]\\text{E}[Y]\\]\n\n\n\n\n\n\n\n\nDerek’s Intuition\n\n\n\n\n\nAs an analogy, the random variables somewhat act like waves in that they can work together and grow or somewhat cancel each other out.\n\n\nImage source: https://www.physics-and-radio-electronics.com/physics/waveinterference.html\n\n\n\n\n\n\n\n\n\n\nThe Pythagorean Theorem of Statistics\n\n\n\n\n\n\n\nImage credit: Bioinformatics professor Dr. David Ardell"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#covariance-2",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#covariance-2",
    "title": "17: Covariance and Correlation",
    "section": "Covariance",
    "text": "Covariance\n\n\nCompute the covariance in the In-n-Out setting\nWhat are the units of the answer in this example?"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#correlation",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#correlation",
    "title": "17: Covariance and Correlation",
    "section": "Correlation",
    "text": "Correlation\n\n\n\n\n\n\nCorrelation\n\n\n\nJust like how the \\(z\\)-score is a standardized and unitless measure, the correlation was designed to be standardized and unitless (i.e. units cancel out).\n\\[r = \\text{Corr}(X,Y) = \\displaystyle\\frac{ \\text{Cov}(X,Y) }{ \\sqrt{ \\text{Var}(X) \\cdot \\text{Var}(Y)} }\\]\n\nIf \\(\\text{Var}(X) = 0\\), the data \\(X\\) are constant, and simply return \\(r = 0\\)\nIf \\(\\text{Var}(Y) = 0\\), the data \\(Y\\) are constant, and simply return \\(r = 0\\)\n\n\n\n\n\nCompute the correlation in the In-n-Out setting"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#interpretation-of-correlation",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#interpretation-of-correlation",
    "title": "17: Covariance and Correlation",
    "section": "Interpretation of Correlation",
    "text": "Interpretation of Correlation\n\n\n\n\n\n\nRanges\n\n\n\n\n\n\n\n\nRanges\n\n\nAside: the infinity-sized expected values might happen in continuous distributions.\n\n\n\n\n\n\n\n\n\nInterpretation of Correlation\n\n\n\nEarly development of the concept of correlation was done by Karl Pearson. Pearson suggested the following interpretations of the correlation (but there is no strict rule for this):\n\n\\(|r| < 0.4\\): virtually uncorrelated\n\\(0.4 \\leq |r| < 0.7\\): slightly correlated\n\\(0.7 \\leq |r| \\leq 1.0\\): strongly correlated"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#continuous-joint-probability-distribution-functions",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#continuous-joint-probability-distribution-functions",
    "title": "17: Covariance and Correlation",
    "section": "Continuous Joint Probability Distribution Functions",
    "text": "Continuous Joint Probability Distribution Functions\n\n\nWe will once again visualize the act of ordering food at In-n-Out.\n\n\\(X\\): number of fries orders\n\\(Y\\): number of beef patties ordered\n\nwith joint PDF\n\\[f(x,y) = \\frac{1}{30}(x + y)e^{-x}e^{-y/5}\\]\n\nAre \\(X\\) and \\(Y\\) independent?\nAre \\(X\\) and \\(Y\\) correlated?\n\n\n\n\n\nIn-n-Out"
  },
  {
    "objectID": "posts/17_-_Covariance/Math_32_17_Covariance.html#looking-ahead",
    "href": "posts/17_-_Covariance/Math_32_17_Covariance.html#looking-ahead",
    "title": "17: Covariance and Correlation",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 28:\n\nWHW8\nLHW8\n\nExam 2 will be on Tues., Nov. 1\n\nmore info in weekly announcement\n\n\n\n tweet source"
  },
  {
    "objectID": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#linear-conversion",
    "href": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#linear-conversion",
    "title": "18: Change of Variables",
    "section": "Linear Conversion",
    "text": "Linear Conversion\nLet \\(F\\) be the daily high temperature in Fahrenheit in Merced, California, with a mean of 76 degrees and a standard deviation of 15 degrees. Compute those sample statistics in Celsius.\n\n\n\n\n\n\nTemperature Conversion\n\n\n\nWe know that the conversion formula is\n\\[C = \\displaystyle\\frac{5}{9}(F - 32)\\]"
  },
  {
    "objectID": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#range-rule-of-thumb",
    "href": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#range-rule-of-thumb",
    "title": "18: Change of Variables",
    "section": "Range Rule of Thumb",
    "text": "Range Rule of Thumb\n\n\n\n\n\n\nRange Rule of Thumb\n\n\n\nRecall\n\nAbout 67 percent of data falls within one standard deviation of the mean\nAbout 95 percent of data falls within two standard deviations of the mean\n\n\\[\\left( \\mu - 2\\sigma, \\mu + 2\\sigma \\right)\\]\n\n\nWe had computed\n\n\\(\\mu_{F} \\approx 76\\) and \\(\\sigma_{F} \\approx 15\\) degrees Fahrenheit\n\\(\\mu_{C} \\approx 24.4444\\) and \\(\\sigma_{C} \\approx 8.3333\\) degrees Celsius\n\nBuild range-rule-of-thumb intervals for the Merced high temperatures in Fahrenheit and in Celsius."
  },
  {
    "objectID": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#distributions",
    "href": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#distributions",
    "title": "18: Change of Variables",
    "section": "Distributions",
    "text": "Distributions\nDetermine the distribution and density functions for\n\\[Y = \\displaystyle\\frac{5}{9}(X - 32)\\]"
  },
  {
    "objectID": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#change-of-coordinates",
    "href": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#change-of-coordinates",
    "title": "18: Change of Variables",
    "section": "Change of Coordinates",
    "text": "Change of Coordinates\n\n\n\n\n\n\nChange of Coordinates\n\n\n\nLet \\(X\\) be a continuous random variable with distribution function \\(F_{X}\\) and density function \\(f_{X}\\). If we apply a linear transformation\n\\[Y = aX + c\\]\nwhere \\(a >0\\) and \\(c\\) are constants, then\n\\[F_{Y}(y) = F_{X}\\left(\\displaystyle\\frac{y - c}{a}\\right) \\text{ and } f_{Y}(y) = \\displaystyle\\frac{1}{a}f_{X}\\left(\\displaystyle\\frac{y - c}{a}\\right)\\]\n\n\nIf \\(X \\sim Exp(1/2)\\), then what kind of distribution does \\(Y = 32X\\) have?"
  },
  {
    "objectID": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#nonlinear-transformations",
    "href": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#nonlinear-transformations",
    "title": "18: Change of Variables",
    "section": "Nonlinear Transformations",
    "text": "Nonlinear Transformations\n\nConcaveConvex\n\n\nLet \\(X \\sim U\\left(0, \\displaystyle\\frac{\\pi}{2}\\right)\\) and \\(Y = \\sin(X)\\).\nCompare \\(\\text{E}[\\sin X]\\) and \\(\\sin(\\text{E}[X])\\)\n\n\n\nSuppose that a disease outbreak can be modeled where \\(X\\) is the population density of a city and \\(Y\\) is the number of diagnosed cases with\n\\[X \\sim U(0,100), \\quad Y = X^{3.2}\\]\nCompare \\(\\text{E}[X^{3.2}]\\) and \\(\\left(\\text{E}[X]\\right)^{3.2}\\)\n\n\n\n\n\n\n\n\n\n\nJensen’s Inequality\n\n\n\nThe previous two examples were demonstrations of , which states that\n\nIf \\(g\\) is a convex function of random variable \\(X\\), then\n\n\\[g(\\text{E}[X]) \\leq \\text{E}[g(X)]\\]\n\nIf \\(g\\) is a concave function of random variable \\(X\\), then\n\n\\[g(\\text{E}[X]) \\geq \\text{E}[g(X)]\\]\nwhere the equal signs are not included when the function \\(g\\) is strictly convex or strictly concave."
  },
  {
    "objectID": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#looking-ahead",
    "href": "posts/18_-_Change in Variables/Math_32_18_Change_of_Variables.html#looking-ahead",
    "title": "18: Change of Variables",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Oct. 28:\n\nWHW8\nLHW8\n\nExam 2 will be on Tues., Nov. 1\n\nmore info in weekly announcement\n\nMath 32 discussion sections on Oct. 31 are optional\n\ncanceled for Nov. 2"
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#today-law-of-large-numbers",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#today-law-of-large-numbers",
    "title": "20: Law of Large Numbers",
    "section": "Today: Law of Large Numbers",
    "text": "Today: Law of Large Numbers\nGoal: start to understand error as it relates to sample size\nObjectives:\n\nDistribution of the mean \\(\\bar{X}_{n}\\)\nChebyshev’s Inequality\nLaw of Large Numbers"
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#i.i.d.",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#i.i.d.",
    "title": "20: Law of Large Numbers",
    "section": "i.i.d.",
    "text": "i.i.d.\n\n\n\n\n\n\ni.i.d.\n\n\n\nLet \\(X_{1}, X_{2}, X_{3}, ...\\) be an independent and identically distributed sequence of random variables (denoted ``i.i.d’’)\n\neach \\(X_{i}\\) is independent of each other\neach \\(X_{i}\\) has mean \\(\\mu\\)\neach \\(X_{i}\\) has variance \\(\\sigma^{2}\\)\n\n\n\nLet us now seek the distribution of the mean\n\\[\\bar{X}_{n} = \\displaystyle\\frac{X_{1} + X_{2} + ... + X_{n}}{n}\\]\n\nexpected value\nvariance"
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#distribution-of-mean",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#distribution-of-mean",
    "title": "20: Law of Large Numbers",
    "section": "Distribution of Mean",
    "text": "Distribution of Mean\n\n\n\n\n\n\nDistribution of Mean\n\n\n\nIf \\(\\bar{X}_{n}\\) is the mean of \\(n\\) independent and identical random variables, each with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\), then we can describe the distribution of \\(\\bar{X}_{n}\\) with\n\\[\\text{E}[\\bar{X}_{n}] = \\mu \\quad\\text{and}\\quad \\text{Var}(\\bar{X}_{n}) = \\displaystyle\\frac{\\sigma^{2}}{n}\\]"
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#far-from-the-mean",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#far-from-the-mean",
    "title": "20: Law of Large Numbers",
    "section": "Far from the Mean",
    "text": "Far from the Mean\n\n\n\n\n\n\nFar from the Mean\n\n\n\nIdea: we can get a sense of the probability that, for a particular boundary location \\(a\\), an observation lies outside of the interval\n\\[(\\mu - a, \\mu + a)\\]\n\n\n\n\\(\\mu\\): population average\n\\(a\\): tolerance\n\nClaim: \\(P(|X - \\mu| \\geq a) \\leq \\displaystyle\\frac{\\text{Var}(X)}{a^{2}}\\)"
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#chebyshevs-inequality",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#chebyshevs-inequality",
    "title": "20: Law of Large Numbers",
    "section": "Chebyshev’s Inequality",
    "text": "Chebyshev’s Inequality\n\n\n\n\n\n\nChebyshev’s Inequality\n\n\n\nFor a random variable \\(X\\) and boundary location \\(a\\),\n\\[P(|X - \\mu| \\geq a) \\leq \\displaystyle\\frac{\\text{Var}(X)}{a^{2}}\\]\n\n\nThat is, if we know the variance of a distribution, we can compute an upper bound for the probability of rare events!"
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#law-of-large-numbers",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#law-of-large-numbers",
    "title": "20: Law of Large Numbers",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\nThe Law of Large Numbers basically combines Chebyshev’s Inequality with the earlier work for the distribution of the mean:\n\n\\(\\text{E}[\\bar{X}_{n}] = \\mu \\quad\\text{and}\\quad \\text{Var}(\\bar{X}_{n}) = \\displaystyle\\frac{\\sigma^{2}}{n}\\)\n\\(P(|X - \\mu| \\geq a) \\leq \\displaystyle\\frac{\\text{Var}(X)}{a^{2}}\\)\n\nIdea: What happens when we observe a lot of data?\n\n\n\n\n\n\nLaw of Large Numbers\n\n\n\nTaking the limit as \\(n\\) goes to infinity, we arrive at the Law of Large Numbers:\n\\[\\displaystyle\\lim_{n \\to \\infty} P(|\\bar{X}_{n} - \\mu| \\geq a) \\leq \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{\\sigma^{2}}{a^{2} n} = 0\\]\n\n\nThat is, the probability that the mean of a sample of random variables is ``far’’ from the inherent expected value eventually goes to zero."
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#nerdy-example",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#nerdy-example",
    "title": "20: Law of Large Numbers",
    "section": "Nerdy Example",
    "text": "Nerdy Example\nHow many numbers between zero and one do we have to add up to have a sum that is greater than one?\n\nLet \\(X_{i} \\sim U(0,1)\\) be i.i.d.\nLet \\(Y\\) be the amount of \\(X_{i}\\) added together to get a sum greater than one\nFor a conservative estimate, suppose \\(Y \\sim U(2,6)\\), then\n\n\\[\\text{Var}(Y) = \\displaystyle\\frac{4}{3}\\]\n\nEmpirically (i.e. computer simulation), we saw convergence toward\n\n\\[\\bar{Y}_{n} = e \\approx 2.7183\\]\nHow many trials are needed so that the simulations converge to a mean within 0.01 of the true answer with at least 95 percent probability?"
  },
  {
    "objectID": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#looking-ahead",
    "href": "posts/20_-_Law of Large Numbers/Math_32_20_Law_of_Large_Numbers.html#looking-ahead",
    "title": "20: Law of Large Numbers",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\nWHW9\n\nFinal Exam will be on Dec. 8\n\n\ntweet source"
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#today-estimators",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#today-estimators",
    "title": "21: Estimators",
    "section": "Today: Estimators",
    "text": "Today: Estimators\nGoal: Explore generalization from samples to populations\nObjectives: Show the biased or unbiased estimation via\n\nsample mean \\(\\bar{x}\\)\nsample variance \\(s^{2}\\)\nsample standard deviation \\(s\\)"
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#demographics-example",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#demographics-example",
    "title": "21: Estimators",
    "section": "Demographics Example",
    "text": "Demographics Example\nFrom our Demographics Survey data of Math 32 students, suppose that the following is a sample of observations of heights (in inches):\n\\[\\{x_{11} = 72, x_{12} = 61, x_{13} = 60, x_{14} = 75, x_{15} = 69\\}\\]\n\nthen \\(t_{1} = 67.4\\) inches is the sample mean.\n\nSuppose that the following is another sample of heights:\n\\[\\{x_{21} = 66, x_{22} = 78, x_{23} = 78, x_{24} = 77, x_{25} = 64\\}\\]\n\nthen \\(t_{2} = 72.6\\) inches is the sample mean.\n\nSuppose that the following is another sample of heights:\n\\[\\{x_{31} = 61, x_{32} = 59, x_{33} = 70, x_{34} = 61, x_{35} = 65\\}\\]\n\nthen \\(t_{3} = 63.2\\) inches is the sample mean.\n\n\n\n\n\n\n\nSampling Itself is Probabilistic\n\n\n\n\n\nObserve: the sample mean (usually) changes upon a new set of observations\n\n\n\n\nCan we calculate the average height of UC Merced students?\nHow can we calculate the average height of UC Merced students?\n\nThought: what if we take a mean of the sample means?"
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#estimators",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#estimators",
    "title": "21: Estimators",
    "section": "Estimators",
    "text": "Estimators\n\n\n\n\n\n\nEstimators\n\n\n\nLet \\(T\\) be a random variable and \\(f\\) be some calculation\n\\[T = f(x_{1}, x_{2}, x_{3}, ...)\\]\nIf we are trying to estimate a population parameter \\(\\theta\\), we say that \\(T\\) is an unbiased estimator of \\(\\theta\\) if\n\\[\\text{E}[T] = \\theta\\]\n\n\nToday, we will look at situations where \\(f\\) is calculating the\n\nmean\nvariance\nstandard deviation"
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#mean",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#mean",
    "title": "21: Estimators",
    "section": "Mean",
    "text": "Mean\n\nSetupCodeSimulationProof\n\n\nWe will run simulations with \\(X \\sim U(0,1)\\) because we know what the answers should be. The population mean is\n\\[\\mu = \\displaystyle\\frac{a + b}{2} = \\displaystyle\\frac{1}{2}\\]\n\n\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- mean(these_numbers) #record average\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = 1/2, color = \"red\", size = 3) +\n  labs(title = \"Simulation Sample Mean\",\n       subtitle = paste(\"black: sample distribution\\nred: true population mean\\nmean of sample means: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLoosely speaking, since the sampling distribution “lines up” with the population mean, we say that the sample median is an unbiased estimator of the population mean.\n\n\n\\[\\begin{array}{rcl}\n      \\text{E}[\\bar{X}_{n}] & = & \\text{E}\\left( \\displaystyle\\frac{X_{1} + X_{2} + ... + X_{n}}{n} \\right) \\\\\n      ~ & = & \\displaystyle\\frac{1}{n}\\text{E}\\left( X_{1} + X_{2} + ... + X_{n} \\right) \\\\\n      ~ & = & \\displaystyle\\frac{1}{n}\\left(\\text{E}[X_{1}] + \\text{E}[X_{2}] + ... + \\text{E}[X_{n}] \\right) \\\\\n      ~ & = & \\displaystyle\\frac{1}{n}\\left(\\mu + \\mu + ... + \\mu \\right) \\\\\n      ~ & = & \\displaystyle\\frac{1}{n}\\left(n \\mu \\right) \\\\\n    \\end{array}\\]\nTherefore \\(\\text{E}[\\bar{X}_{n}] = \\mu\\)"
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#population-variance",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#population-variance",
    "title": "21: Estimators",
    "section": "Population Variance",
    "text": "Population Variance\n\nSetupCodeSimulation\n\n\nWe will run simulations with \\(X \\sim U(0,1)\\) because we know what the answers should be. The population variance is\n\\[\\sigma^{2} = \\displaystyle\\frac{(b-a)^{2}}{12} = \\displaystyle\\frac{1}{12}\\]\nWe will explore what happens if we apply the population variance formula\n\\[\\sigma^{2} = \\frac{1}{N}\\displaystyle\\sum_{i=1}^{N} (x_{i} - \\mu)^{2}\\]\nto samples.\n\n\n\n# user-defined function\npop_var <- function(x){\n  N <- length(!is.na(x)) #population size\n  mu <- mean(x, na.rm = TRUE) #population mean\n  \n  # return population mean (note use of \"N\")\n  sum( (x - mu)^2 ) / N\n}\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- pop_var(these_numbers) #record population variance\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = 1/12, color = \"red\", size = 3) +\n  labs(title = \"Simulation of Population Variances\",\n       subtitle = paste(\"black: sample distribution\\nred: true population variance\\nmean of population variances: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLoosely speaking, since the sampling distribution tends to underestimate the population variance, we say that the population variance (with \\(N\\)) is a biased estimator of the population variance."
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#bessels-correction",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#bessels-correction",
    "title": "21: Estimators",
    "section": "Bessel’s Correction",
    "text": "Bessel’s Correction\nCan we rescale the process for computing variance so that the operation is an unbiased estimator for the population variance?\nLet \\(X_{i}\\) be a set of \\(n\\) i.i.d. random variables from the same distribution with the same population variance \\(\\sigma^{2}\\). By independence, there is zero covariance.\nWe will compute the value of \\(k\\) so that\n\\[\\text{E}\\left[k \\cdot \\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X}_{n})^{2}}{n}\\right] = \\sigma^{2}\\]\nLemma: \\(\\text{Var}(X_{i} - \\bar{X}_{n}) = \\displaystyle\\frac{n-1}{n} \\cdot \\sigma^{2}\\)\n\n\n\n\n\n\nBessel’s Correction\n\n\n\n\n\nWe have derived the formula for the sample variance\n\\[S_{n}^{2} = \\displaystyle\\frac{1}{n-1}\\displaystyle\\sum_{i=1}^{n}(X_{i} - \\bar{X}_{n})^{2}\\]\nThat is, the “\\(n-1\\)” (Bessel’s correction) is in place so that the sample variance \\(s^{2}\\) is an unbiased estimator of the population variance \\(\\sigma^{2}\\)"
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#sample-variance",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#sample-variance",
    "title": "21: Estimators",
    "section": "Sample Variance",
    "text": "Sample Variance\n\nSetupCodeSimulation\n\n\nWe will run simulations with \\(X \\sim U(0,1)\\) because we know what the answers should be. The population variance is\n\\[\\sigma^{2} = \\displaystyle\\frac{(b-a)^{2}}{12} = \\displaystyle\\frac{1}{12}\\]\nWe will explore what happens if we apply the sample variance formula\n\\[s^{2} = \\frac{1}{n-1}\\displaystyle\\sum_{i=1}^{n} (x_{i} - \\mu)^{2}\\]\nto samples.\n\n\n\n# user-defined function\nsamp_var <- function(x){\n  n  <- length(!is.na(x)) #sample size\n  xbar <- mean(x, na.rm = TRUE) #sample mean\n  \n  # return population mean (note use of \"n-1\")\n  sum( (x - xbar)^2 ) / (n-1)\n}\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- samp_var(these_numbers) #record sample variance\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = 1/12, color = \"red\", size = 3) +\n  labs(title = \"Simulation of Sample Variances\",\n       subtitle = paste(\"black: sample distribution\\nred: true population variance\\nmean of sample variances: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLoosely speaking, since the sampling distribution “lines up” with the population variance, we say that the sample variance (with \\(n-1\\)) is an unbiased estimator of the population variance."
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#sample-standard-deviation",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#sample-standard-deviation",
    "title": "21: Estimators",
    "section": "Sample Standard Deviation",
    "text": "Sample Standard Deviation\n\nSetupCodeSimulationCommentary\n\n\nWe will run simulations with \\(X \\sim U(0,1)\\) because we know what the answers should be. The population standard deviation is\n\\[\\sigma = \\sqrt{\\displaystyle\\frac{(b-a)^{2}}{12}} = \\sqrt{\\displaystyle\\frac{1}{12}}\\]\nWe will explore what happens if we apply the sample variance formula\n\\[s = \\sqrt{\\frac{1}{n-1}\\displaystyle\\sum_{i=1}^{n} (x_{i} - \\mu)^{2}}\\]\nto samples.\n\n\n\n# user-defined function\nsamp_var <- function(x){\n  n  <- length(!is.na(x)) #sample size\n  xbar <- mean(x, na.rm = TRUE) #sample mean\n  \n  # return population mean (note use of \"n-1\")\n  sum( (x - xbar)^2 ) / (n-1)\n}\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- sqrt(samp_var(these_numbers)) #record sample standard deviation\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = sqrt(1/12), color = \"red\", size = 3) +\n  labs(title = \"Simulation of Sample Variances\",\n       subtitle = paste(\"black: sample distribution\\nred: true population variance\\nmean of sample standard deviations: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nLet \\(X_{i}\\) be a set of \\(n\\) i.i.d. random variables from the same distribution with the same population standard deviation \\(\\sigma\\). To avoid trivial situations, assume non-zero variance, so \\(\\sigma \\neq 0\\).\nIf \\(s = \\sqrt{ \\displaystyle\\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X}_{n})^{2}}{n-1} }\\) was an unbiased estimator, then \\(\\text{E}[s] = \\sigma\\)\nHowever, by Jensen’s Inequality, since \\(g(x) = x^{2}\\) is a convex function,\n\\[\\sigma^{2}  = \\text{E}[S_{n}^{2}] > \\left(\\text{E}[S_{n}]\\right)^{2}\\]\nand it follows that \\(\\text{E}[S_{n}] < \\sigma\\). Due to the underestimation, sample standard deviation \\(s\\) is a biased estimator of population standard deviation \\(\\sigma\\).\n\\[~\\]\nHowever, in practice, the discrepancy is usually so small that it is ignored."
  },
  {
    "objectID": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#looking-ahead",
    "href": "posts/21_-_Estimators/Math_32_21_Law_of_Large_Numbers.html#looking-ahead",
    "title": "21: Estimators",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\nWHW9\nWHW10\n(next LHW assignments will be given after Thanksgiving)\n\nFinal Exam will be on Dec. 8"
  },
  {
    "objectID": "posts/22_-_Central Limit Theorem/Math_32_22_Central_Limit_Theorem.html#today-central-limit-theorem",
    "href": "posts/22_-_Central Limit Theorem/Math_32_22_Central_Limit_Theorem.html#today-central-limit-theorem",
    "title": "22: Central Limit Theorem",
    "section": "Today: Central Limit Theorem",
    "text": "Today: Central Limit Theorem\nGoal: Consolidate our understanding of variance\nObjectives: Update our understanding of the normal distribution"
  },
  {
    "objectID": "posts/22_-_Central Limit Theorem/Math_32_22_Central_Limit_Theorem.html#bringing-it-all-together",
    "href": "posts/22_-_Central Limit Theorem/Math_32_22_Central_Limit_Theorem.html#bringing-it-all-together",
    "title": "22: Central Limit Theorem",
    "section": "Bringing it all Together",
    "text": "Bringing it all Together\n\n\n\n\n\n\nLinear Operators\n\n\n\nRecall, from our studies into linear operators,\n\n\\(\\text{E}[aX + bY + c] = a\\text{E}[X] + b\\text{E}[Y] + c\\)\n\\(\\text{Var}[aX + bY + c] = a^{2}\\text{Var}[X] + b^{2}\\text{Var}[Y] + ab\\text{Cov}(X,Y)\\)\n\nLet \\(X_{i}\\) be a set of \\(n\\) i.i.d. random variables from the same distribution with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). By induction, we can extrapolate these results for summations:\n\n\\(\\text{E}\\left[ \\displaystyle\\sum_{i=1}^{n} X_{i} \\right] = \\displaystyle\\sum_{i=1}^{n}\\text{E}\\left[ X_{i} \\right] = n\\mu\\)\n\\(\\text{Var}\\left[ \\displaystyle\\sum_{i=1}^{n} X_{i} \\right] = \\displaystyle\\sum_{i=1}^{n}\\text{Var}\\left[ X_{i} \\right] = n\\sigma^{2}\\)\n\nRecall, independence implies zero covariance.\n\n\n\n\n\n\n\n\nLaw of Large Numbers\n\n\n\nRecall, from our look at the Law of Large Numbers, for the sample mean of random variables \\(\\bar{X}_{n}\\),\n\n\\(\\text{E}[\\bar{X}_{n}] = \\mu\\)\n\\(\\text{Var}(\\bar{X}_{n}) = \\displaystyle\\frac{\\sigma^{2}}{n}\\)\n\n\n\n\n\n\n\n\n\nEstimators\n\n\n\nFinally,\n\nsample mean \\(\\bar{X}_{n}\\) is an unbiased estimator of population mean \\(\\mu\\)\nsample variance \\(S_{n}^{2}\\) is an unbiased estimator of population variance \\(\\sigma^{2}\\)\n\nand the sample standard deviation \\(s\\) is almost an unbiased estimator of the population standard deviation \\(\\sigma\\)"
  },
  {
    "objectID": "posts/22_-_Central Limit Theorem/Math_32_22_Central_Limit_Theorem.html#summation",
    "href": "posts/22_-_Central Limit Theorem/Math_32_22_Central_Limit_Theorem.html#summation",
    "title": "22: Central Limit Theorem",
    "section": "Summation",
    "text": "Summation\n\nLarger VarianceExample\n\n\nFollowing from how \\(\\text{Var}\\left[ \\displaystyle\\sum_{i=1}^{n} X_{i} \\right] = n\\sigma^{2}\\), it follows that the standard deviation for a summation is\n\\[\\sigma_{n} = \\sigma\\sqrt{n}\\]\nThink: as sample size \\(n\\) increases, does \\(\\sigma_{n}\\) increase or decrease?\n\\[z\\text{-score:} \\quad z = \\displaystyle\\frac{x - \\mu}{\\sigma} \\quad\\rightarrow\\quad Z_{n} = \\displaystyle\\frac{\\sum X_{i} - n\\mu}{\\sigma_{n}} = \\displaystyle\\frac{\\sum X_{i} - n\\mu}{\\sigma\\sqrt{n}}\\]\n\n\nFor Covid-19, the population statistics for sick patients are a mean of \\(\\mu = 4\\) days and standard deviation \\(\\sigma = 2\\) days (assuming a normal distribution since the number of confirmed cases is numerous). For an incoming case load of 9 sick patients, what is the probability that they need at least 32 days combined in the hospital?"
  },
  {
    "objectID": "posts/22_-_Central Limit Theorem/Math_32_22_Central_Limit_Theorem.html#central-limit-theorem",
    "href": "posts/22_-_Central Limit Theorem/Math_32_22_Central_Limit_Theorem.html#central-limit-theorem",
    "title": "22: Central Limit Theorem",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nSmaller VarianceExample\n\n\nFollowing from how \\(\\text{Var}\\left[ \\bar{X}_{n} \\right] = \\displaystyle\\frac{\\sigma^{2}}{n}\\), it follows that the standard deviation for an average is\n\\[\\sigma_{n} = \\displaystyle\\frac{\\sigma}{\\sqrt{n}}\\]\n\nThis \\(\\displaystyle\\frac{\\sigma}{\\sqrt{n}}\\) is also called the standard error.\n\nThink: as sample size \\(n\\) increases, does \\(\\sigma_{n}\\) increase or decrease?\n\\[z\\text{-score:} \\quad z = \\displaystyle\\frac{x - \\mu}{\\sigma} \\quad\\rightarrow\\quad Z_{n} = \\displaystyle\\frac{\\bar{X}_{n} - \\mu}{\\sigma_{n}} = \\displaystyle\\frac{\\bar{X}_{n} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\]\n\n\nFor Covid-19, the population statistics for the incubation period are a mean of \\(\\mu = 8\\) days and standard deviation \\(\\sigma = 3\\) days (assuming a normal distribution since the number of confirmed cases is numerous). For a sample of 25 infected people, what is the probability that their average incubation period is fewer than 7 days?"
  },
  {
    "objectID": "posts/22_-_Central Limit Theorem/Math_32_22_Central_Limit_Theorem.html#looking-ahead",
    "href": "posts/22_-_Central Limit Theorem/Math_32_22_Central_Limit_Theorem.html#looking-ahead",
    "title": "22: Central Limit Theorem",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nUpcomingExam 1 DistExam 2 Dist\n\n\n\nWHW9\nWHW10\n(next LHW assignments will be given after Thanksgiving)\n\nFinal Exam will be on Dec. 8"
  },
  {
    "objectID": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#today-maximum-likelihood",
    "href": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#today-maximum-likelihood",
    "title": "23: Maximum Likelihood",
    "section": "Today: Maximum Likelihood",
    "text": "Today: Maximum Likelihood\nGoal: Modify distribution parameters based on observed data\nObjectives:\n\nderive maximum likelihood estimate for the exponential distribution\nderive maximum likelihood estimate for the Poisson distribution"
  },
  {
    "objectID": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#notation",
    "href": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#notation",
    "title": "23: Maximum Likelihood",
    "section": "Notation",
    "text": "Notation\n\n\n\n\n\n\nNotation\n\n\n\nRecall,\n\nLower-case \\(\\{x_{1}, x_{2}, x_{3}, ..., x_{n}\\}\\) is a set of observations\nUpper-case \\(\\{X_{1}, X_{2}, X_{3}, ..., X_{n}\\}\\) is a set of random variables (i.e. a data set)\nTreating \\(\\{X_{1}, X_{2}, ..., X_{n}\\}\\) as a set of \\(n\\) i.i.d. (independent and identically distributed) random variables is a common assumption.\nWith independence,\n\\[P(X_{1}, X_{2}, ..., X_{n}) = P(X_{1}) \\cdot P(X_{2}) \\cdot ... \\cdot P(X_{n})\\]\nEach individual probability is computed (at least theoretically) with a PDF (probability density function)\n\\[P(x_{i}) = f_{X}(x_{i})\\]"
  },
  {
    "objectID": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#inverse",
    "href": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#inverse",
    "title": "23: Maximum Likelihood",
    "section": "Inverse",
    "text": "Inverse\n\n\n\n\n\n\nInverse\n\n\n\nSuppose that we have a sample of data \\(\\{x_{1}, x_{2}, x_{3}, ..., x_{n}\\}\\). Now we want to model with a probability distribution, but we need to figure out the distribution’s parameters. Let us think about this in a Bayesian way:\n\\[{\\color{purple}{P(\\text{model} | \\text{data})}} = \\displaystyle\\frac{ {\\color{blue}{P(\\text{data} | \\text{model})} \\cdot P(\\text{model})} }{ {\\color{red}{P(\\text{data})}} }\\]\n\n\\({\\color{purple}{P(\\text{model} | \\text{data})}}\\) is the posterior probability that we want\n\\({\\color{blue}{P(\\text{data} | \\text{model})}}\\) is a likelihood\nSince the prior probability \\({\\color{red}{P(\\text{data})}}\\) is a constant …\n\n… we say that the posterior probability is proportional to the likelihood."
  },
  {
    "objectID": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#likelihood",
    "href": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#likelihood",
    "title": "23: Maximum Likelihood",
    "section": "Likelihood",
    "text": "Likelihood\n\nDefinitionExample\n\n\n\n\n\n\n\n\nLikelihood Function\n\n\n\nLet the likelihood function, in terms of a parameter \\(\\theta\\), be the joint probability\n\\[L(\\theta) = P(X_{1} = x_{1}, X_{2} = x_{2}, ..., X_{n} = x_{n}) = f_{X}(x_{1}) \\cdot f_{X}(x_{2}) \\cdots f_{X}(x_{n})\\]\nor\n\\[L\\left(\\theta; \\left\\{x_{i}\\right\\}_{i=1}^{n}\\right) = \\displaystyle\\prod_{i = 1}^{n} f_{X}(x_{i})\\]\n\n\n\n\nSuppose that we have data for how long a certain type and brand of light bulb operated (in the same working conditions), and that data in months was\n\\[6, \\quad 18, \\quad 29, \\quad 44, \\quad 48\\]\nGoal: characterize the top 5 percent of light bulbs.\n\nBuild the likelihood function assuming an exponential distribution.\nCompute the likelihood that \\(\\mu = 25\\).\nCompute the likelihood that \\(\\mu = 50\\)."
  },
  {
    "objectID": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#log-likelihood",
    "href": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#log-likelihood",
    "title": "23: Maximum Likelihood",
    "section": "Log Likelihood",
    "text": "Log Likelihood\n\nLogarithmsExample\n\n\n\n\n\n\n\n\nLogarithms\n\n\n\nYou know that logarithms make large numbers smaller. More precisely,\n\\[\\ln(x) < x, \\quad x > 1\\]\nExample: \\(\\ln(1234) \\approx 7.1180\\)\nDid you know that logarithms make small numbers larger (in size). More precisely,\n\\[|\\ln(x)| > x, \\quad 0 < x < 1\\]\nExample: \\(|\\ln(0.1234)| \\approx 2.0923\\)\nFrom pre-calculus, recall the properties of logarithms:\n\\[\\ln(AB) = \\ln(A) + \\ln(B), \\quad \\ln\\left(\\displaystyle\\frac{A}{B}\\right) = \\ln A - \\ln B, \\quad \\ln(A^{c}) = c\\ln A\\]\n\n\n\n\nFor modeling with the exponential distribution, we saw that the likelihood function was\n\\[L\\left(\\lambda; \\{x_{i}\\}_{i=1}^{n}\\right) = \\displaystyle\\prod_{i=1}^{n} f_{X}(x_{i}) = \\lambda^{n}e^{-\\lambda\\sum x_{i}}\\]\nWe take the natural logarithm to compute the log-likelihood function\n\\[\\ell\\left(\\lambda; \\{x_{i}\\}_{i=1}^{n}\\right) = \\ln L\\left(\\lambda; \\{x_{i}\\}_{i=1}^{n}\\right) = n\\ln\\lambda - \\lambda\\displaystyle\\sum_{i=1}^{n} x_{i}\\]\n\nCompute the log-likelihood that \\(\\mu = 25\\).\nCompute the log-likelihood that \\(\\mu = 50\\)."
  },
  {
    "objectID": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#maximum-likelihood",
    "href": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#maximum-likelihood",
    "title": "23: Maximum Likelihood",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nDefinitionVisualsExample 1Example 2\n\n\n\n\n\n\n\n\nLikelihood Function\n\n\n\n Given a data set \\(\\{x_{1}, x_{2}, ..., x_{n}\\}\\), we seek the desired parameter(s) that makes realizing the data set most likely.\n\\[L\\left(\\theta; \\left\\{x_{i}\\right\\}_{i=1}^{n}\\right) = \\displaystyle\\prod_{i = 1}^{n} f_{X}(x_{i})\\]\n\n\n\n\n\n\n\n\nMaximization\n\n\n\nFrom calculus, recall that the main step in maximizing the value of a function is setting the first derivative equal to zero.\n\n\n\n\n \n\n\nGiven a data set \\(\\{x_{1}, x_{2}, ..., x_{n}\\}\\), assume an \\(\\text{Exp}(\\lambda)\\) distribution.\n\nCompute the value of rate parameter \\(\\lambda\\) that maximizes the likelihood of the data set.\nCompute the likelihood at the maximum likelihood estimate (MLE).\nCharacterize the top 5 percent of light bulbs.\n\n\n\nGiven a data set \\(\\{x_{1}, x_{2}, ..., x_{n}\\}\\), assume an \\(\\text{Pois}(\\lambda)\\) distribution. Compute the value of parameter \\(\\lambda\\) that maximizes the likelihood of the data set."
  },
  {
    "objectID": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#estimators-revisited",
    "href": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#estimators-revisited",
    "title": "23: Maximum Likelihood",
    "section": "Estimators Revisited",
    "text": "Estimators Revisited\nIf we sample from a theoretical \\(U(0, M)\\) distribution, the sample maximum \\(s_{M}\\) of each sample is less than or equal to \\(M\\)\n\\[s_{M} \\leq M\\]\nIt would follow that the average of the sample maxima underestimates the true maximum\n\\[\\text{E}[s_{M}] \\leq M\\]\nTherefore the sample maximum is a biased estimator of the true maximum.\n\\[~\\]\nSimilarly, the sample minimum \\(s_{m}\\) from a \\(U(m, 0)\\) distribution overestimates\n\\[\\text{E}[s_{m}] \\geq m\\]\nTherefore the sample minumum is a biased estimator of the true minumum."
  },
  {
    "objectID": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#looking-ahead",
    "href": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#looking-ahead",
    "title": "23: Maximum Likelihood",
    "section": "Looking Ahead",
    "text": "Looking Ahead"
  },
  {
    "objectID": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#upcoming",
    "href": "posts/23_-_Maximum Likelihood/Math_32_23_Maximum_Likelihood.html#upcoming",
    "title": "23: Maximum Likelihood",
    "section": "Upcoming",
    "text": "Upcoming\n\nWHW10\n(next LHW assignments will be given after Thanksgiving)\n\nFinal Exam will be on Dec. 8"
  },
  {
    "objectID": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#today-linear-regression",
    "href": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#today-linear-regression",
    "title": "24: Linear Regression",
    "section": "Today: Linear Regression",
    "text": "Today: Linear Regression\nGoal: Summarize bivariate data\nObjectives:\n\ndetermine a best-fit line from a bivariate data set\nmake predictions based on a linear regression model"
  },
  {
    "objectID": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#moxillation",
    "href": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#moxillation",
    "title": "24: Linear Regression",
    "section": "Moxillation",
    "text": "Moxillation\n\nQuery: predict how much moxillation will take place at 70 traxolline."
  },
  {
    "objectID": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#residuals",
    "href": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#residuals",
    "title": "24: Linear Regression",
    "section": "Residuals",
    "text": "Residuals\n\n\nGoal: Given a bivariate data set \\(\\{x_{i}, y_{i}\\}_{i=1}^{n}\\), form a linear regression model\n\\[\\hat{y} = a + bx\\]\nthat ``best fits’’ the data. Note that such a line will not go through all of the data (except in linear, deterministic situations), so\n\ndenote \\(y_{i}\\) for true outcomes\ndenote \\(\\hat{y}_{i}\\) for estimates (or predictions)\nthen \\(y_{i} - \\hat{y}_{i}\\) is the \\(i^{\\text{th}}\\) residual\n\n\n\n\n\nimage credit: www.jmp.com"
  },
  {
    "objectID": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#method-of-least-squares",
    "href": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#method-of-least-squares",
    "title": "24: Linear Regression",
    "section": "Method of Least Squares",
    "text": "Method of Least Squares\nLike our derivation of formulas for variance and standard deviation, scientists decided to square the residuals (focus on size of residuals, avoid positive versus negative signs). Let the total error be\n\\[E(a,b) = \\displaystyle\\sum_{i = 1}^{n} (y_{i} - \\hat{y}_{i})^{2} = \\displaystyle\\sum_{i = 1}^{n} (y_{i} - a - bx_{i})^{2} \\]\n\nThe ``best-fit line’’ minimizes the error.\nTo minimize the error, start by setting the partial derivatives equal to zero:\n\n\\[\\displaystyle\\frac{\\partial E}{\\partial a} = 0, \\quad \\displaystyle\\frac{\\partial E}{\\partial b} = 0\\]\nThankfully, the function \\(E(a,b)\\) is an elliptical paraboloid, so there is a global minimum at the critical point, and that minimum is found where\n\\[a = \\displaystyle\\frac{ (\\sum y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }, \\quad b = \\displaystyle\\frac{ n\\sum x_{i}y_{i} - (\\sum x_{i})(\\sum y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }\\]\n\n\n\n\n\n\nBest-Fit Linear Regression Model\n\n\n\nIf sample means \\(\\bar{x}\\) and \\(\\bar{y}\\), sample standard deviations \\(s_{x}\\) and \\(s_{y}\\), and correlation coefficient \\(r\\) were previously computed, then the best-fit linear regression line \\(\\hat{y} = mx + b\\) is computed with\n\\[m = \\displaystyle\\frac{ rs_{y} }{ s_{x} }, \\quad b = \\bar{y} - m\\bar{x}\\]\n\nIf correlation \\(r > 0\\), then the slope of the regression line is also positive\nIf correlation \\(r < 0\\), then the slope of the regression line is also negative\n\n\n\nIn a scatterplot, an outlier is a point lying far away from the other data points. Paired sample data may include one or more influential points, which are points that strongly affect the graph of the regression line."
  },
  {
    "objectID": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#estimators-for-the-coefficients",
    "href": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#estimators-for-the-coefficients",
    "title": "24: Linear Regression",
    "section": "Estimators for the Coefficients",
    "text": "Estimators for the Coefficients\n\n\n\n\n\n\nEstimators for the Coefficients\n\n\n\nClaim: Treating \\(Y\\) as a random variable for the true outcomes, the least-squares estimators\n\\[A = \\displaystyle\\frac{ (\\sum Y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}Y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }, \\quad B = \\displaystyle\\frac{ n\\sum x_{i}Y_{i} - (\\sum x_{i})(\\sum Y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }\\]\nare unbiased estimators of \\(a\\) and \\(b\\) respectively.\n\n\nProof: We need to show that the expected values \\(\\text{E}[A] = a\\) and \\(\\text{E}[B] = b\\)\n\\[\\begin{array}{rcl}\n  \\text{E}[B] & = & \\displaystyle\\frac{ n\\sum x_{i}\\text{E}[Y_{i}] - (\\sum x_{i})(\\sum \\text{E}[Y_{i}]) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n  ~ & = & \\displaystyle\\frac{ n\\sum x_{i}(a + bx_{i}) - (\\sum x_{i})[\\sum (a + bx_{i})] }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n  ~ & = & \\displaystyle\\frac{ n\\sum x_{i}(a + bx_{i}) - (\\sum x_{i})(na + b\\sum x_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n  ~ & = & \\displaystyle\\frac{ an\\sum x_{i} + bn\\sum x_{i}y_{i} - an\\sum x_{i} + b(\\sum x_{i})^{2} }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n  ~ & = & \\displaystyle\\frac{ b[n\\sum x_{i}^{2} - (\\sum x_{i})^{2}] }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n\\end{array}\\]\nWe have shown that expected value \\(\\text{E}[B] = b\\), so \\(B\\) is an unbiased estimator of \\(b\\).\nThen,\n\\[\\begin{array}{rcl}\n  \\text{E}[A] & = & \\text{E}[\\bar{Y}_{n}] - \\text{E}[B]\\bar{x} \\\\\n  ~ & = & \\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} \\text{E}[Y_{i}] - b\\bar{x} \\\\\n  ~ & = & \\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} (a + bx_{i}) - b\\bar{x} \\\\\n  ~ & = & a + b\\bar{x} - b\\bar{x} \\\\\n\\end{array}\\]\nWe have shown that expected value \\(\\text{E}[A] = a\\), so \\(A\\) is an unbiased estimator of \\(a\\)."
  },
  {
    "objectID": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#estimator-for-the-variance",
    "href": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#estimator-for-the-variance",
    "title": "24: Linear Regression",
    "section": "Estimator for the Variance",
    "text": "Estimator for the Variance\n\nthis slide is optional (i.e. not on exams)\n\n\n\n\n\n\n\nEstimator for the Variance\n\n\n\n\nBut can we estimate the population variance \\(\\sigma_{y}^{2}\\) without knowing the relationship between \\(x\\) and \\(y\\)?\nStarting with the average of the squared residuals,\n\\[R_{n}^{2} = \\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} (Y_{i} - a - bx_{i})^{2}\\]\nit can be computed that expected value \\(\\text{E}[R_{n}^{2}] = \\displaystyle\\frac{n-2}{n}\\sigma_{y}^{2}\\)\nRescaling,\n\\[S_{n}^{2} = \\displaystyle\\frac{1}{n-2}\\displaystyle\\sum_{i=1}^{n} (Y_{i} - a - bx_{i})^{2}\\]\nis an unbiased estimator of the population variance \\(\\sigma_{y}^{2}\\)"
  },
  {
    "objectID": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#predictions",
    "href": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#predictions",
    "title": "24: Linear Regression",
    "section": "Predictions",
    "text": "Predictions\n\n\n\n\n\n\nPredictions\n\n\n\nFinally, with a linear regression model \\(\\hat{y} = a + bx\\) in place, plug in a data value \\(x\\) to form a prediction \\(\\hat{y}\\)\n\n\n\nExample 1Example 2\n\n\nThe table below displays data for enrollment levels at UC Merced between the years 2011 and 2016.\n\nIf the enrollment numbers are \\(\\{x_{i}\\}\\) data, then the sample mean is \\(\\bar{x} = 6240.5\\) students and the sample standard deviation is \\(s_{x} = 737.3091\\) students. If the drug abuse violations are \\(\\{y_{i}\\}\\) data, then the sample mean is \\(\\bar{y} = 22.5\\) violations and the sample standard deviation is \\(s_{y} = 9.7108\\) violations. Finally, the correlation coefficient is \\(r \\approx 0.0471\\).\n\nFind the best-fit linear regression line in the \\(\\hat{y} = mx + b\\) form.\nWhat does the regression model predict for the number of drug abuse violations for the year 2017 enrollment of 7967 students?\nWhat does the regression model predict for the enrollment if there are 36 drug abuse violations reported?\n\n\n\n\n\n\n\nJoyce Byers notices that magnets are falling off the shelves. She consults the kids’ science teacher Mr. Clarke. Suppose that they conduct an investigation where they strength of magnetic fields versus the distance from Joyce’s house and then record the amounts (in the table below). Let us treat the distances as the \\(\\{x_{i}\\}\\) data and the magnetic flux density as the \\(\\{y_{i}\\}\\) data. Joyce fortunately has a ``scientific calculator’’ from the government lab, and she calculates the sample means of \\(\\bar{x} = 15\\) miles and \\(\\bar{y} \\approx 8.9999\\) teslas. Mr. Clarke then calculates sample standard deviations of \\(s_{x} \\approx 1.9850\\) and \\(s_{y} \\approx 1.3742\\) along with a correlation of \\(r \\approx 0.9630\\). Build a linear regression model to predict the magnetic flux density when they are 18 miles away from Joyce’s house."
  },
  {
    "objectID": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#looking-ahead",
    "href": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#looking-ahead",
    "title": "24: Linear Regression",
    "section": "Looking Ahead",
    "text": "Looking Ahead"
  },
  {
    "objectID": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#upcoming",
    "href": "posts/24_-_Linear Regression/Math_32_24_Linear_Regression.html#upcoming",
    "title": "24: Linear Regression",
    "section": "Upcoming",
    "text": "Upcoming\n\nLHW9\nLHW10\n\nFinal Exam will be on Dec. 8\n\nmore information in weekly announcement"
  },
  {
    "objectID": "posts/25_-_Confidence Intervals_-_concept/Math_32_-_25_-_Confidence_Intervals_concept.html",
    "href": "posts/25_-_Confidence Intervals_-_concept/Math_32_-_25_-_Confidence_Intervals_concept.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\n# install.packages() if needed\nlibrary(\"infer\")\n\nWarning: package 'infer' was built under R version 4.2.2\n\nlibrary(\"moderndive\")\n\nWarning: package 'moderndive' was built under R version 4.2.2\n\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\nSource: Statistical Inference via Data Science: A Modern Dive into R and the Tidyverse"
  },
  {
    "objectID": "posts/25_-_Confidence Intervals_-_concept/Math_32_-_25_-_Confidence_Intervals_concept.html#resampling-once",
    "href": "posts/25_-_Confidence Intervals_-_concept/Math_32_-_25_-_Confidence_Intervals_concept.html#resampling-once",
    "title": "Confidence Intervals",
    "section": "Resampling Once",
    "text": "Resampling Once\nSuppose that we took the 50 pennies and resampled once while sampling with replacement.\n\npennies_resampled_once <- pennies_sample %>%\n  sample_n(size = 50, replace = TRUE)\n\n\n# visualizing the distribution of the pennies\np2 <- pennies_resampled_once %>%\n  ggplot(aes(x = year)) +\n  geom_histogram(binwidth = 10, color = \"tan3\", fill = \"tan4\") +\n  labs(title = \"Pennies Resampled Once\",\n       subtitle = \"sampled with replacement\",\n       caption = \"Source: Modern Dive\")\n\n# (using `patchwork` package to arrange plots side-by-side\np1 + p2\n\n\n\n\n\npennies_resampled_once %>% summarize(xbar = mean(year))\n\n# A tibble: 1 × 1\n   xbar\n  <dbl>\n1 1991."
  },
  {
    "objectID": "posts/25_-_Confidence Intervals_-_concept/Math_32_-_25_-_Confidence_Intervals_concept.html#resampled-many-times",
    "href": "posts/25_-_Confidence Intervals_-_concept/Math_32_-_25_-_Confidence_Intervals_concept.html#resampled-many-times",
    "title": "Confidence Intervals",
    "section": "Resampled Many Times",
    "text": "Resampled Many Times\nSuppose now that we have each person in a 30-student discussion section repeat the act of drawing those 50 pennies with replacement.\n\npennies_resampled_many <- pennies_sample %>%\n  rep_sample_n(size = 50, replace = TRUE, reps = 30)\n\nNow we have each virtual student report their mean year.\n\npennies_resampled_many %>%\n  group_by(replicate) %>%\n  summarize(mean_year = mean(year))\n\n# A tibble: 30 × 2\n   replicate mean_year\n       <int>     <dbl>\n 1         1     1992.\n 2         2     1997.\n 3         3     1995.\n 4         4     1995.\n 5         5     1995.\n 6         6     1995.\n 7         7     1994.\n 8         8     1994.\n 9         9     1995.\n10        10     1999.\n# … with 20 more rows\n\n\n\nsummary(pennies_sample$year)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1962    1983    1996    1995    2008    2018 \n\n\n\npennies_resampled_many %>%\n  group_by(replicate) %>%\n  mutate(mean_year = mean(year)) %>%\n  ungroup() %>%\n  select(replicate, mean_year) %>%\n  distinct() %>%\n  ggplot(aes(x = mean_year)) +\n  geom_histogram(binwidth = 1, color = \"tan3\", fill = \"tan4\") +\n  labs(title = \"Resampling Results\",\n       subtitle = \"N = 30 resamples\",\n       caption = \"Source: Modern Dive\")\n\n\n\n\nOut of curiosity, let us push this process to \\(N = 1337\\) resamples.\n\npennies_resampled_means <- pennies_sample %>%\n  rep_sample_n(size = 50, replace = TRUE, reps = 1337) %>%\n  group_by(replicate) %>%\n  mutate(mean_year = mean(year)) %>%\n  ungroup() %>%\n  select(replicate, mean_year) %>%\n  distinct() \n\npennies_resampled_means %>%\n  ggplot(aes(x = mean_year)) +\n  geom_histogram(binwidth = 1, color = \"tan3\", fill = \"tan4\") +\n  labs(title = \"Resampling Results\",\n       subtitle = \"N = 1337 resamples\",\n       caption = \"Source: Modern Dive\")"
  },
  {
    "objectID": "posts/25_-_Confidence Intervals_-_concept/Math_32_-_25_-_Confidence_Intervals_concept.html#toward-confidence-intervals",
    "href": "posts/25_-_Confidence Intervals_-_concept/Math_32_-_25_-_Confidence_Intervals_concept.html#toward-confidence-intervals",
    "title": "Confidence Intervals",
    "section": "Toward Confidence Intervals",
    "text": "Toward Confidence Intervals\nThe standard deviation of a sampling distribution is called the standard error.\n\nxbar <- mean(pennies_resampled_means$mean_year)\nSE   <- sd(pennies_resampled_means$mean_year)\n\nWe can build a 95% confidence interval by computing \\(\\bar{x} \\pm 1.96*SE\\)\n\nc(xbar - 1.96*SE, xbar + 1.96*SE)\n\n[1] 1991.443 1999.620\n\n\n\npennies_resampled_means %>%\n  ggplot(aes(x = mean_year)) +\n  geom_histogram(binwidth = 1, color = \"tan3\", fill = \"tan4\") +\n  geom_vline(xintercept = c(xbar - 1.96*SE, xbar + 1.96*SE), color = \"yellow\", size = 2) +\n  labs(title = \"Resampling Results\",\n       subtitle = \"N = 1337 resamples\",\n       caption = \"Source: Modern Dive\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/25_-_Confidence Intervals_-_concept/Math_32_-_25_-_Confidence_Intervals_concept.html#using-the-infer-package",
    "href": "posts/25_-_Confidence Intervals_-_concept/Math_32_-_25_-_Confidence_Intervals_concept.html#using-the-infer-package",
    "title": "Confidence Intervals",
    "section": "Using the infer package",
    "text": "Using the infer package\n\npennies_sample %>%\n  specify(response = year)\n\nResponse: year (numeric)\n# A tibble: 50 × 1\n    year\n   <dbl>\n 1  2002\n 2  1986\n 3  2017\n 4  1988\n 5  2008\n 6  1983\n 7  2008\n 8  1996\n 9  2004\n10  2000\n# … with 40 more rows\n\n\n\npennies_sample %>%\n  specify(response = year) %>%\n  calculate(stat = \"mean\")\n\nResponse: year (numeric)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1 1995.\n\n\n\npennies_sample %>%\n  specify(response = year) %>%\n  generate(reps = 1337, type = \"bootstrap\")\n\nResponse: year (numeric)\n# A tibble: 66,850 × 2\n# Groups:   replicate [1,337]\n   replicate  year\n       <int> <dbl>\n 1         1  1976\n 2         1  1971\n 3         1  2000\n 4         1  2015\n 5         1  2016\n 6         1  2002\n 7         1  1979\n 8         1  1994\n 9         1  2018\n10         1  2016\n# … with 66,840 more rows\n\n\n\nbootstrap_distribution <- pennies_sample %>%\n  specify(response = year) %>%\n  generate(reps = 1337, type = \"bootstrap\") %>%\n  calculate(stat = \"mean\")\n\n# print\nbootstrap_distribution\n\nResponse: year (numeric)\n# A tibble: 1,337 × 2\n   replicate  stat\n       <int> <dbl>\n 1         1 1993.\n 2         2 1995.\n 3         3 1996.\n 4         4 1998.\n 5         5 1994.\n 6         6 1997.\n 7         7 1998.\n 8         8 1995.\n 9         9 1996.\n10        10 1996.\n# … with 1,327 more rows\n\n\n\nvisualise(bootstrap_distribution)\n\n\n\n\nThere are also wrappers in the infer package to extract the confidence interval\n\nbootstrap_distribution %>%\n  get_confidence_interval(point_estimate = mean(bootstrap_distribution$stat), \n                          level = 0.95, type = \"se\")\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1    1992.    2000.\n\n\nAlternatively, we can use percentiles to build our confidence intervals. This is useful when the data is not normally distributed.\n\nbootstrap_distribution %>%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1    1992.    2000.\n\n\n\nSE_CI <- bootstrap_distribution %>%\n  get_ci(point_estimate = mean(bootstrap_distribution$stat),\n         level = 0.95, type = \"se\")\n\nvisualize(bootstrap_distribution) +\n  shade_ci(endpoints = SE_CI, color = \"#DAA900\", fill = \"#002856\")"
  },
  {
    "objectID": "posts/26_-_Confidence Intervals_-_case studies/Math_32_-_26_-_Confidence_Intervals_case_studies.html",
    "href": "posts/26_-_Confidence Intervals_-_case studies/Math_32_-_26_-_Confidence_Intervals_case_studies.html",
    "title": "Case Study: Yawning",
    "section": "",
    "text": "After each code block, to the best of your current ability, describe what the R code is doing.\nThis material comes from Chapter 8 of Statistical Inference via Data Science: A ModernDive into R and the Tidyverse\n\nhttps://moderndive.com/8-confidence-intervals.html#case-study-two-prop-ci\n\n“Let’s apply our knowledge of confidence intervals to answer the question: “Is yawning contagious?”. If you see someone else yawn, are you more likely to yawn? In an episode of the US show Mythbusters, the hosts conducted an experiment to answer this question.”\n“Fifty adult participants who thought they were being considered for an appearance on the show were interviewed by a show recruiter. In the interview, the recruiter either yawned or did not. Participants then sat by themselves in a large van and were asked to wait. While in the van, the Mythbusters team watched the participants using a hidden camera to see if they yawned. The data frame containing the results of their experiment is available in the mythbusters_yawn data frame included in the moderndive package:”\n\nmythbusters_yawn\n\n# A tibble: 50 × 3\n    subj group   yawn \n   <int> <chr>   <chr>\n 1     1 seed    yes  \n 2     2 control yes  \n 3     3 seed    no   \n 4     4 seed    yes  \n 5     5 seed    no   \n 6     6 control no   \n 7     7 seed    yes  \n 8     8 control no   \n 9     9 control no   \n10    10 seed    no   \n# … with 40 more rows\n\n\n\nmythbusters_yawn %>% \n  group_by(group, yawn) %>% \n  summarize(count = n())\n\n`summarise()` has grouped output by 'group'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   group [2]\n  group   yawn  count\n  <chr>   <chr> <int>\n1 control no       12\n2 control yes       4\n3 seed    no       24\n4 seed    yes      10\n\n\n\nmythbusters_yawn %>% \n  specify(formula = yawn ~ group, success = \"yes\")\n\nResponse: yawn (factor)\nExplanatory: group (factor)\n# A tibble: 50 × 2\n   yawn  group  \n   <fct> <fct>  \n 1 yes   seed   \n 2 yes   control\n 3 no    seed   \n 4 yes   seed   \n 5 no    seed   \n 6 no    control\n 7 yes   seed   \n 8 no    control\n 9 no    control\n10 no    seed   \n# … with 40 more rows\n\n\n\nfirst_six_rows <- head(mythbusters_yawn)\n\n\nfirst_six_rows %>% \n  sample_n(size = 6, replace = TRUE)\n\n# A tibble: 6 × 3\n   subj group   yawn \n  <int> <chr>   <chr>\n1     4 seed    yes  \n2     6 control no   \n3     4 seed    yes  \n4     6 control no   \n5     2 control yes  \n6     2 control yes  \n\n\n\nmythbusters_yawn %>% \n  specify(formula = yawn ~ group, success = \"yes\") %>% \n  generate(reps = 1000, type = \"bootstrap\")\n\nResponse: yawn (factor)\nExplanatory: group (factor)\n# A tibble: 50,000 × 3\n# Groups:   replicate [1,000]\n   replicate yawn  group  \n       <int> <fct> <fct>  \n 1         1 no    control\n 2         1 yes   seed   \n 3         1 no    seed   \n 4         1 no    seed   \n 5         1 no    seed   \n 6         1 no    seed   \n 7         1 no    seed   \n 8         1 yes   seed   \n 9         1 no    control\n10         1 no    control\n# … with 49,990 more rows\n\n\n\nbootstrap_distribution_yawning <- mythbusters_yawn %>% \n  specify(formula = yawn ~ group, success = \"yes\") %>% \n  generate(reps = 1000, type = \"bootstrap\") %>% \n  calculate(stat = \"diff in props\", order = c(\"seed\", \"control\"))\nbootstrap_distribution_yawning\n\nResponse: yawn (factor)\nExplanatory: group (factor)\n# A tibble: 1,000 × 2\n   replicate    stat\n       <int>   <dbl>\n 1         1  0.225 \n 2         2 -0.0870\n 3         3 -0.0762\n 4         4  0.103 \n 5         5  0.0350\n 6         6  0.333 \n 7         7  0.0390\n 8         8  0.228 \n 9         9 -0.075 \n10        10 -0.105 \n# … with 990 more rows\n\n\n\nvisualize(bootstrap_distribution_yawning) +\n  geom_vline(xintercept = 0)\n\n\n\n\n\nbootstrap_distribution_yawning %>% \n  get_confidence_interval(type = \"percentile\", level = 0.95)\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1   -0.233    0.289\n\n\n\nobs_diff_in_props <- mythbusters_yawn %>% \n  specify(formula = yawn ~ group, success = \"yes\") %>% \n  # generate(reps = 1000, type = \"bootstrap\") %>% \n  calculate(stat = \"diff in props\", order = c(\"seed\", \"control\"))\nobs_diff_in_props\n\nResponse: yawn (factor)\nExplanatory: group (factor)\n# A tibble: 1 × 1\n    stat\n   <dbl>\n1 0.0441\n\n\n\nmyth_ci_se <- bootstrap_distribution_yawning %>% \n  get_confidence_interval(type = \"percentile\", level = 0.95)\nmyth_ci_se\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1   -0.233    0.289"
  },
  {
    "objectID": "posts/26_-_Confidence Intervals_-_case studies/Math_32_-_26_-_Confidence_Intervals_case_studies.html#example-how-often-do-you-brush-your-teeth-per-week",
    "href": "posts/26_-_Confidence Intervals_-_case studies/Math_32_-_26_-_Confidence_Intervals_case_studies.html#example-how-often-do-you-brush-your-teeth-per-week",
    "title": "Case Study: Yawning",
    "section": "Example: How often do you brush your teeth per week?",
    "text": "Example: How often do you brush your teeth per week?\n\n# sample statistics\ndemo_df %>%\n  summarize(count = sum(!is.na(brushingTeeth)),\n            xbar = mean(brushingTeeth, na.rm = TRUE),\n            med  = median(brushingTeeth, na.rm = TRUE),\n            s    = sd(brushingTeeth, na.rm = TRUE),\n            missing = sum(is.na(brushingTeeth)))\n\n# A tibble: 1 × 5\n  count  xbar   med     s missing\n  <int> <dbl> <dbl> <dbl>   <int>\n1   275  12.0    14  3.54       6\n\n\n\n# bootstrap distribution\nbootstrap_distribution <- demo_df %>%\n  specify(response = brushingTeeth) %>%\n  generate(reps = 1337, type = \"bootstrap\") %>%\n  calculate(stat = \"mean\")\n\nWarning: Removed 6 rows containing missing values.\n\n\n\n# form a 95% confidence interval\nbootstrap_distribution %>%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1     11.6     12.4\n\n\n\n# visualize the confidence interval and sampling distribution\nSE_CI <- bootstrap_distribution %>%\n  get_ci(level = 0.95, type = \"percentile\")\n\nvisualize(bootstrap_distribution) +\n  shade_ci(endpoints = SE_CI, color = \"#DAA900\", fill = \"#002856\")"
  },
  {
    "objectID": "posts/26_-_Confidence Intervals_-_case studies/Math_32_-_26_-_Confidence_Intervals_case_studies.html#exercises",
    "href": "posts/26_-_Confidence Intervals_-_case studies/Math_32_-_26_-_Confidence_Intervals_case_studies.html#exercises",
    "title": "Case Study: Yawning",
    "section": "Exercises",
    "text": "Exercises\n\nUsing the GPA numerical data,\n\n\ncompute sample statistics\n\n\n\n\n\ncompute a 95% confidence interval, and describe the result in a complete sentence.\n\n\n\n\n\nproduce a visual of the sampling distribution and the confidence interval.\n\n\n\n\n\nUsing the hoursStudying numerical data,\n\n\ncompute sample statistics\n\n\n\n\n\ncompute a 95% confidence interval, and describe the result in a complete sentence.\n\n\n\n\n\nproduce a visual of the sampling distribution and the confidence interval.\n\n\n\n\n\nUsing one more column of numerical data of your choosing,\n\n\ncompute sample statistics\n\n\n\n\n\ncompute a 95% confidence interval, and describe the result in a complete sentence.\n\n\n\n\n\nproduce a visual of the sampling distribution and the confidence interval."
  },
  {
    "objectID": "posts/19_-_Poisson Process/Math_32_19_Poisson_Process.html#motivation-for-the-poisson-process",
    "href": "posts/19_-_Poisson Process/Math_32_19_Poisson_Process.html#motivation-for-the-poisson-process",
    "title": "19: Poisson Process",
    "section": "Motivation for the Poisson Process",
    "text": "Motivation for the Poisson Process\n\nAssume a constant \\(\\lambda\\) of arrivals\nLet \\(N_{t}\\) be the number of arrivals in time interval \\([0,t]\\)\nHomogeneity: \\(\\text{E}[N_{t}] = \\lambda t\\) (``rate times time’’)\nIndependence: numbers of arrivals in disjoint time intervals are independent random variables\n\nGoal: Derive distribution of number of arrivals\n\nWe expect \\(\\text{E}[N_{t}] = \\lambda t\\) (``rate times time’’)\nPartition time interval \\([0,t]\\) into \\(n\\) subintervals\nAssuming \\(n\\) is large enough so that each subinterval has zero or one arrival (i.e. Bernoulli trial)\nProbability of arrival in a random subinterval: \\(p = \\displaystyle\\frac{\\lambda t}{n}\\)\n\nSo far, we are assuming \\(N_{t} \\sim \\text{Bin}(n,p)\\)\n\\[P(N_{t} = k) = \\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k} \\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n-k}\\]"
  },
  {
    "objectID": "posts/19_-_Poisson Process/Math_32_19_Poisson_Process.html#infinitessimal",
    "href": "posts/19_-_Poisson Process/Math_32_19_Poisson_Process.html#infinitessimal",
    "title": "19: Poisson Process",
    "section": "Infinitessimal",
    "text": "Infinitessimal\nHowever,\n\n\\(n\\) was arbitrary\ntime is a continuous variable\n\nSo let’s take the limit as \\(n\\) goes to infinity.\n\\[\\displaystyle\\lim_{n \\to \\infty} P(N_{t} = k) = \\displaystyle\\lim_{n \\to \\infty} {\\color{purple}\\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k}} {\\color{blue}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n}} {\\color{red}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{-k}}\\]\n\n\n\n\n\n\nInfinitessimal\n\n\n\n\n\nHandling the limit by its factors:\n\\[\\displaystyle\\lim_{n \\to \\infty} {\\color{red}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{-k}} = 1, \\quad \\displaystyle\\lim_{n \\to \\infty} {\\color{blue}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n}} = e^{-\\lambda t}\\]\n\\[\\begin{array}{rcl}\n  \\displaystyle\\lim_{n \\to \\infty} {\\color{purple}\\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k}} & = & (\\lambda t)^{k} \\displaystyle\\lim_{n \\to \\infty} \\binom{n}{k} \\left(\\displaystyle\\frac{1}{n}\\right)^{k} \\\\\n  ~ & = & (\\lambda t)^{k} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n!}{k!(n-k)!} \\cdot \\displaystyle\\frac{1}{n^{k}} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n!}{(n-k)!} \\cdot \\displaystyle\\frac{1}{n^{k}} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\prod_{i = 0}^{k-1} \\displaystyle\\frac{n - i}{n} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!}  \\displaystyle\\prod_{i = 0}^{k-1} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n - i}{n} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!}  \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/19_-_Poisson Process/Math_32_19_Poisson_Process.html#probability-density-function",
    "href": "posts/19_-_Poisson Process/Math_32_19_Poisson_Process.html#probability-density-function",
    "title": "19: Poisson Process",
    "section": "Probability Density Function",
    "text": "Probability Density Function\n\n\n\n\n\n\nProbability Density Function\n\n\n\nThus, if \\(X\\) is the number of observed events in this model, let \\(\\mu = \\lambda t\\) (was assumed to be constant), and\n\\[P(X = k) = \\displaystyle\\frac{\\mu^{k}e^{-\\mu}}{k!}\\]\nThis is called the probability mass function for the Poisson distribution. The Poisson distribution is a discrete distribution that tends to be used to model rare events."
  },
  {
    "objectID": "posts/19_-_Poisson Process/Math_32_19_Poisson_Process.html#expected-value",
    "href": "posts/19_-_Poisson Process/Math_32_19_Poisson_Process.html#expected-value",
    "title": "19: Poisson Process",
    "section": "Expected Value",
    "text": "Expected Value\nHere we will derive the expected value for a \\(\\text{Pois}(\\mu)\\) distribution.\n\n\n\n\n\n\nExpected Value\n\n\n\n\n\n\\[\\begin{array}{rcl}\n  \\text{E}[X] & = & \\displaystyle\\sum_{k = 0}^{\\infty} k \\cdot P(x = k) \\\\\n  ~ & = & \\displaystyle\\sum_{k = 0}^{\\infty} k \\cdot \\displaystyle\\frac{\\mu^{k}e^{-\\mu}}{k!} \\\\\n  ~ & = & 0 + \\displaystyle\\sum_{k = 1}^{\\infty} k \\cdot \\displaystyle\\frac{\\mu^{k}e^{-\\mu}}{k!} \\\\\n  ~ & = & \\displaystyle\\sum_{k = 1}^{\\infty} \\displaystyle\\frac{\\mu^{k}e^{-\\mu}}{(k-1)!} \\\\\n  ~ & = & \\displaystyle\\sum_{k = 1}^{\\infty} \\displaystyle\\frac{\\mu\\mu^{k-1}e^{-\\mu}}{(k-1)!} \\\\\n  ~ & = & \\mu e^{-\\mu}\\displaystyle\\sum_{k = 1}^{\\infty} \\displaystyle\\frac{\\mu^{k-1}}{(k-1)!} \\\\\n  ~ & = & \\mu e^{-\\mu}\\displaystyle\\sum_{k = 0}^{\\infty} \\displaystyle\\frac{\\mu^{k}}{k!} \\\\\n  ~ & = & \\mu e^{-\\mu}e^{\\mu} \\\\\n\\end{array}\\]\nRecall: By Taylor series, \\(e^{x} = \\displaystyle\\sum_{n = 0}^{\\infty} \\displaystyle\\frac{x^{n}}{n!}\\)\nTherefore \\(\\text{E}[X] = \\mu\\) for a \\(\\text{Pois}(\\mu)\\) distribution. Furthermore, \\(\\text{Var}(X) = \\mu\\) too for a \\(\\text{Pois}(\\mu)\\) distribution."
  },
  {
    "objectID": "posts/19_-_Poisson Process/Math_32_19_Poisson_Process.html#examples",
    "href": "posts/19_-_Poisson Process/Math_32_19_Poisson_Process.html#examples",
    "title": "19: Poisson Process",
    "section": "Examples",
    "text": "Examples\n\nExample 1Example 2\n\n\nCampus Safety: Stalking The following data on reported incidents on stalking comes from the University of California Merced report Safety Matters. Assume a Poisson distribution.\n\n\nFind the mean of the data.\nCompute the probability that exactly 2 stalking incidents will be reported this year.\n\n\n\n\n\n\n\nProbability Density Function\n\n\n\n\n\n\n\n\n\n\nCompute the probability that at least one stalking incident will be reported this year.\n\n\n\n\n\n\n\nProbability Density Function\n\n\n\n\n\n\n\n\n\n\n\nCampus Safety: Drug Law Violations The following data on arrests for drug law violations comes from the University of California Merced report Safety Matters. Assume a Poisson distribution.\n\n\nFind the mean of the data.\nCompute the probability that exactly 3 arrests will be made this year.\n\n\n\n\n\n\n\nProbability Density Function\n\n\n\n\n\n\n\n\n\n\nCompute the probability that at most one arrest will be made this year.\n\n\n\n\n\n\n\nProbability Density Function"
  },
  {
    "objectID": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html",
    "href": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\n# install.packages() if needed\nlibrary(\"infer\")         #pipeline workflow for hypothesis testing\n\nWarning: package 'infer' was built under R version 4.2.2\n\nlibrary(\"moderndive\")    #textbook's package and data\n\nWarning: package 'moderndive' was built under R version 4.2.2\n\nlibrary(\"patchwork\")     #easily let's me show graphs side-by-side\nlibrary(\"tidyverse\")     #the overall programming style universe\nSource: Statistical Inference via Data Science: A Modern Dive into R and the Tidyverse"
  },
  {
    "objectID": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html#shuffling",
    "href": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html#shuffling",
    "title": "Hypothesis Testing",
    "section": "Shuffling",
    "text": "Shuffling\nIf gender did not matter when it comes to these job promotions, then it should not matter if we shuffle the gender labels in the data.\n\noriginal_bar_graph <- ggplot(promotions, aes(x = gender, fill = decision)) +\n  geom_bar() +\n  labs(title = \"original\",\n       x = \"Gender of name on resume\") +\n  theme(legend.position = \"none\")\n\ngender_shuffled <- promotions\ngender_shuffled$gender <- sample(promotions$gender) \n#sampled without replacement\n\nshuffled_bar_graph <- gender_shuffled %>%\n  ggplot(aes(x = gender, fill = decision)) +\n  geom_bar() +\n  labs(title = \"shuffled\",\n       x = \"Gender of name on resume\")\n\noriginal_bar_graph + shuffled_bar_graph\n\n\n\n\n\nin the previous chapter, we did a bootstrap method that used sampling with replacement\nhere, we are performing a permutation test that uses sampling without replacement"
  },
  {
    "objectID": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html#conducting-a-hypothesis-test-with-infer",
    "href": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html#conducting-a-hypothesis-test-with-infer",
    "title": "Hypothesis Testing",
    "section": "Conducting a Hypothesis Test with infer",
    "text": "Conducting a Hypothesis Test with infer\n\npromotions %>% \n  specify(formula = decision ~ gender, success = \"promoted\")\n\nResponse: decision (factor)\nExplanatory: gender (factor)\n# A tibble: 48 × 2\n   decision gender\n   <fct>    <fct> \n 1 promoted male  \n 2 promoted male  \n 3 promoted male  \n 4 promoted male  \n 5 promoted male  \n 6 promoted male  \n 7 promoted male  \n 8 promoted male  \n 9 promoted male  \n10 promoted male  \n# … with 38 more rows\n\n\n\npromotions %>% \n  specify(formula = decision ~ gender, success = \"promoted\") %>% \n  # \"point\" for single sample or \"independence\" for two samples\n  hypothesize(null = \"independence\")\n\nResponse: decision (factor)\nExplanatory: gender (factor)\nNull Hypothesis: independence\n# A tibble: 48 × 2\n   decision gender\n   <fct>    <fct> \n 1 promoted male  \n 2 promoted male  \n 3 promoted male  \n 4 promoted male  \n 5 promoted male  \n 6 promoted male  \n 7 promoted male  \n 8 promoted male  \n 9 promoted male  \n10 promoted male  \n# … with 38 more rows\n\n\n\npromotions_permutations <- promotions %>% \n  specify(formula = decision ~ gender, success = \"promoted\") %>% \n  hypothesize(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\")\n\n\nnull_distribution <- promotions %>% \n  specify(formula = decision ~ gender, success = \"promoted\") %>% \n  hypothesize(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\n\n\n# observed difference in proportions\nobs_diff_prop <- promotions %>% \n  specify(decision ~ gender, success = \"promoted\") %>% \n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\nobs_diff_prop\n\nResponse: decision (factor)\nExplanatory: gender (factor)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1 0.292\n\n\n\nvisualize(null_distribution, bins = 10)\n\n\n\n\n\nvisualize(null_distribution, bins = 10) + \n  # choices for direction are \"right\", \"left\", and \"both\"\n  shade_p_value(obs_stat = obs_diff_prop, direction = \"right\")\n\n\n\n\n\nnull_distribution %>% \n  get_p_value(obs_stat = obs_diff_prop, direction = \"right\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1    0.03\n\n\nFor NHST (null hypothesis significance testing), many scientists compare the p-value to a significance level of \\(\\alpha = 0.05\\). Since the p-value < 0.05, we reject the null hypothesis of equal proportions of promotions among men and women."
  },
  {
    "objectID": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html#comparision-to-confidence-intervals",
    "href": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html#comparision-to-confidence-intervals",
    "title": "Hypothesis Testing",
    "section": "Comparision to Confidence Intervals",
    "text": "Comparision to Confidence Intervals\n\nbootstrap_distribution <- promotions %>% \n  specify(formula = decision ~ gender, success = \"promoted\") %>% \n  # Change 1 - Remove hypothesize():\n  # hypothesize(null = \"independence\") %>% \n  # Change 2 - Switch type from \"permute\" to \"bootstrap\":\n  generate(reps = 1000, type = \"bootstrap\") %>% \n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\n\n\npercentile_ci <- bootstrap_distribution %>% \n  get_confidence_interval(level = 0.95, type = \"percentile\")\npercentile_ci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1   0.0624    0.508\n\n\n\nvisualize(bootstrap_distribution) + \n  shade_confidence_interval(endpoints = percentile_ci)"
  },
  {
    "objectID": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html#smart-phone-loyalty-vs-class-standing",
    "href": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html#smart-phone-loyalty-vs-class-standing",
    "title": "Hypothesis Testing",
    "section": "Smart Phone Loyalty vs Class Standing",
    "text": "Smart Phone Loyalty vs Class Standing\n\nnull hypothesis: Sophomores and Juniors choose iPhones and Androids with the same proportions\nalternative hypothesis: Sophomores and Juniors choose iPhones and Androids with different proportions\npredictor variable (categorical): classStanding\nresponse variable (numerical): smartPhones\n\n\n# table(demo_df$classStanding)\n# table(demo_df$smartPhones)\ndemo_df %>%\n  filter(classStanding == \"Sophomore\" | classStanding == \"Junior\") %>%\n  filter(smartPhones == \"Android\" | smartPhones == \"iPhone\") %>%\n  group_by(classStanding, smartPhones) %>%\n  summarize(classStanding = n())\n\n`summarise()` has grouped output by 'classStanding'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 2\n# Groups:   classStanding [4]\n  classStanding smartPhones\n          <int> <chr>      \n1            47 Android    \n2           118 iPhone     \n3            12 Android    \n4            23 iPhone     \n\n\nIn this scenario, our one-sided hypothesis test looks at a difference of two means.\n$$\n\\[\\begin{array}{rrcl}\n\n\n  H_{0}: p_{F} - p_{M} & = & 0 \\\\\n  H_{a}: p_{F} - p_{M} & \\neq & 0 \\\\\n\\end{array}\\]\n$$\n\n# observed difference in proportions\nobs_diff_prop <- demo_df %>%\n  filter(classStanding == \"Sophomore\" | classStanding == \"Junior\") %>%\n  filter(smartPhones == \"Android\" | smartPhones == \"iPhone\") %>%\n  specify(smartPhones ~ classStanding, success = \"iPhone\") %>% \n  calculate(stat = \"diff in props\", order = c(\"Sophomore\", \"Junior\"))\nobs_diff_prop\n\nResponse: smartPhones (factor)\nExplanatory: classStanding (factor)\n# A tibble: 1 × 1\n     stat\n    <dbl>\n1 -0.0580\n\n\n\nnull_distribution <- demo_df %>%\n  filter(classStanding == \"Sophomore\" | classStanding == \"Junior\") %>%\n  filter(smartPhones == \"Android\" | smartPhones == \"iPhone\") %>%\n  specify(smartPhones ~ classStanding, success = \"iPhone\") %>% \n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(stat = \"diff in props\", order = c(\"Sophomore\", \"Junior\"))\n\n\nvisualize(null_distribution, bins = 10) + \n  shade_p_value(obs_stat = obs_diff_prop, direction = \"two_sided\")\n\n\n\n\n\nnull_distribution %>% \n  get_p_value(obs_stat = obs_diff_prop, direction = \"two_sided\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.632"
  },
  {
    "objectID": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html#procrastination-vs-partying",
    "href": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html#procrastination-vs-partying",
    "title": "Hypothesis Testing",
    "section": "Procrastination vs Partying",
    "text": "Procrastination vs Partying\nThe survey questions were\n\n“Do you consider yourself to be a procrastinator?”\n“Do you plan to attend at least one party per month this semester?”\ncolloquial hypothesis: Party goers are more likely to be procrastinators\nnull hypothesis: Procrastinator proportion is the same regardless of party going plans\nalternative hypothesis: Party goers are more likely to be procrastinators\npredictor variable (categorical): parties\nresponse variable (categorical): procrastinator\n\nIn this scenario, our one-sided hypothesis test looks at a difference of two proportions.\n\\[\\begin{array}{rrcl}\n  H_{0}: p_{Y} - p_{N} & = & 0 \\\\\n  H_{a}: p_{Y} - p_{N} & > & 0 \\\\\n\\end{array}\\]\n\nCompute the observed difference in proportions for the procrastination grouped by parties\n\n\n# table(demo_df$procrastinator)\n# table(demo_df$parties)\n\n\nBuild a null distribution toward the hypothesis test.\n\n\n\n\n\nUse the visualize function to graph the null distribution and shade the p-value.\n\n\n\n\n\nCompute the p-value for the hypothesis test, and then write a sentence to describe the conclusion of the hypothesis test."
  },
  {
    "objectID": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html#campus-food-satisfaction-vs-class-standing",
    "href": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html#campus-food-satisfaction-vs-class-standing",
    "title": "Hypothesis Testing",
    "section": "Campus Food Satisfaction vs Class Standing",
    "text": "Campus Food Satisfaction vs Class Standing\n\nnull hypothesis: Sophomores and Juniors report the same levels of happiness with campus food\nalternative hypothesis: Sophomores and Juniors report different levels of happiness with campus food\npredictor variable (categorical): classStanding\nresponse variable (numerical): diningCommons\n\n\n# table(demo_df$classStanding)\n# summary(demo_df$diningCommons)\ndemo_df |>\n  filter(classStanding == \"Sophomore\" | classStanding == \"Junior\") |>\n  filter(diningCommons >= 0 & diningCommons <= 100) |>\n  group_by(classStanding) |>\n  summarize(min = min(diningCommons, na.rm = TRUE),\n            xbar = mean(diningCommons, na.rm = TRUE),\n            median = median(diningCommons, na.rm = TRUE),\n            max = sd(diningCommons, na.rm = TRUE))\n\n# A tibble: 2 × 5\n  classStanding   min  xbar median   max\n  <chr>         <dbl> <dbl>  <dbl> <dbl>\n1 Junior            0  30.1     30  25.7\n2 Sophomore         0  30.8     35  26.4\n\n\n\n# table(demo_df$classStanding)\n# summary(demo_df$diningCommons)\ndemo_df |>\n  filter(classStanding == \"Sophomore\" | classStanding == \"Junior\") |>\n  filter(diningCommons >= 0 & diningCommons <= 100) |>\n  ggplot(aes(x = classStanding, y = diningCommons)) +\n  geom_boxplot(aes(fill = classStanding)) +\n  labs(title = \"Students Rating of the Main Campus Food Options\",\n       subtitle = \"2022\",\n       caption = \"UC Merced\",\n       x = \"class standing\",\n       y = \"favorability (0 = low, 100 = high)\") +\n  theme_minimal()\n\n\n\n\nIn this scenario, our two-sided hypothesis test looks at a difference of two means.\n$$\n\\[\\begin{array}{rrcl}\n\n\n  H_{0}: \\mu_{S} - \\mu_{J} & = & 0 \\\\\n  H_{a}: \\mu_{S} - \\mu_{J} & \\neq & 0 \\\\\n\\end{array}\\]\n$$\n\n# observed difference in means\nobs_diff_means <- demo_df |>\n  filter(classStanding == \"Sophomore\" | classStanding == \"Junior\") |>\n  filter(diningCommons >= 0 & diningCommons <= 100) |>\n  specify(diningCommons ~ classStanding) |> \n  calculate(stat = \"diff in means\", order = c(\"Sophomore\", \"Junior\"))\nobs_diff_means\n\nResponse: diningCommons (numeric)\nExplanatory: classStanding (factor)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1 0.677\n\n\n\nnull_distribution <- demo_df |>\n  filter(classStanding == \"Sophomore\" | classStanding == \"Junior\") |>\n  filter(diningCommons >= 0 & diningCommons <= 100) |>\n  specify(diningCommons ~ classStanding) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 1000, type = \"permute\") |> \n  calculate(stat = \"diff in means\", order = c(\"Sophomore\", \"Junior\"))\n\n\nvisualize(null_distribution, bins = 10) + \n  shade_p_value(obs_stat = obs_diff_means, direction = \"two_sided\")\n\n\n\n\n\nnull_distribution |> \n  get_p_value(obs_stat = obs_diff_means, direction = \"two_sided\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.906"
  },
  {
    "objectID": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html#happiness-vs-top-choice",
    "href": "posts/27_-_Hypothesis Testing/Math_32_-_27_-_Hypothesis_Testing.html#happiness-vs-top-choice",
    "title": "Hypothesis Testing",
    "section": "Happiness vs Top Choice",
    "text": "Happiness vs Top Choice\nThe survey questions were\n\n“Was UC Merced your top choice for university?”\n“Rate your own happiness level”\ncolloquial hypothesis: Those who felt UC Merced was their top choice are currently more happy.\nnull hypothesis: Average happiness levels are the same for those who felt UCM was their top choice or not.\nalternative hypothesis: Those who felt UC Merced was their top choice are currently more happy.\npredictor variable (categorical): UCMtopChoice\nresponse variable (categorical): happiness\n\nIn this scenario, our one-sided hypothesis test looks at a difference of two proportions.\n\\[\\begin{array}{rrcl}\n  H_{0}: \\mu_{A} - \\mu_{B} & = & 0 \\\\\n  H_{a}: \\mu_{A} - \\mu_{B} & > & 0 \\\\\n\\end{array}\\]\n\n# table(demo_df$UCMtopChoice)\ndemo_df |>\n  filter(happiness >= 0 & happiness <= 100) |>\n  specify(happiness ~ UCMtopChoice) |> \n  ggplot(aes(x = UCMtopChoice, y = happiness)) +\n  geom_boxplot(aes(fill = UCMtopChoice)) +\n  labs(title = \"Does Past Feelings about UCM Affect Current Happiness\",\n       subtitle = \"2022\",\n       caption = \"UC Merced\",\n       x = \"Was UC Merced their top choice?\",\n       y = \"happiness (0 = low, 100 = high)\") +\n  theme_minimal()\n\n\n\n\n\n# observed difference in means\nobs_diff_means_2 <- demo_df |>\n  filter(happiness >= 0 & happiness <= 100) |>\n  specify(happiness ~ UCMtopChoice) |> \n  calculate(stat = \"diff in means\", order = c(\"Yes\", \"No\"))\nobs_diff_means_2\n\nResponse: happiness (numeric)\nExplanatory: UCMtopChoice (factor)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1  2.22\n\n\n\nnull_distribution_2 <- demo_df |>\n  filter(happiness >= 0 & happiness <= 100) |>\n  specify(happiness ~ UCMtopChoice) |> \n  hypothesize(null = \"independence\") |>\n  generate(reps = 1000, type = \"permute\") |> \n  calculate(stat = \"diff in means\", order = c(\"Yes\", \"No\"))\n\n\nvisualize(null_distribution_2, bins = 10) + \n  shade_p_value(obs_stat = obs_diff_means_2, direction = \"greater\")\n\n\n\n\n\nnull_distribution_2 |> \n  get_p_value(obs_stat = obs_diff_means_2, direction = \"two_sided\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.486"
  },
  {
    "objectID": "posts/01_-_Introduction/Math_32_-_Introductions.html#identity-statement",
    "href": "posts/01_-_Introduction/Math_32_-_Introductions.html#identity-statement",
    "title": "Introduction",
    "section": "Identity Statement",
    "text": "Identity Statement\n\n\n\nOriginally from Los Angeles\nMath: easier to understand through graphs\nComputer Programming: years of experience with R, Python, MATLAB, PHP, HTML, etc.\nLearning: drawn to puzzles and manageable tasks\nPersonality: shy, introvert"
  },
  {
    "objectID": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html",
    "href": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html",
    "title": "2: Inclusion-Exclusion",
    "section": "",
    "text": "library(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#starter-examples",
    "href": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#starter-examples",
    "title": "2: Inclusion-Exclusion",
    "section": "Starter Examples",
    "text": "Starter Examples\nIn order to introduce probability concepts, you will notice that many sections of this book start out by talking about simple situations such as flipping a coin or rolling a die (or rolling a pair of dice). This is to ease the reader into more complicated examples.\n\n\n\n\n\n\nNote\n\n\n\nA possibility space is a set of all of the possible outcomes."
  },
  {
    "objectID": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#example-1-one-die",
    "href": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#example-1-one-die",
    "title": "2: Inclusion-Exclusion",
    "section": "Example 1: One Die",
    "text": "Example 1: One Die\n\n\nConsider rolling one six-sided die. For each of the following events, list their possible ways, and then find their probabilities:\n\nA: rolling an even number\nB: rolling a number greater than 3\nC: rolling a double-digit number\n\n\n\n\n\none, six-sided die"
  },
  {
    "objectID": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#starting-to-combine",
    "href": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#starting-to-combine",
    "title": "2: Inclusion-Exclusion",
    "section": "Starting to Combine",
    "text": "Starting to Combine\nAt first, we consider simply adding up the probabilities to compute the probability of the union of the probabilities:\n\\[P(A \\cup B) = P(A) + P(B) = ?\\] ## Inconsistent?\nSo far,\n\\[P(A \\cup B) = P(A) + P(B) = 100\\%\\] What went wrong? (Discuss with a neighbor)"
  },
  {
    "objectID": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#some-set-notation",
    "href": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#some-set-notation",
    "title": "2: Inclusion-Exclusion",
    "section": "Some Set Notation",
    "text": "Some Set Notation\n\n\n\n\n\n\nTip\n\n\n\nThe union of sets A and B is the set of all elements that appear either in set A OR set B \\[A \\cup B = \\{x : x \\in A \\text{ OR } x \\in B\\}\\]\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe intersection of sets A and B is the set of all elements that appear both in set A AND set B \\[A \\cap B = \\{x : x \\in A \\text{ AND } x \\in B\\}\\]"
  },
  {
    "objectID": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#inclusion-exclusion-principle",
    "href": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#inclusion-exclusion-principle",
    "title": "2: Inclusion-Exclusion",
    "section": "Inclusion-Exclusion Principle",
    "text": "Inclusion-Exclusion Principle\nWe observe that there was an overlap in the calculation. Since some elements were counted twice, we can compensate by subtracting one copy of that overlap. This notion is called the Inclusion-Exclusion Principle\n\n\n\n\n\n\nNote\n\n\n\nTo compute the probability of a set union, we need to consider the overlapping portions. For two sets A and B, the probability of observing the union is \\[P(A \\cup B) = {\\color{red}P(A)} + {\\color{blue}P(B)} - {\\color{purple}P(A \\cap B)}\\]"
  },
  {
    "objectID": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#example-2-two-dice",
    "href": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#example-2-two-dice",
    "title": "2: Inclusion-Exclusion",
    "section": "Example 2: Two dice",
    "text": "Example 2: Two dice\n\n\nConsider rolling two six-sided dice. Find the probability that their total is 8 or the second die shows a number greater or equal to 5.\n\n\n\n\ntwo dice"
  },
  {
    "objectID": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#subtlety-in-assumptions",
    "href": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#subtlety-in-assumptions",
    "title": "2: Inclusion-Exclusion",
    "section": "Subtlety in Assumptions",
    "text": "Subtlety in Assumptions\nIn the previous example, we assumed a notion of sampling with replacement. That is, when rolling dice, we know that a number can be repeated. In other situations were observations cannot repeat, we are sampling without replacement."
  },
  {
    "objectID": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#example-3-faculty-matters",
    "href": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#example-3-faculty-matters",
    "title": "2: Inclusion-Exclusion",
    "section": "Example 3: Faculty Matters",
    "text": "Example 3: Faculty Matters\n\nPromptSample Space 1Sample Space 2\n\n\nConsider a subset of the UC Merced Applied Math department with 6 faculty members—Blanchette, Buvoli, Sandoval, Stepanian, Thompson, Yatskar—must select two of its members to serve on a personnel review committee. Because the work will be time-consuming, no one is anxious to serve, so it is decided that the representatives will be selected by putting 6 slips of paper in a box, mixing them, and selecting two without replacement.\n\nWhat is the probability that both Thompson and Yatskar will be selected?\nWhat is the probability that at least one of the two members whose name begins with ‘B’ is selected?\nIf the 6 faculty members have taught for 15, 3, 5, 9, 4, and 12 years, respectively, at the university, what is the probability that the two chosen representatives have at least a combined 10 years’ teaching experience at the university?"
  },
  {
    "objectID": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#looking-ahead",
    "href": "posts/02_-_Inclusion-Exclusion/Math_32_-_02_-_Inclusion_Exclusion.html#looking-ahead",
    "title": "2: Inclusion-Exclusion",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Jan. 20:\n\nPerceptions of Probability (survey)\nWHW1\n\nIdentity Statement (essay)\nand the before-lecture quizzes\n\nExam 1 will be on Wed., Mar. 1"
  },
  {
    "objectID": "posts/04_-_Conditional_Probability/Math_32_04_Conditional_Probability.html#dependence",
    "href": "posts/04_-_Conditional_Probability/Math_32_04_Conditional_Probability.html#dependence",
    "title": "4: Conditional Probability",
    "section": "Dependence",
    "text": "Dependence\nGoal: Start to consider dependence in probability\nObjective: Practice conditional probability calculations"
  },
  {
    "objectID": "posts/04_-_Conditional_Probability/Math_32_04_Conditional_Probability.html#example-subsetting",
    "href": "posts/04_-_Conditional_Probability/Math_32_04_Conditional_Probability.html#example-subsetting",
    "title": "4: Conditional Probability",
    "section": "Example: Subsetting",
    "text": "Example: Subsetting\nConsider the months of the year\n\\[M = \\{\\text{Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec} \\}\\] Let us say that a month is ``long’’ if it has 31 days. What is the probability that we have a long month given that we are in the Spring semester?\n\\[S = \\{ \\text{Jan, Feb, Mar, Apr, May} \\}\\]"
  },
  {
    "objectID": "posts/04_-_Conditional_Probability/Math_32_04_Conditional_Probability.html#conditional-probability",
    "href": "posts/04_-_Conditional_Probability/Math_32_04_Conditional_Probability.html#conditional-probability",
    "title": "4: Conditional Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nWe can condense this process into a formula for conditional probability:\n\n\n\n\n\n\nConditional Probability\n\n\n\nThe conditional probability of observing event \\(A\\) given event \\(B\\) has already taken place is\n\\[P(A|B) = \\displaystyle\\frac{P(A \\cap B)}{P(B)}\\]"
  },
  {
    "objectID": "posts/04_-_Conditional_Probability/Math_32_04_Conditional_Probability.html#example-contigency-tables",
    "href": "posts/04_-_Conditional_Probability/Math_32_04_Conditional_Probability.html#example-contigency-tables",
    "title": "4: Conditional Probability",
    "section": "Example: Contigency Tables",
    "text": "Example: Contigency Tables\nIn this hypothetical example, suppose that we are following an epidemiologist who is testing patients at a hospital in for the novel strain of coronavirus.\n\nBuild a contingency table with the following data\n\n\n175 true positives\n32 false negatives\n18 false positives\n2019 true negatives\n\n\nCompute the probability that a randomly selected patient is disease free given that the drug test is positive."
  },
  {
    "objectID": "posts/04_-_Conditional_Probability/Math_32_04_Conditional_Probability.html#prosecutors-fallacy",
    "href": "posts/04_-_Conditional_Probability/Math_32_04_Conditional_Probability.html#prosecutors-fallacy",
    "title": "4: Conditional Probability",
    "section": "Prosecutor’s Fallacy",
    "text": "Prosecutor’s Fallacy\n\nUsing the same counts as the previous example, compute the probability that for a randomly selected patient the test returns positive given that the patient is disease free.\n\n\n\n\n\n\n\nWarning\n\n\n\n\n\nConverses for conditional probability are almost never equal.\n\\[P(A|B) \\neq P(B|A)\\]\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nWhat do we do when the order of the events switch?\nNext time: Bayes’ Rule"
  },
  {
    "objectID": "posts/04_-_Conditional_Probability/Math_32_04_Conditional_Probability.html#looking-ahead",
    "href": "posts/04_-_Conditional_Probability/Math_32_04_Conditional_Probability.html#looking-ahead",
    "title": "4: Conditional Probability",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Jan. 27:\n\nWHW2\nJHW0\nCLO (survey)\n\nand the before-lecture quizzes\n\nExam 1 will be on Wed., Mar. 1"
  },
  {
    "objectID": "posts/05_-_Bayes_Rule/Math_32_05_Bayes_Rule.html#bayes-rule",
    "href": "posts/05_-_Bayes_Rule/Math_32_05_Bayes_Rule.html#bayes-rule",
    "title": "5: Bayes’ Rule (Concepts)",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\nIn the previous section, we studied conditional probability \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] and we talked about how the inverse probabilities \\(P(A|B)\\) and \\(P(B|A)\\) are almost never equal. In this section, we discuss how to properly think and calculate that inverse probability.\n\n\n\n\n\n\nTip\n\n\n\nAnother look at conditional probability is \\[P(A \\text{ and } B) = P(B|A) \\cdot P(A)\\] This is read as “The probability of the intersection \\(A\\) and \\(B\\) is the probability of event \\(B\\) conditioned on event \\(A\\).”\n\n\n\n\n\n\n\n\nTip\n\n\n\nMoreover, if we consider how if event \\(B\\) is dependent on event \\(A\\), then sometimes \\(B\\) happens when \\(A\\) happens and sometimes when \\(A\\) does not occur. More succinctly, the total probability of event \\(B\\) is \\[P(B) = P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nStaring with the conditional probability formula \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] Bayes’ Rule combines the ideas of conditioned probability and total probability as \\[P(A|B) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(B)} = \\displaystyle\\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})}\\]"
  },
  {
    "objectID": "posts/05_-_Bayes_Rule/Math_32_05_Bayes_Rule.html#a-deep-dive",
    "href": "posts/05_-_Bayes_Rule/Math_32_05_Bayes_Rule.html#a-deep-dive",
    "title": "5: Bayes’ Rule (Concepts)",
    "section": "A Deep Dive",
    "text": "A Deep Dive\n\nExampleTree DiagramNumeratorDenominatorBayes’ Rule\n\n\nAn executive has their blood tested for boneitis. Let \\(B\\) be the event that an executive has the disease, and let \\(T\\) be the event that the test returns positive. Laboratory trials yielded the following information:\n\\[P(T|B) = 0.70 \\quad\\text{and}\\quad P(T|B^{c}) = 0.10\\]\nAssume a prior probability of \\(P(B) = 0.0032\\). Compute \\(P(B|T)\\)\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram"
  },
  {
    "objectID": "posts/05_-_Bayes_Rule/Math_32_05_Bayes_Rule.html#more-practice",
    "href": "posts/05_-_Bayes_Rule/Math_32_05_Bayes_Rule.html#more-practice",
    "title": "5: Bayes’ Rule (Concepts)",
    "section": "More Practice",
    "text": "More Practice\n\nExampleTree DiagramNumeratorDenominatorBayes’ Rule\n\n\nAn executive has their blood tested for boneitis. Let \\(B\\) be the event that an executive has the disease, and let \\(T\\) be the event that the test returns positive. Laboratory trials yielded the following information:\n\\[P(T|B) = 0.70 \\quad\\text{and}\\quad P(T|B^{c}) = 0.10\\]\nAssume a prior probability of \\(P(B) = 0.0032\\). Compute \\(P(B|T^{c})\\)\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram"
  },
  {
    "objectID": "posts/05_-_Bayes_Rule/Math_32_05_Bayes_Rule.html#example-monty-hall-problem",
    "href": "posts/05_-_Bayes_Rule/Math_32_05_Bayes_Rule.html#example-monty-hall-problem",
    "title": "5: Bayes’ Rule (Concepts)",
    "section": "Example: Monty Hall Problem",
    "text": "Example: Monty Hall Problem\n\n\n\n\nMonty Hall asks you to choose one of three doors. One of the doors hides a prize and the other two doors have no prize. You state out loud which door you pick, but you don’t open it right away.\n“Monty opens one of the other two doors, and there is no prize behind it.\n“At this moment, there are two closed doors, one of which you picked. The prize is behind one of the closed doors, but you don’t know which one. Monty asks you, ‘Do you want to switch doors?’”\n\nswitch doors\ndo not switch doors"
  },
  {
    "objectID": "posts/05_-_Bayes_Rule/Math_32_05_Bayes_Rule.html#looking-ahead",
    "href": "posts/05_-_Bayes_Rule/Math_32_05_Bayes_Rule.html#looking-ahead",
    "title": "5: Bayes’ Rule (Concepts)",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Jan. 27:\n\nWHW2\nJHW0\nCLO (survey)\n\nand the before-lecture quizzes\n\nExam 1 will be on Wed., Mar. 1"
  },
  {
    "objectID": "posts/06_-_Bayes_Rule_2/Math_32_06_Bayes_Rule.html#example-spam-filtering",
    "href": "posts/06_-_Bayes_Rule_2/Math_32_06_Bayes_Rule.html#example-spam-filtering",
    "title": "6: Bayes’ Rule (Examples)",
    "section": "Example: Spam Filtering",
    "text": "Example: Spam Filtering\nIn 2002, Paul Graham used Bayes’ Rule as part of his algorithms to greatly decrease false positive rates of unwanted e-mails (“spam”). Let \\(H^{c}\\) be the event that an e-mail is “spam”. Let \\(W\\) be the event that an e-mail contains a trigger word such as “watches”. Suppose that\n\nthe probability that an e-mail contains that word given that it is spam is 17%\nthe probability that an e-mail contains that word given that it is not spam is 9%\nthe probability that a randomly selected e-mail message is spam is 80%\n\nFind the probability that an e-mail message is spam, given that the trigger word appears."
  },
  {
    "objectID": "posts/06_-_Bayes_Rule_2/Math_32_06_Bayes_Rule.html#example-quality-control",
    "href": "posts/06_-_Bayes_Rule_2/Math_32_06_Bayes_Rule.html#example-quality-control",
    "title": "6: Bayes’ Rule (Examples)",
    "section": "Example: Quality Control",
    "text": "Example: Quality Control\nA manufacturing process produces integrated circuit chips. Over the long run the fraction of bad chips produced by the process is around 20%. Thoroughly testing a chip to determine whether it is good or bad is rather expensive, so a cheap test is tried. All good chips will pass the cheap test, but so will 10% of the bad chips. Given that a chip passes the test, what is the probability that the chip was defective?"
  },
  {
    "objectID": "posts/06_-_Bayes_Rule_2/Math_32_06_Bayes_Rule.html#example-dui-checkpoint",
    "href": "posts/06_-_Bayes_Rule_2/Math_32_06_Bayes_Rule.html#example-dui-checkpoint",
    "title": "6: Bayes’ Rule (Examples)",
    "section": "Example: DUI Checkpoint",
    "text": "Example: DUI Checkpoint\nA breath analyzer, used by the police to test whether drivers exceed the legal limit set for the blood alcohol percentage while driving, is known to satisfy\n\\[P(A|B) = P(A^{c}|B^{c}) = x\\]\nwhere \\(A\\) is the event “breath analyzer indicates that legal limit is exceeded” and \\(B\\) “driver’s blood alcohol percentage exceeds legal limit.” On Saturday nights, about 4% of the drivers are known to exceed the limit.\n\nDescribe in words the meaning of \\(P(B|A)\\)\nDetermine \\(P(B|A)\\) if \\(x = 0.90\\)\nHow big should \\(x\\) be so that \\(P(B|A) \\geq 0.95\\)?"
  },
  {
    "objectID": "posts/06_-_Bayes_Rule_2/Math_32_06_Bayes_Rule.html#example-disease-outbreak",
    "href": "posts/06_-_Bayes_Rule_2/Math_32_06_Bayes_Rule.html#example-disease-outbreak",
    "title": "6: Bayes’ Rule (Examples)",
    "section": "Example: Disease Outbreak",
    "text": "Example: Disease Outbreak\nSuppose that at UC Merced, there is a two percent chance that a freshman has herpes at the end of the school year. Let \\(H\\) be the event of having the virus, while \\(C\\) represents the event that the freshman is from the Cathedral dorm. Among the herpes carriers, the probably of being a Cathedral resident is 32%. Among those free of disease, the probably of being a Cathedral resident is 13%. What is the probability that a freshman has herpes, given that you know that he or she lived in the Cathedral dorm?"
  },
  {
    "objectID": "posts/06_-_Bayes_Rule_2/Math_32_06_Bayes_Rule.html#generalized-bayes-rule",
    "href": "posts/06_-_Bayes_Rule_2/Math_32_06_Bayes_Rule.html#generalized-bayes-rule",
    "title": "6: Bayes’ Rule (Examples)",
    "section": "Generalized Bayes’ Rule",
    "text": "Generalized Bayes’ Rule\nIf we are conditioning \\(B\\) on an event \\(A\\), where the latter can be partitioned into several subsets,\n\\[A = \\{ A_{1}, A_{2}, ..., A_{j} \\}\\]\nthen the total probability is\n\\[P(B) = P(B|A_{1}) \\cdot P(A_{1}) + P(B|A_{2}) \\cdot P(A_{2}) + ... + P(B|A_{n}) \\cdot P(A_{n})\\]\nand Bayes Rule for computing the probability of \\(A_{i}\\) given \\(B\\) becomes\n\\[P(A_{i}|B) = \\displaystyle\\frac{ P(B|A_{i}) \\cdot P(A_{i}) }{ P(B|A_{1}) \\cdot P(A_{1}) + P(B|A_{2}) \\cdot P(A_{2}) + ... + P(B|A_{n}) \\cdot P(A_{n}) }\\]"
  },
  {
    "objectID": "posts/06_-_Bayes_Rule_2/Math_32_06_Bayes_Rule.html#bayesian-odds",
    "href": "posts/06_-_Bayes_Rule_2/Math_32_06_Bayes_Rule.html#bayesian-odds",
    "title": "6: Bayes’ Rule (Examples)",
    "section": "Bayesian Odds",
    "text": "Bayesian Odds\n\n\n\n\n\n\nNote\n\n\n\nThe Bayesian odds of event \\(A\\) to event \\(B\\) given that event \\(C\\) has already taken place is \\[\\displaystyle\\frac{ P(A|C) }{ P(B|C) }\\]"
  },
  {
    "objectID": "posts/06_-_Bayes_Rule_2/Math_32_06_Bayes_Rule.html#looking-ahead",
    "href": "posts/06_-_Bayes_Rule_2/Math_32_06_Bayes_Rule.html#looking-ahead",
    "title": "6: Bayes’ Rule (Examples)",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Feb. 3:\n\nWHW3\nJHW1\nIdentity Statement (short essay)\n\nand the before-lecture quizzes\n\nExam 1 will be on Wed., Mar. 1\nDiscussion section 24D (Wed., 530 PM) now meets in GRAN 135"
  },
  {
    "objectID": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#review-summation-notation",
    "href": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#review-summation-notation",
    "title": "07: Measures of Centrality",
    "section": "Review: Summation Notation",
    "text": "Review: Summation Notation\nThe Fibonnaci Sequence \\(\\{1, 1, 2, 3, 5, 8, 13, ...\\}\\) is an example of a sequence where\n\\(F_{1} = 1, F_{2} = 1, F_{n} = F_{n-1} + F_{n-2}\\)\nCompute\n\n\\(\\displaystyle\\sum_{i=1}^{1} a_{i}\\)\n\\(\\displaystyle\\sum_{i=1}^{2} a_{i}\\)\n\\(\\displaystyle\\sum_{i=1}^{3} a_{i}\\)\n\\(\\displaystyle\\sum_{i=1}^{4} a_{i}\\)"
  },
  {
    "objectID": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#populations-versus-samples",
    "href": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#populations-versus-samples",
    "title": "07: Measures of Centrality",
    "section": "Populations versus Samples",
    "text": "Populations versus Samples\nWe tend to study a relatively small sample to understand the behavior of a much larger population."
  },
  {
    "objectID": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#mean-average",
    "href": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#mean-average",
    "title": "07: Measures of Centrality",
    "section": "Mean (Average)",
    "text": "Mean (Average)\n\n“mean” and “average” are synonymous and will be used interchangably\nThe mean of \\(\\{ x_{1}, x_{2}, ..., x_{n} \\}\\) is denoted by\n\nGreek letter \\(\\mu\\) (“mu”) for a population mean (where we know all of the elements)\nAnglicized \\(\\bar{x}\\) (“x bar”) for a sample mean (where we are working with a sample of data)\n\nTo calculate the mean\n\nAdd up all of the numbers\nDivide by the amount of numbers\n\n\n\\[\\mu = \\displaystyle\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N} x_{i} \\quad\\text{or}\\quad \\bar{x} = \\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} x_{i}\\]"
  },
  {
    "objectID": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#example-one-die",
    "href": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#example-one-die",
    "title": "07: Measures of Centrality",
    "section": "Example: One Die",
    "text": "Example: One Die\n\nFind the mean of the roll of one six-sided die.\nFind the mean of the sample \\(\\{21, 22, 23, 32\\}\\)."
  },
  {
    "objectID": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#median",
    "href": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#median",
    "title": "07: Measures of Centrality",
    "section": "Median",
    "text": "Median\nThe median of an ordered, discrete set of numbers is the number in the middle. If there are an even amount of data, then the median is the average of the middle two numbers in the ordered data set.\n\nCompute the median of \\(\\{1, 2, 1, 5, 3\\}\\)\nCompute the median of \\(\\{1, 1, 2, 3, 5, 8\\}\\)"
  },
  {
    "objectID": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#weighted-mean",
    "href": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#weighted-mean",
    "title": "07: Measures of Centrality",
    "section": "Weighted Mean",
    "text": "Weighted Mean\n\\[\\bar{x} = \\displaystyle\\frac{ \\displaystyle\\sum_{i=1}^{n} {\\color{red}w_{i}} \\cdot {\\color{blue}x_{i}}  }{ \\displaystyle\\sum_{i=1}^{n} {\\color{red}w_{i}} }\\]"
  },
  {
    "objectID": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#example-cal-kulas",
    "href": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#example-cal-kulas",
    "title": "07: Measures of Centrality",
    "section": "Example: Cal Kulas",
    "text": "Example: Cal Kulas\n\n\n\nSetting\nGoing into the final exam for a Statistics course, Cal Kulas had earned the following marks in the other categories.\n\n\n\n\n\nCategory\n\n\nWeight\n\n\nCal Kulas\n\n\n\n\n\n\nattendance\n\n\n10%\n\n\n95%\n\n\n\n\nquizzes\n\n\n20%\n\n\n75%\n\n\n\n\nmidterms\n\n\n25%\n\n\n60%\n\n\n\n\nproject\n\n\n20%\n\n\n90%\n\n\n\n\n\n\n\n\nTasks\n\nWhat is his current grade in the course?\nWhat does Cal Kulas need on the final exam so that he earns at least 80% in the course?"
  },
  {
    "objectID": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#review-summation-notation-1",
    "href": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#review-summation-notation-1",
    "title": "07: Measures of Centrality",
    "section": "Review: Summation Notation",
    "text": "Review: Summation Notation\nThe Fibonnaci Sequence \\(\\{1, 1, 2, 3, 5, 8, 13, ...\\}\\) is an example of a sequence where\n\\(F_{1} = 1, F_{2} = 1, F_{n} = F_{n-1} + F_{n-2}\\)\nCompute\n\n\\(\\displaystyle\\prod_{i=1}^{1} a_{i}\\)\n\\(\\displaystyle\\prod_{i=1}^{2} a_{i}\\)\n\\(\\displaystyle\\prod_{i=1}^{3} a_{i}\\)\n\\(\\displaystyle\\prod_{i=1}^{4} a_{i}\\)"
  },
  {
    "objectID": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#for-formulas",
    "href": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#for-formulas",
    "title": "07: Measures of Centrality",
    "section": "For Formulas",
    "text": "For Formulas\nIn particular, sigma and product notation will allow us to express probability formulas more efficiently. For example, the independence formula for two events \\(A\\) and \\(B\\),\n\\[P(AB) = P(A) \\cdot P(B)\\] becomes the following for \\(n\\) independent events:\n\\[P\\left(  \\{X_{i}\\}_{i=1}^{n} \\right) = \\displaystyle\\prod_{i=1}^{n} P(X_{i})\\]"
  },
  {
    "objectID": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#looking-ahead",
    "href": "posts/07_-_Measures of Centrality/Math_32_07_Centrality.html#looking-ahead",
    "title": "07: Measures of Centrality",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Feb. 3:\n\nWHW3\nJHW1\nIdentity Statement (short essay)\n\nand the before-lecture quizzes\n\nExam 1 will be on Wed., Mar. 1\nDiscussion section 24D (Wed., 530 PM) now meets in GRAN 135\n\n\n\n\nEven NASA makes mistakes!\n\n\nsource"
  },
  {
    "objectID": "posts/08_-_Variance/Math_32_08_Variance.html#overview",
    "href": "posts/08_-_Variance/Math_32_08_Variance.html#overview",
    "title": "08: Variance",
    "section": "Overview",
    "text": "Overview\n\\[s^{2} = \\displaystyle\\frac{1}{n-1}\\displaystyle\\sum_{i = 1}^{n} (x_{i} - \\bar{x})^{2}\\]\nToday’s main questions are “What is variance and what is a standard deviation?” We will go through\n\nthe formulas and calculations\ndemostrations\napplications (word problems)"
  },
  {
    "objectID": "posts/08_-_Variance/Math_32_08_Variance.html#notation",
    "href": "posts/08_-_Variance/Math_32_08_Variance.html#notation",
    "title": "08: Variance",
    "section": "Notation",
    "text": "Notation\n\n\n\n\nWe tend to study a relatively small sample to understand the behavior of a much larger population."
  },
  {
    "objectID": "posts/08_-_Variance/Math_32_08_Variance.html#example-nathans-hot-dog-eating-contest",
    "href": "posts/08_-_Variance/Math_32_08_Variance.html#example-nathans-hot-dog-eating-contest",
    "title": "08: Variance",
    "section": "Example: Nathan’s Hot Dog Eating Contest",
    "text": "Example: Nathan’s Hot Dog Eating Contest\n\n\nEach year on July 4, the Nathan’s Hot Dog Eating Contest takes place on Coney Island in New York. The rule is simple: eat as many hot dogs (and buns) as you can in 10 minutes. The past 5 winning amounts were: 63, 70, 72, 74, 71. Compute the variance.\n\\[s^{2} = \\displaystyle\\frac{1}{n-1}\\displaystyle\\sum_{i = 1}^{n} (x_{i} - \\bar{x})^{2}\\]"
  },
  {
    "objectID": "posts/08_-_Variance/Math_32_08_Variance.html#units",
    "href": "posts/08_-_Variance/Math_32_08_Variance.html#units",
    "title": "08: Variance",
    "section": "Units?",
    "text": "Units?\n\n\n\nsquare hot dogs?"
  },
  {
    "objectID": "posts/08_-_Variance/Math_32_08_Variance.html#demostrations",
    "href": "posts/08_-_Variance/Math_32_08_Variance.html#demostrations",
    "title": "08: Variance",
    "section": "Demostrations",
    "text": "Demostrations\n\nSetupRA to BC to D\n\n\nFor each of the following sets\n\n\\(A = \\{1, 2, 3, 4, 5, 6, 7\\}\\)\n\\(B = \\{3, 4, 5, 6, 7, 8, 9\\}\\)\n\\(C = \\{-3, -2, -1, 0, 1, 2, 3\\}\\)\n\\(D = \\{-9, -6, -3, 0, 3, 6, 9\\}\\)\n\nwe will compute the sample mean, sample median, and sample standard deviation.\n\n\n\nA <- seq(1, 7, 1)\nB <- A + 2\nC <- seq(-3, 3, 3)\nD <- 3*C\n\n\n\n\nmean(A)\n\n[1] 4\n\nmean(B)\n\n[1] 6\n\n\n\nmedian(A)\n\n[1] 4\n\nmedian(B)\n\n[1] 6\n\n\n\nsd(A)\n\n[1] 2.160247\n\nsd(B)\n\n[1] 2.160247\n\n\n\n\n\nmean(C)\n\n[1] 0\n\nmean(D)\n\n[1] 0\n\n\n\nmedian(C)\n\n[1] 0\n\nmedian(D)\n\n[1] 0\n\n\n\nsd(C)\n\n[1] 3\n\nsd(D)\n\n[1] 9"
  },
  {
    "objectID": "posts/08_-_Variance/Math_32_08_Variance.html#standardization",
    "href": "posts/08_-_Variance/Math_32_08_Variance.html#standardization",
    "title": "08: Variance",
    "section": "Standardization",
    "text": "Standardization\n\\[z = \\displaystyle\\frac{x - \\mu}{\\sigma}\\]\nTo standardize data, compute a z-score by\n\nsubtracting by the mean\nthen dividing by the standard deviation\n\nThis calculation is considered to be “unitless”, and the units are usually said as “[number of] standard deviations above/below the mean”\nMost data falls within two standard deviations of the mean,\n\\[\\text{usually } z \\in (-2, 2)\\]\nbut \\(z \\in (-\\infty, \\infty)\\)"
  },
  {
    "objectID": "posts/08_-_Variance/Math_32_08_Variance.html#example-dating-website-data",
    "href": "posts/08_-_Variance/Math_32_08_Variance.html#example-dating-website-data",
    "title": "08: Variance",
    "section": "Example: Dating Website Data",
    "text": "Example: Dating Website Data\n\nScenario 1Scenario 2\n\n\n\nAccording to OkCupid data, if men rate women on a scale from 1 = “least attractive” to 7 = “most attractive”, the average score is 3.99 with a sample standard deviation of 1.6401.\n\nWhat is the \\(z\\)-score of a woman rated a “6”?\nWhat is the attractiveness score of a woman at a \\(z\\)-score of 1.5?\n\n\n\n\nAccording to OkCupid data, if women rate men on a scale from 1 = “least attractive” to 7 = “most attractive”, the average score is 2.43 with a sample standard deviation of 1.2510.\n\nWhat is the \\(z\\)-score of a man rated a “6”?\nWhat is the attractiveness score of a man at a \\(z\\)-score of 1.5?"
  },
  {
    "objectID": "posts/08_-_Variance/Math_32_08_Variance.html#looking-ahead",
    "href": "posts/08_-_Variance/Math_32_08_Variance.html#looking-ahead",
    "title": "08: Variance",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Feb. 3:\n\nWHW3\nJHW1\nIdentity Statement (short essay)\n\nand the before-lecture quizzes\n\nExam 1 will be on Wed., Mar. 1\n\nmore information will be in the weekly announcements\n\n\n\nsource"
  },
  {
    "objectID": "posts/09_-_Expectation/Math_32_09_Expectation.html#example-demographics",
    "href": "posts/09_-_Expectation/Math_32_09_Expectation.html#example-demographics",
    "title": "09: Expectation",
    "section": "Example: Demographics",
    "text": "Example: Demographics\n\nSuppose that all of the students in Math 32 are between ages 19 and 21 inclusively with the following distribution:\n\nAge 19: 35%\nAge 20: 45%\nAge 21: 20%\n\nRewrite the data as a discrete mass function."
  },
  {
    "objectID": "posts/09_-_Expectation/Math_32_09_Expectation.html#discrete-probability-distributions",
    "href": "posts/09_-_Expectation/Math_32_09_Expectation.html#discrete-probability-distributions",
    "title": "09: Expectation",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\n\n\n\n\n\n\nNote\n\n\n\nA discrete probability distribution is a population where we can list the possible values\n\\[X = \\{ x_{1}, x_{2}, x_{3}, ... x_{n} \\}\\]\nand measure the respective probabilities\n\\[P(X = x_{1}), P(X = x_{2}), P(X = x_{3}), ..., P(X = x_{n})\\]\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs usual, the probabilities rules include that each probability is between zero and one inclusively\n\\[0 \\leq P(X = x_{i}) \\leq 1\\]\nand that all probabilities add up to 100 percent\n\\[\\displaystyle\\sum_{x \\in X} P(X = x) = 1\\]"
  },
  {
    "objectID": "posts/09_-_Expectation/Math_32_09_Expectation.html#expectation",
    "href": "posts/09_-_Expectation/Math_32_09_Expectation.html#expectation",
    "title": "09: Expectation",
    "section": "Expectation",
    "text": "Expectation\n\n\n\n\n\n\nNote\n\n\n\nFor a random variable \\(X\\) (understood through a discrete probability distribution), its expected value is \\[\\mu = \\text{E}[{\\color{blue}X}] = \\displaystyle\\sum_{x \\in X} {\\color{blue}x} \\cdot {\\color{red}P(X = x)}\\]\n\n\n\nCompute the expected value of the roll of one six-sided die."
  },
  {
    "objectID": "posts/09_-_Expectation/Math_32_09_Expectation.html#variance",
    "href": "posts/09_-_Expectation/Math_32_09_Expectation.html#variance",
    "title": "09: Expectation",
    "section": "Variance",
    "text": "Variance\n\n\n\n\n\n\nNote\n\n\n\nThe variance of a random variable \\(X\\) is defined as the expected squared deviation from the mean\n\\[\\text{Var}(X) = \\text{E}[(X - \\text{E}[X])^{2}]\\]\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe above theoretical formula is built from first principles and is good for building the math foundation. However, the following practical formula is better for hand calculations and computer calculations.\n\n\nClaim:\n\\[\\text{Var}(X) = \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2}\\]"
  },
  {
    "objectID": "posts/09_-_Expectation/Math_32_09_Expectation.html#example-boxing-bets",
    "href": "posts/09_-_Expectation/Math_32_09_Expectation.html#example-boxing-bets",
    "title": "09: Expectation",
    "section": "Example: Boxing Bets",
    "text": "Example: Boxing Bets\n\n\nBefore watching a boxing match, my friends and I made bets over which round the fight would end. A boxing match lasts up to 12 rounds. Each gambler pays $5 and is assigned a round randomly. The winner garners the whole pot of money. What is the expected value of the bet? What is the variance of the bet?"
  },
  {
    "objectID": "posts/09_-_Expectation/Math_32_09_Expectation.html#looking-ahead",
    "href": "posts/09_-_Expectation/Math_32_09_Expectation.html#looking-ahead",
    "title": "09: Expectation",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Feb. 10:\n\nWHW4\nJHW2\nDemographics Part 1 (survey)\n\nBe mindful of before-lecture quizzes\n\nNo lecture session for Math 32:\n\nFeb 20, Mar 10, Mar 24\n\nExam 1 will be on Wed., Mar. 1\n\nmore information in weekly announcements\n\n\n\nsource"
  },
  {
    "objectID": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#bernoulli-trials",
    "href": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#bernoulli-trials",
    "title": "10: Binomial Distribution",
    "section": "Bernoulli Trials",
    "text": "Bernoulli Trials\nTo continue our exploration of discrete distributions, we will look at situations that have two disjoint possibilities.\n\n\n\n\n\n\nBernoulli Trials\n\n\n\nFor math symbols to represent a Bernoulli trial, the events \\(\\{1, 0\\}\\) have respective probabilities \\(p\\) and \\(1-p\\).\n\n\nFor example, for one flip of a coin \\[P(\\text{heads}) = p, \\quad P(\\text{tails}) = 1-p\\]\n\n\n\none coin, but not necessarily fair"
  },
  {
    "objectID": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#arrangements",
    "href": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#arrangements",
    "title": "10: Binomial Distribution",
    "section": "Arrangements",
    "text": "Arrangements\n\n\n\n\n\n\nPermutations\n\n\n\nPermutations (and the number of permutations) are the arrangements when order matters\n\n\n\n\n\n\n\n\nCombinations\n\n\n\nCombinations (and the number of combinations) are the arrangements when order does not matter\n\n\nFlipping 3 fair coins, what is the probability that heads will be observed exactly twice?"
  },
  {
    "objectID": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#possibility-spaces",
    "href": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#possibility-spaces",
    "title": "10: Binomial Distribution",
    "section": "Possibility Spaces",
    "text": "Possibility Spaces\n\n3 Coins4 coins5 coins\n\n\n\ncoin <- c(\"H\", \"T\")\ndf <- data.frame(expand.grid(coin, coin, coin)) |>\n  tidyr::unite(\"obs\", c(\"Var1\", \"Var2\", \"Var3\"),\n               sep = \"\", remove = FALSE)\n# print\ndput(df$obs)\n\nc(\"HHH\", \"THH\", \"HTH\", \"TTH\", \"HHT\", \"THT\", \"HTT\", \"TTT\")\n\n\n\n\n\ncoin <- c(\"H\", \"T\")\ndf <- data.frame(expand.grid(coin, coin, coin, coin)) |>\n  tidyr::unite(\"obs\", c(\"Var1\", \"Var2\", \"Var3\", \"Var4\"),\n               sep = \"\", remove = FALSE)\n# print\ndput(df$obs)\n\nc(\"HHHH\", \"THHH\", \"HTHH\", \"TTHH\", \"HHTH\", \"THTH\", \"HTTH\", \"TTTH\", \n\"HHHT\", \"THHT\", \"HTHT\", \"TTHT\", \"HHTT\", \"THTT\", \"HTTT\", \"TTTT\"\n)\n\n\n\n\n\ncoin <- c(\"H\", \"T\")\ndf <- data.frame(expand.grid(coin, coin, coin, coin, coin)) |>\n  tidyr::unite(\"obs\", c(\"Var1\", \"Var2\", \"Var3\", \"Var4\", \"Var5\"),\n               sep = \"\", remove = FALSE)\n# print\ndput(df$obs)\n\nc(\"HHHHH\", \"THHHH\", \"HTHHH\", \"TTHHH\", \"HHTHH\", \"THTHH\", \"HTTHH\", \n\"TTTHH\", \"HHHTH\", \"THHTH\", \"HTHTH\", \"TTHTH\", \"HHTTH\", \"THTTH\", \n\"HTTTH\", \"TTTTH\", \"HHHHT\", \"THHHT\", \"HTHHT\", \"TTHHT\", \"HHTHT\", \n\"THTHT\", \"HTTHT\", \"TTTHT\", \"HHHTT\", \"THHTT\", \"HTHTT\", \"TTHTT\", \n\"HHTTT\", \"THTTT\", \"HTTTT\", \"TTTTT\")"
  },
  {
    "objectID": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#choose",
    "href": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#choose",
    "title": "10: Binomial Distribution",
    "section": "Choose",
    "text": "Choose\n\n\n\\[\\binom{n}{k} = \\displaystyle\\frac{n!}{k!(n-k)!}\\]\n\nsaid ``n choose k’’\nThis choose operator keeps track of the number of permutations in a certain combination\nnote \\(0! = 1\\) (to avoid dividing by zero)\n\n\n\n\n\nfrom The Simpsons"
  },
  {
    "objectID": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#binomial-distribution",
    "href": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#binomial-distribution",
    "title": "10: Binomial Distribution",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\n\n\n\n\nBinomial Distribution\n\n\n\n\\[P(x = k) = \\binom{n}{k} p^{k}(1-p)^{n-k}\\]\n\n\\(0 \\leq k \\leq n\\), where \\(n\\) and \\(k\\) are whole numbers\n\\(0 \\leq p \\leq 1\\)"
  },
  {
    "objectID": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#example-squirtle",
    "href": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#example-squirtle",
    "title": "10: Binomial Distribution",
    "section": "Example: Squirtle",
    "text": "Example: Squirtle\n\n\n\n\n\n\n\n\nBinomial Distribution\n\n\n\n\\[P(x = k) = \\binom{n}{k} p^{k}(1-p)^{n-k}\\]\n\n\\(0 \\leq k \\leq n\\), where \\(n\\) and \\(k\\) are whole numbers\n\\(0 \\leq p \\leq 1\\)\n\n\n\n\nHistorically, Squirtle defeats Charizard 32% of the time. If there are 5 battles, what is the probability that Squirtle wins exactly 2 times?\n\nBattle Arena\n\nbattle <- c(\"S\", \"C\")\ndf <- data.frame(expand.grid(battle, battle, battle, battle, battle)) |>\n  tidyr::unite(\"obs\", c(\"Var1\", \"Var2\", \"Var3\", \"Var4\", \"Var5\"),\n               sep = \"\", remove = FALSE)\n# print\ndput(df$obs)\n\nc(\"SSSSS\", \"CSSSS\", \"SCSSS\", \"CCSSS\", \"SSCSS\", \"CSCSS\", \"SCCSS\", \n\"CCCSS\", \"SSSCS\", \"CSSCS\", \"SCSCS\", \"CCSCS\", \"SSCCS\", \"CSCCS\", \n\"SCCCS\", \"CCCCS\", \"SSSSC\", \"CSSSC\", \"SCSSC\", \"CCSSC\", \"SSCSC\", \n\"CSCSC\", \"SCCSC\", \"CCCSC\", \"SSSCC\", \"CSSCC\", \"SCSCC\", \"CCSCC\", \n\"SSCCC\", \"CSCCC\", \"SCCCC\", \"CCCCC\")\n\n\nBut these observations have different weights!"
  },
  {
    "objectID": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#battle-arena",
    "href": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#battle-arena",
    "title": "10: Binomial Distribution",
    "section": "Battle Arena",
    "text": "Battle Arena\n\nbattle <- c(\"S\", \"C\")\ndf <- data.frame(expand.grid(battle, battle, battle, battle, battle)) |>\n  tidyr::unite(\"obs\", c(\"Var1\", \"Var2\", \"Var3\", \"Var4\", \"Var5\"),\n               sep = \"\", remove = FALSE)\n# print\ndput(df$obs)\n\nc(\"SSSSS\", \"CSSSS\", \"SCSSS\", \"CCSSS\", \"SSCSS\", \"CSCSS\", \"SCCSS\", \n\"CCCSS\", \"SSSCS\", \"CSSCS\", \"SCSCS\", \"CCSCS\", \"SSCCS\", \"CSCCS\", \n\"SCCCS\", \"CCCCS\", \"SSSSC\", \"CSSSC\", \"SCSSC\", \"CCSSC\", \"SSCSC\", \n\"CSCSC\", \"SCCSC\", \"CCCSC\", \"SSSCC\", \"CSSCC\", \"SCSCC\", \"CCSCC\", \n\"SSCCC\", \"CSCCC\", \"SCCCC\", \"CCCCC\")\n\n\nBut these observations have different weights!"
  },
  {
    "objectID": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#example-charizard",
    "href": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#example-charizard",
    "title": "10: Binomial Distribution",
    "section": "Example: Charizard",
    "text": "Example: Charizard\n\n\n\n\n\nCharizard\n\n\n\nHistorically, Charizard defeats Squirtle 68% of the time. If there are 5 battles, what is the probability that Charizard wins exactly 3 times?"
  },
  {
    "objectID": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#symmetry",
    "href": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#symmetry",
    "title": "10: Binomial Distribution",
    "section": "Symmetry",
    "text": "Symmetry\n\nPropertySquirtleCharizardR code\n\n\nThe previous two examples had the same answer, which is true due to a symmetry property in the choose operator:\n\\[\\binom{n}{k} = \\binom{n}{n-k}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk <- 0:5\npk <- dbinom(k, 5, 0.32)\nk_bool <- k == 2\ndf <- data.frame(k, pk, k_bool)\n\ndf |>\n  ggplot(aes(x = k, y = pk, \n             color = k_bool, fill = k_bool)) +\n  geom_bar(stat = \"identity\") +\n  geom_label(aes(x = k, y = pk, \n                 label = round(pk, 4)),\n             color = \"black\", fill = \"white\") +\n  labs(title = \"2 Squirtle Wins\",\n       subtitle = \"n = 5, k = 2, p = 0.32, P(k = 2) = 0.3220\",\n       caption = \"Math 32\",\n       x = \"wins\",\n       y = \"probability\") +\n  scale_color_manual(values = c(\"black\", \"#ca7721\")) +\n  scale_fill_manual(values = c(\"gray70\", \"#297383\")) +\n  theme(\n    legend.position = \"none\",\n    panel.background = element_blank()\n  )\n\n\n\n# plotly::ggplotly(ex2_plot)\n\n\nk <- 0:5\npk <- dbinom(k, 5, 0.68)\nk_bool <- k == 3\ndf <- data.frame(k, pk, k_bool)\n\ndf |>\n  ggplot(aes(x = k, y = pk, \n             color = k_bool, fill = k_bool)) +\n  geom_bar(stat = \"identity\") +\n  geom_label(aes(x = k, y = pk, \n                 label = round(pk, 4)),\n             color = \"black\", fill = \"white\") +\n  labs(title = \"3 Charizard Wins\",\n       subtitle = \"n = 5, k = 3, p = 0.68, P(k = 3) = 0.3220\",\n       caption = \"Math 32\",\n       x = \"wins\",\n       y = \"probability\") +\n  scale_color_manual(values = c(\"black\", \"#de5138\")) +\n  scale_fill_manual(values = c(\"gray70\", \"#e53800\")) +\n  theme(\n    legend.position = \"none\",\n    panel.background = element_blank()\n  )\n\n\n\n# plotly::ggplotly(ex2_plot)\n\n\n\n\n\n\n\n\n\n\nHow do we pick between \\(p\\) and \\(1-p\\)?\n\n\n\nAt first, it does not matter how you define the binomial setting for what corresponds to \\(p\\) and what corresponds to \\(1-p\\), but you need to be consistent in the rest of the task for how you defined your variables and use the value(s) for \\(k\\)."
  },
  {
    "objectID": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#parameters",
    "href": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#parameters",
    "title": "10: Binomial Distribution",
    "section": "Parameters",
    "text": "Parameters\nThe notation \\(X \\sim Ber(p)\\) is read as “random variable \\(X\\) has a Bernoulli distribution with parameter \\(p\\)”. Compute the expected value and variance for a Bernoulli trial."
  },
  {
    "objectID": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#parameters-1",
    "href": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#parameters-1",
    "title": "10: Binomial Distribution",
    "section": "Parameters",
    "text": "Parameters\nThe notation \\(X \\sim Bin(n,p)\\) is read as ``random variable \\(X\\) has a binomial distribution with parameters \\(n\\) and \\(p\\)’’. Compute the expected value and variance for a binomial distribution.\nWe are assuming that the \\(n\\) trials are from each other, where independence in probability means that\n\\[P\\left(  \\{X_{i}\\}_{i=1}^{n} \\right) = \\displaystyle\\prod_{i=1}^{n} P(X_{i})\\]\nIn other words, we are sampling the Bernoulli trial \\(n\\) times with replacement, so we can simply multiply the results from the previous example by \\(n\\).\n\\[\\begin{array}{|c|c|c|}\n\\hline\n\\textbf{mean}               & \\mu & np \\\\ \\hline\n\\textbf{variance}           & \\sigma^{2} & np(1-p) \\\\ \\hline\n\\textbf{standard deviation} & \\sigma & \\sqrt{np(1-p)} \\\\ \\hline\n\\end{array}\\]"
  },
  {
    "objectID": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#looking-ahead",
    "href": "posts/10_-_Binomial Distribution/Math_32_10_Binomial_Distribution.html#looking-ahead",
    "title": "10: Binomial Distribution",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Feb. 10:\n\nWHW4\nJHW2\nDemographics Survey Part 1\n\nBe mindful of before-lecture quizzes\n\nNo lecture session for Math 32:\n\nFeb 20, Mar 10, Mar 24\n\n\nExam 1 will be on Wed., Mar. 1\n\nmore information in weekly announcements\n\n\nsource"
  },
  {
    "objectID": "posts/11_-_Cumulative Calculations/Math_32_11_Cumulative_Computation.html#probability-mass-function",
    "href": "posts/11_-_Cumulative Calculations/Math_32_11_Cumulative_Computation.html#probability-mass-function",
    "title": "11: Cumulative Computation",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\nLast time, we developed the probability mass function for the binomial distribution. The probability of choosing \\(k\\) observations among a sample size of \\(n\\), each observation with prior probability \\(p\\), is given by\n\\[P(k) = \\binom{n}{k}p^{k}(1-p)^{n-k}\\]\nNote the usual properties of probability:\n\neach probability is between zero and one (inclusively)\n\n\\[0 \\leq P(k) \\leq 1 \\quad\\text{for each } k\\] - all probabilities add up to 100%\n\\[1 = \\displaystyle\\sum_{k = 0}^{n} \\binom{n}{k}p^{k}(1-p)^{n-k}\\]"
  },
  {
    "objectID": "posts/11_-_Cumulative Calculations/Math_32_11_Cumulative_Computation.html#from-one-to-many",
    "href": "posts/11_-_Cumulative Calculations/Math_32_11_Cumulative_Computation.html#from-one-to-many",
    "title": "11: Cumulative Computation",
    "section": "From One to Many",
    "text": "From One to Many\n\nExactlyAt MostExactlyMore Than\n\n\nThere are 4 parking spaces in front of the boba place. Suppose that each parking space tends to be occupied about 57 percent of the time. What is the probability that exactly 3 of the parking spaces are open?\n\n\n\nboba!\n\n\n\\[k \\in \\{0, 1, 2, 3, 4\\}\\]\n\n\nThere are 4 parking spaces in front of the boba place. Suppose that each parking space tends to be occupied about 57 percent of the time. What is the probability that at most 2 of the parking spaces are open?\n \\[k \\in \\{0, 1, 2, 3, 4\\}\\]\n\n\nThere are 32 parking spaces in a row in a UC Merced parking lot. Suppose that each parking space tends to be occupied about 81 percent of the time. What is the probability that exactly 4 of the parking spaces are open?\n\n\n\nparking\n\n\n\\[k \\in \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\\}\\] \\[\\cup \\{17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32\\}\\]\n\n\nThere are 32 parking spaces in a row in a UC Merced parking lot. Suppose that each parking space tends to be occupied about 81 percent of the time. What is the probability that more than 5 of the parking spaces are open?\n\n\n\nparking\n\n\n\\[k \\in \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\\}\\] \\[\\cup \\{17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32\\}\\]"
  },
  {
    "objectID": "posts/11_-_Cumulative Calculations/Math_32_11_Cumulative_Computation.html#leveraging-complements",
    "href": "posts/11_-_Cumulative Calculations/Math_32_11_Cumulative_Computation.html#leveraging-complements",
    "title": "11: Cumulative Computation",
    "section": "Leveraging Complements",
    "text": "Leveraging Complements\nThere are 32 parking spaces in a row in a UC Merced parking lot. Suppose that each parking space tends to be occupied about 97 percent of the time. What is the probability that at least one of the parking spaces is open?\n\n\n\nparking\n\n\n\\[k \\in \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\\}\\] \\[\\cup \\{17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32\\}\\]"
  },
  {
    "objectID": "posts/11_-_Cumulative Calculations/Math_32_11_Cumulative_Computation.html#looking-ahead",
    "href": "posts/11_-_Cumulative Calculations/Math_32_11_Cumulative_Computation.html#looking-ahead",
    "title": "11: Cumulative Computation",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Feb. 10:\n\nWHW4\nJHW2\nDemographics Survey Part 1\n\nBe mindful of before-lecture quizzes\n\nNo lecture session for Math 32:\n\nFeb 20, Mar 10, Mar 24\n\n\nExam 1 will be on Wed., Mar. 1\n\nmore information in weekly announcements\n\n\n\n\nsome found sign in Sausalito"
  },
  {
    "objectID": "posts/12_-_Geometric Distribution/Math_32_12_Geometric_Distribution.html#infinite-support",
    "href": "posts/12_-_Geometric Distribution/Math_32_12_Geometric_Distribution.html#infinite-support",
    "title": "12: Geometric Distribution",
    "section": "Infinite Support",
    "text": "Infinite Support\n\n\nHere let us assume an endless box of chocolates with random selection with replacement of\nWrite out some of the sample space. Let \\(F\\) be the event of choosing a favorite chocolate, so \\(F^{c}\\) is the event of choosing an average chocolate, \\(P(F) = p\\) and \\(P(F^{c}) = 1-p\\). Let \\(k\\) be the amount of chocolates chosen reaching a favorite chocolate\n\n\n\n\nForest Gump"
  },
  {
    "objectID": "posts/12_-_Geometric Distribution/Math_32_12_Geometric_Distribution.html#probability-mass-function",
    "href": "posts/12_-_Geometric Distribution/Math_32_12_Geometric_Distribution.html#probability-mass-function",
    "title": "12: Geometric Distribution",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\n\n\n\n\n\n\nGeometric Distribution Probability Mass Function\n\n\n\nA geometric distribution is a discrete probability distribution with\n\nprobability \\(p\\) for “success”\nprobability \\(1-p\\) for “failure”\nfor \\(k = 0, 1, 2, 3, ...\\)\n“success” on \\((k+1)^{\\text{th}}\\) trial\n\nThe probability mass function (PMF) is\n\\[f(X = k) = (1-p)^{k}p\\]\nThe cumulative mass function (CMF) is\n\\[F(X \\leq k) = 1 - (1-p)^{k+1}\\]"
  },
  {
    "objectID": "posts/12_-_Geometric Distribution/Math_32_12_Geometric_Distribution.html#cumulative-calculations",
    "href": "posts/12_-_Geometric Distribution/Math_32_12_Geometric_Distribution.html#cumulative-calculations",
    "title": "12: Geometric Distribution",
    "section": "Cumulative Calculations",
    "text": "Cumulative Calculations\n\n\n\n\n\nTed Mosby\n\n\n\nTed Mosby is setting up his weekend plans. Barney convinced him to try out an app called Tinder. “Ted Mosby, Architect” has a 2/3 chance of setting up a date among those he “swipes right”. Suppose that Ted stops using Tinder this session once he sets up a date.\n\nCompute the probability that Ted needs exactly 4 ``swipes right’’ to set up a date.\nCompute the probability that Ted needs at most 4 ``swipes right’’ to set up a date."
  },
  {
    "objectID": "posts/12_-_Geometric Distribution/Math_32_12_Geometric_Distribution.html#complementary-cumulative-mass-function",
    "href": "posts/12_-_Geometric Distribution/Math_32_12_Geometric_Distribution.html#complementary-cumulative-mass-function",
    "title": "12: Geometric Distribution",
    "section": "Complementary Cumulative Mass Function",
    "text": "Complementary Cumulative Mass Function\n\n\n\n\n\n\nComplementary Cumulative Mass Function\n\n\n\nWhen modeling with a geometric distribution—i.e. \\(X \\sim Geo(p)\\)—the probability that “success” takes more than \\(k\\) trials is\n\\[P(X > k) = (1-p)^{k+1}\\]\n\n\n\n\n\n\n\n\nComplementary Cumulative Mass Function\n\n\n\nWhen modeling with a geometric distribution—i.e. \\(X \\sim Geo(p)\\)—the probability that “success” takes at least \\(k\\) trials is\n\\[P(X \\geq k) = (1-p)^{k}\\]\n\n\n\nMore ThanConditional ProbabilityConditional Probability 2\n\n\n\n\n\n\n\nRebecca Bunch\n\n\n\nRebecca Bunch follows Josh Chan, whom she briefly dated as a teenager, by moving to West Covina, California. Suppose that it may take a while for Rebecca and Josh to reunite and there is a 29 percent chance of them meeting during any particular week. What is the probability that it will take Rebecca more than 3 weeks to reunite with Josh?\n\n\n\n\n\n\n\n\n\nRebecca Bunch\n\n\n\nWhat is the probability that it will take Rebecca at least 9 weeks to reunite with Josh given that she has already spent at least 5 weeks in West Covina?\n\n\n\n\n\n\n\n\n\nRebecca Bunch\n\n\n\nStill holding on to that 29 percent chance of reuniting with Josh, compute the probability that it will take Rebecca at least 36 weeks to reunite with Josh given that she has already spent at least 32 weeks in West Covina."
  },
  {
    "objectID": "posts/12_-_Geometric Distribution/Math_32_12_Geometric_Distribution.html#memoryless-property",
    "href": "posts/12_-_Geometric Distribution/Math_32_12_Geometric_Distribution.html#memoryless-property",
    "title": "12: Geometric Distribution",
    "section": "Memoryless Property",
    "text": "Memoryless Property\nThe previous two examples demonstrated the memoryless property.\n\n\n\n\n\n\nMemoryless Property\n\n\n\nThe geometric distribution is the only discrete probability distribution that has the memoryless property:\n\\[P(X \\geq a + b | X \\geq b) = P(X \\geq a)\\]"
  },
  {
    "objectID": "posts/12_-_Geometric Distribution/Math_32_12_Geometric_Distribution.html#looking-ahead",
    "href": "posts/12_-_Geometric Distribution/Math_32_12_Geometric_Distribution.html#looking-ahead",
    "title": "12: Geometric Distribution",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Feb. 10:\n\nWHW5\nJHW3\nDemographics Survey Part 2\n\nBe mindful of before-lecture quizzes\n\nNo lecture session for Math 32:\n\nFeb 20, Mar 10, Mar 24\n\n\nExam 1 will be on Wed., Mar. 1\n\nmore information in weekly announcements"
  },
  {
    "objectID": "posts/13_-_Continuous Distributions/Math_32_13_Continuous_Distributions.html#continuous-variables",
    "href": "posts/13_-_Continuous Distributions/Math_32_13_Continuous_Distributions.html#continuous-variables",
    "title": "13: Continuous Distributions",
    "section": "Continuous Variables",
    "text": "Continuous Variables\n\n\n\nImage Credit: G2 Learing Hub\n\n\n\nA discrete variable is “countable” (has values that can be in a list)\nA continuous variable has values that cannot be written as a list (“uncountable”)"
  },
  {
    "objectID": "posts/13_-_Continuous Distributions/Math_32_13_Continuous_Distributions.html#uniform-distribution",
    "href": "posts/13_-_Continuous Distributions/Math_32_13_Continuous_Distributions.html#uniform-distribution",
    "title": "13: Continuous Distributions",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\nYou arrive at a bus stop at 10 o’clock, knowing that the bus will arrive at some time uniformly distributed between 10:00 and 10:30.\n\nNormalizationFewerMoreConditional\n\n\n\n\nWe build a probability density function (PDF) by ensuring that the area under the curve equals 100 percent (i.e. one square unit).\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait fewer than 12 minutes?\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait longer than 10 minutes?\n\n\n\n\n\n\n\n\nIf at 10:15 the bus has not yet arrived, what is the probability that you will have to wait at least an additional 10 minutes?"
  },
  {
    "objectID": "posts/13_-_Continuous Distributions/Math_32_13_Continuous_Distributions.html#linear-distribution",
    "href": "posts/13_-_Continuous Distributions/Math_32_13_Continuous_Distributions.html#linear-distribution",
    "title": "13: Continuous Distributions",
    "section": "Linear Distribution",
    "text": "Linear Distribution\n\nYou arrive at a bus stop at 10 o’clock, knowing that the bus will arrive at some time linearly distributed between 10:00 and 10:30. The probability density function (PDF) is\n\\[f(x) = \\begin{cases} -\\displaystyle\\frac{1}{450}x + \\displaystyle\\frac{1}{15} & 0 \\leq x \\leq 30 \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\nPDFFewerFewerBetween\n\n\n\n\nProbability Density Function (PDF)\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait fewer than 7 minutes?\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait fewer than 11 minutes?\n\n\n\n\n\n\n\n\nWhat is the probability that you will have to wait between 7 and 11 minutes?"
  },
  {
    "objectID": "posts/13_-_Continuous Distributions/Math_32_13_Continuous_Distributions.html#cumulative-density-function",
    "href": "posts/13_-_Continuous Distributions/Math_32_13_Continuous_Distributions.html#cumulative-density-function",
    "title": "13: Continuous Distributions",
    "section": "Cumulative Density Function",
    "text": "Cumulative Density Function\n\nThere are no nonzero probabilities to the left. The CDF “starts with zero” probability. Here, \\(F(0) = 0\\)\nSince all probabilities add up to 100%, the CDF ends at one”. Here, \\(F(30) = 1\\)\n\n\n\n\nCDF"
  },
  {
    "objectID": "posts/13_-_Continuous Distributions/Math_32_13_Continuous_Distributions.html#looking-ahead",
    "href": "posts/13_-_Continuous Distributions/Math_32_13_Continuous_Distributions.html#looking-ahead",
    "title": "13: Continuous Distributions",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Feb. 10:\n\nWHW5\nJHW3\nDemographics Survey Part 2\n\nBe mindful of before-lecture quizzes\n\nNo lecture session for Math 32:\n\nFeb 20, Mar 10, Mar 24\n\nExam 1 will be on Wed., Mar. 1\n\nmore information in weekly announcements"
  },
  {
    "objectID": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#inequalities",
    "href": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#inequalities",
    "title": "14: Exponential Distribution",
    "section": "Inequalities",
    "text": "Inequalities\n\n\n\n\n\n\nDiscrete Probability Distributions\n\n\n\nThe inclusive versus exclusive variation in inequalities matter in discrete probability distributions. With a random variable \\(X\\) defined over a support of \\(k = \\{0, 1, 2, 3, ... \\}\\) number of trials,\n\\[P(X < k) = \\displaystyle\\sum_{i=0}^{k-1} P(i), \\quad P(X \\leq k) = \\displaystyle\\sum_{i=0}^{k} P(i)\\]\n\\[P(X < 32) = P(k = 0) + P(k = 1) + \\cdots + P(k = 31)\\]\n\\[P(X \\leq 32) = P(k = 0) + P(k = 1) + \\cdots + P(k = 31) + P(k = 32)\\]\n\n\n\n\n\n\n\n\nContinuous Probability Distributions\n\n\n\nThe inclusive versus exclusive variation in inequalities do not matter with continuous probability distributions.\nClaim: For a random variable \\(X\\) with probability density function \\(f\\), \\[P(X < b) = P(X \\leq b)\\]"
  },
  {
    "objectID": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#motivation",
    "href": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#motivation",
    "title": "14: Exponential Distribution",
    "section": "Motivation",
    "text": "Motivation\n\n\nThere are many situations that are better modeled with an open set \\([a, \\infty)\\) of support, so we can look at the mother function\n\\[f(x) = e^{-x}, \\quad a < x\\]\nwhich has a horizontal asymptote. Next, a rate parameter \\(\\lambda\\) (“lambda”) gives flexibility in models (i.e. the number we plug in for \\(\\lambda\\) depends on the word problem).\n\\[f(x) = e^{-\\lambda x}, \\quad a < x\\]"
  },
  {
    "objectID": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#normalization",
    "href": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#normalization",
    "title": "14: Exponential Distribution",
    "section": "Normalization",
    "text": "Normalization\nWithout much loss of generality, we can shift to a support of \\([0, \\infty)\\). Our next goal is to rescale the function values so that the area under the curve is equal to 100 percent.\n\nFind the value of \\(k\\) so that \\(f\\) is a probability density function."
  },
  {
    "objectID": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#probability-density-function",
    "href": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#probability-density-function",
    "title": "14: Exponential Distribution",
    "section": "Probability Density Function",
    "text": "Probability Density Function\n\n\n\n\n\n\nProbability Density Function\n\n\n\nHaving found the value of the scalar \\(k\\), our probability density function (PDF) is\n\\[\\text{PDF: } f(x) = \\begin{cases} \\lambda e^{-\\lambda x}, & x > 0 \\\\ 0, & \\text{otherwise} \\end{cases}\\]"
  },
  {
    "objectID": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#cumulative-distribution-function",
    "href": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#cumulative-distribution-function",
    "title": "14: Exponential Distribution",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function\n\n\n\n\n\n\nCumulative Distribution Function\n\n\n\nTo continue to think of probabilities as area under a curve, we derive the cumulative distribution function (CDF) as the integral of the probability density function\n\\[F(x) = \\displaystyle\\int_{-\\infty}^{x} \\! f(t) \\, dt = \\displaystyle\\int_{0}^{x} \\! \\lambda e^{-\\lambda t} \\, dt = 1 - e^{-\\lambda x}, \\quad x > 0\\]\nThat is, the CDF of an exponential distribution is\n\\[F(x) = \\begin{cases} 1 - e^{-\\lambda x}, & x > 0 \\\\ 0, & x < 0 \\end{cases}\\]\n\\[~\\]\nThe properties of probability include\n\nWe start with zero probability \\[\\displaystyle\\lim_{x \\to -\\infty} F(x) = 0\\]\nWe end with all probability \\[\\displaystyle\\lim_{x \\to \\infty} F(x) = 1\\]"
  },
  {
    "objectID": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#conventions",
    "href": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#conventions",
    "title": "14: Exponential Distribution",
    "section": "Conventions",
    "text": "Conventions\n\n\n\n\n\n\nConventions\n\n\n\n\nThe models are related with \\(\\lambda = \\displaystyle\\frac{1}{\\beta}\\). Here in Math 32, we will use the model with the rate parameter \\(\\lambda\\) (lambda) to match the convention used by the R programming language"
  },
  {
    "objectID": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#sample-statistics",
    "href": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#sample-statistics",
    "title": "14: Exponential Distribution",
    "section": "Sample Statistics",
    "text": "Sample Statistics\n\nMeanVarianceMedian?Median\n\n\n\n\n\n\n\n\nMean\n\n\n\nRecall that the mean and the expected value are synonymous.\n\\[\\mu = \\text{E}[{\\color{blue}X}] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! {\\color{blue}x} \\cdot f(x) \\, dx = \\lambda\\displaystyle\\int_{0}^{\\infty} \\! {\\color{blue}x} \\cdot e^{-\\lambda x}\\, dx = \\displaystyle\\frac{1}{\\lambda}\\]\nWe have shown that the expected value for \\(X \\sim Exp(\\lambda)\\) is \\(\\text{E}[X] = \\displaystyle\\frac{1}{\\lambda}\\).\n\n\n\n\n\n\n\n\n\n\nVariance\n\n\n\nFurther analogues to the formulas used for discrete probability distributions include the the second moment\n\\[\\text{E}[{\\color{blue}X^{2}}] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! {\\color{blue}x^{2}} \\cdot f(x) \\, dx = \\lambda\\displaystyle\\int_{0}^{\\infty} \\! {\\color{blue}x^{2}} \\cdot e^{-\\lambda x}\\, dx = \\displaystyle\\frac{2}{\\lambda^{2}}\\]\nand it follows that the variance for an exponential distribution with rate parameter \\(\\lambda\\) is\n\\[\\sigma^{2} = \\text{Var}(X) = \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2} = \\displaystyle\\frac{2}{\\lambda^{2}} - \\left(\\displaystyle\\frac{1}{\\lambda}\\right)^{2} = \\displaystyle\\frac{1}{\\lambda^{2}}\\]\nAs usual, the standard deviation is the square root of the variance.\n\\[\\sigma = \\sqrt{ \\displaystyle\\frac{1}{\\lambda^{2}} } = \\displaystyle\\frac{1}{\\lambda}\\]\n\n\n\n\n\n\n\n\n\n\nMedian\n\n\n\nBack in discrete data, if we had an ordered list of data, \\[\\{ x_{(1)}, x_{(2)}, x_{(3)}, ...\\}\\] we said that the median is the value in the ``middle’’ of the list.\nQuery: How do you think we define a median here in the setting of continuous distributions?\n\n\n\n\n\n\n\n\n\n\nMedian\n\n\n\nBack in discrete data, if we had an ordered list of data, \\[\\{ x_{(1)}, x_{(2)}, x_{(3)}, ...\\}\\] we said that the median is the value in the ``middle’’ of the list."
  },
  {
    "objectID": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#waiting-times",
    "href": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#waiting-times",
    "title": "14: Exponential Distribution",
    "section": "Waiting Times",
    "text": "Waiting Times\nLet us now return to the notion of waiting times. Suppose that a friend of yours is going to pick you up for a carpool, and you estimate that he tends to arrive with a mean time of 30 minutes. Assume an exponential distribution.\n\nNormalizationLessMore\n\n\n\n\n\n\n\n\nCareful!\n\n\n\n\\[\\text{Since } \\mu = \\displaystyle\\frac{1}{\\lambda} \\quad\\Rightarrow\\quad 30 = \\displaystyle\\frac{1}{\\lambda} \\quad\\Rightarrow\\quad \\lambda = \\displaystyle\\frac{1}{30}\\]\n\n\n\n\nCompute the probability that your friend will arrive in less than 25 minutes.\n\n\n\n\n\nCompute the probability that your friend will take more than 40 minutes to arrive."
  },
  {
    "objectID": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#memoryless-property",
    "href": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#memoryless-property",
    "title": "14: Exponential Distribution",
    "section": "Memoryless Property",
    "text": "Memoryless Property\n\nExampleProof\n\n\nYou inherit an exquisite cabin in the woods, but on one condition: you must stay in the cabin overnight on the witch’s sabbath—Halloween. The cabin is notorious for housing the ghost of Cal Kulas, and he strikes sometime after the stroke of midnight with a mean time of 60 minutes.\n\nGiven that you have already waited 32 minutes to see the ghost, what is the probability that you will have to wait at least another 10 minutes?\nGiven that you have already waited 181 minutes to see the ghost, what is the probability that you will have to wait at least another 10 minutes?\n\n\n\nClaim: An exponential distribution has the memoryless property \\[P(T > a + b | T > b) = P(T > a)\\]\nProof:\n\\[\\begin{array}{rcl}\n      P(T > a + b | T > b) & = & \\displaystyle\\frac{ P(T > a + b \\text{ and } T > b) }{ P(T > b) } \\\\\n      ~ & = & \\displaystyle\\frac{ P(T > a+b) }{ P(T > b) } \\\\\n      ~ & = & \\displaystyle\\frac{ e^{-\\lambda(a+b)} }{ e^{-\\lambda b} } \\\\\n      ~ & = & e^{-\\lambda a} \\\\\n      ~ & = & P(T > a) \\\\\n    \\end{array}\\]\nClaim: The exponential distribution is the only continuous distribution with the memoryless property.\nProof: (See Math 181)"
  },
  {
    "objectID": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#looking-ahead",
    "href": "posts/14_-_Exponential Distribution/Math_32_14_Exponential_Distribution.html#looking-ahead",
    "title": "14: Exponential Distribution",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue soon\n\nWHW6\nJHW4\nMid-Semester Survey\n\nBe mindful of before-lecture quizzes\n\nExam 1 will be on Wed., Mar. 1\n\nmore information in weekly announcements\n\nNo lecture session for Math 32:\n\nMar 10, Mar 24\n\n\n\n\n\nart by @38mo1"
  },
  {
    "objectID": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#development",
    "href": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#development",
    "title": "15: Normal Distribution",
    "section": "Development",
    "text": "Development\n\n\nLet us start with the mother function \\(y = e^{-x^{2}}\\)\n\n\n\nsymmetric graph and function\nhorizontal asymptote\n\n\n\nNext, most of the scientific community accepts placing a horizontal scaling factor of 1/2 in the exponent\n\\[y = e^{-x^{2}/2} = \\text{exp}\\left(-\\displaystyle\\frac{x^{2}}{2}\\right)\\]\nto ensure that later calculations in the standard normal distribution have unit variance: \\(\\sigma^{2} = 1\\)"
  },
  {
    "objectID": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#bell-curves",
    "href": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#bell-curves",
    "title": "15: Normal Distribution",
    "section": "Bell Curves",
    "text": "Bell Curves\n\n\n\n\n\n\nSample Size Consideration\n\n\n\nFor elementary statistics courses, teachers say that using the normal distribution is a good idea when the sample size \\(n > 30\\).\n\n\nDepending on the situation, modeling with a normal distribution may need a transformation (moving the graph horizontally, stretching, etc.).\n\n\n\nbell curves"
  },
  {
    "objectID": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#probability-density-function",
    "href": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#probability-density-function",
    "title": "15: Normal Distribution",
    "section": "Probability Density Function",
    "text": "Probability Density Function\n\n\n\n\n\n\nStandard Normal Distribution\n\n\n\nHistorically, it was a good practice to pick one bell curve for calculations.\n\n\n\nFind the value of \\(k\\) so that \\(f(x) = ke^{-x^{2}/2}\\) is a probability density function."
  },
  {
    "objectID": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#probability-density-function-1",
    "href": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#probability-density-function-1",
    "title": "15: Normal Distribution",
    "section": "Probability Density Function",
    "text": "Probability Density Function\n\n\n\n\n\n\nProbability Density Function\n\n\n\nAt this point, we have the probability density function (PDF) of the standard normal distribution, denoted by lower-case Greek letter phi:\n\\[\\text{PDF: } \\phi(z) = \\displaystyle\\frac{1}{\\sqrt{2\\pi}}e^{-z^{2}/2}\\]\n\n\n\n\n\nstandard normal distribution"
  },
  {
    "objectID": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#parameters",
    "href": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#parameters",
    "title": "15: Normal Distribution",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\nParameters\n\n\n\nWe will now find the parameters—mean and variance—for the standard normal distribution. We find the expected value with\n\\[\\text{E}[Z] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! z \\cdot \\phi(z) \\, dz = \\displaystyle\\frac{1}{\\sqrt{2\\pi}}\\displaystyle\\int_{-\\infty}^{\\infty} \\! z e^{-z^{2}/2} \\, dz = 0\\]\nThe second moment is\n\\[\\text{E}[Z^{2}] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! z^{2} \\cdot \\phi(z) \\, dz = \\displaystyle\\frac{1}{\\sqrt{2\\pi}}\\displaystyle\\int_{-\\infty}^{\\infty} \\! z^{2} e^{-z^{2}/2} \\, dz = \\displaystyle\\frac{ \\sqrt{2\\pi} }{ \\sqrt{2\\pi} } = 1\\]\nIt follows that the variance is also one unit, so the parameters of the standard normal distribution are \\(\\mu = 0\\) and \\(\\sigma^{2} = 1\\).\n\n\n\n\n\n\n\n\nNotation\n\n\n\nThe notation \\(X \\sim N(\\mu, \\sigma^{2})\\) says that random variable \\(X\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). For example, the standard normal distribution is denoted as \\(Z \\sim N(0,1)\\)\n\n\n\nWhat is the median of the standard normal distribution?\n\n\n\n\nmedian?"
  },
  {
    "objectID": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#cumulative-distribution-function",
    "href": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#cumulative-distribution-function",
    "title": "15: Normal Distribution",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function\n\n\n\n\n\n\nCDF\n\n\n\nThe cumulative distribution function (CDF) for the standard normal distribution is defined as the following integral function and denoted by upper-case Greek letter Phi:\n\\[\\Phi(z) = \\displaystyle\\frac{1}{\\sqrt{2\\pi}} \\displaystyle\\int_{-\\infty}^{z} \\! e^{-t^{2}/2} \\, dt\\]\n\n\n\n\nRecall: for continuous probability distributions, probabilities are the areas under the curve\n\n\n\narea under the curve\n\n\n\nHere in Math 32, instead of doing the integral (with the ``polar trick’’) or referring to a textbook standard normal distribution table, we will perform calculations for the normal distribution in terms of CDF \\(\\Phi\\)"
  },
  {
    "objectID": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#empirical-rule",
    "href": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#empirical-rule",
    "title": "15: Normal Distribution",
    "section": "Empirical Rule",
    "text": "Empirical Rule\nStatistics instructors like to make the following statements to guide intuition about the normal distribution and standard deviations.\n\n1 SD2 SDs3 SDs\n\n\nAbout 67% of data falls within one standard deviation of the mean.\n\n\n\nAbout 95% of data falls within 2 standard deviations of the mean.\n\n\n\nAbout 99% of data falls within 3 standard deviations of the mean."
  },
  {
    "objectID": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#optional-error-function",
    "href": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#optional-error-function",
    "title": "15: Normal Distribution",
    "section": "(optional) Error Function",
    "text": "(optional) Error Function\n\n\n\n\n\n\n(optional) Error Function\n\n\n\n\n\nSome scientific literature refers to the area under the curve of the probability density function of the \\(X \\sim N(0, 1/2)\\) normal distribution as the \n\\[\\text{erf}(x) = \\displaystyle\\frac{2}{\\sqrt{\\pi}}\\int_{0}^{x} \\! e^{-t^{2}/2} \\, dt\\]\nand we can recover the CDF of the standard normal distribution with\n\\[\\Phi(x) = \\displaystyle\\frac{1}{2}\\left[  1 + \\text{erf}\\left(\\displaystyle\\frac{x}{\\sqrt{2}}\\right) \\right]\\]"
  },
  {
    "objectID": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#general-normal-distribution",
    "href": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#general-normal-distribution",
    "title": "15: Normal Distribution",
    "section": "General Normal Distribution",
    "text": "General Normal Distribution\n\n\n\n\n\n\nGeneral Normal Distribution\n\n\n\nWhen we model applications with \\(X \\sim N(\\mu, \\sigma^{2})\\), by applying the \\(z\\)-score transformation\n\\[z = \\displaystyle\\frac{x - \\mu}{\\sigma}\\]\nthe normal distribution has probability density function\n\\[\\text{PDF: } f(x; \\mu, \\sigma) = \\displaystyle\\frac{1}{\\sigma\\sqrt{2\\pi}} \\cdot e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}\\]\nand cumulative distribution function\n\\[F(x) = \\Phi\\left(\\displaystyle\\frac{x-\\mu}{\\sigma}\\right) = \\displaystyle\\frac{1}{2}\\left[1 + \\text{erf}\\left(\\displaystyle\\frac{x-\\mu}{\\sigma\\sqrt{2}}\\right)\\right]\\]\n\\[~\\]\nR code: pnorm(x, mu, sd)"
  },
  {
    "objectID": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#applications-of-the-normal-distribution",
    "href": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#applications-of-the-normal-distribution",
    "title": "15: Normal Distribution",
    "section": "Applications of the Normal Distribution",
    "text": "Applications of the Normal Distribution\n\nFewerMoreBetweenCharacterize\n\n\nSuppose that the incubation period—that is, the time between being infected with the virus and showing symptoms—for Covid-19 is normally distributed with a mean of 8 days and a standard deviation of 3 days. Find the probability that a randomly selected case demonstrated symptoms in fewer than 7 days.\n\n\n\nGirl Scout Thin Mint cookies have a mean size of 0.25 ounces. Find the probability that one randomly selected cookie has a size of more than 0.27 ounces if the standard deviation is 0.03 ounces. Assume a normal distribution.\n\n\n\nThe cones in the eye detect light. The absorption rate of cones is normally distributed. In particular, the “green” cones have a mean of 535 nanometers and a standard deviation of 65 nanometers. If an incoming ray of light has wavelengths between 550 and 575 nanometers, calculate the percentage of that ray of light that will be absorbed by the green cones.\n\n\n\nSuppose that the number of french fries in the batches at In-n-Out are normally distributed with a mean of 42 french fries and a standard deviation of 3.7 french fries. Your friend tells you that the In-n-Out employee is flirting with you if you end up with a french fry count in the top 5 percent. How should we characterize the top 5 percent of french fries?"
  },
  {
    "objectID": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#looking-ahead",
    "href": "posts/15_-_Normal Distribution/Math_32_15_Normal_Distribution.html#looking-ahead",
    "title": "15: Normal Distribution",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue soon\n\nWHW6\nJHW4\nMid-Semester Survey\n\nBe mindful of before-lecture quizzes\n\nExam 1 will be on Wed., Mar. 1\n\nmore information in weekly announcements\n\nNo lecture session for Math 32:\n\nMar 10, Mar 24\n\n\n\n\n\n\ntweet source"
  },
  {
    "objectID": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#joint-probability-mass-function",
    "href": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#joint-probability-mass-function",
    "title": "16: Discrete Joint Distributions",
    "section": "Joint Probability Mass Function",
    "text": "Joint Probability Mass Function\n\n\n\n\n\n\nJoint Probability Mass Function\n\n\n\nThe joint probability mass function (joint PMF) to handle simultaneous calculations of random variables \\(X\\) and \\(Y\\) can be expressed as\n\n\n\\(X = \\{a_{1}, a_{2}, ..., a_{m}\\}\\)\n\\(Y = \\{b_{1}, b_{2}, ..., b_{n}\\}\\)\n\\(p_{ij} = P(X = a_{i}, Y = b_{j})\\)\n\n\n\n\n\n\n\n\n\nProperties\n\n\n\n\nEach probability is between zero and one inclusively \\[0 \\leq p_{ij} \\leq 1\\]\nAll probabilities add up to 100 percent \\[\\displaystyle\\sum_{i = 1}^{m}\\sum_{j = 1}^{n} p_{ij} = 1\\]\nAside: it is okay if the total is 0.99 or 1.01 (artifact of rounding errors)"
  },
  {
    "objectID": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#setting",
    "href": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#setting",
    "title": "16: Discrete Joint Distributions",
    "section": "Setting",
    "text": "Setting\nThe setting for the examples in this lecture is The Lantern—our beloved coffee shop.\n\n\n\n\n\nLantern\n\n\n\n\n\\(X\\): number of beverages purchased by a customer\n\\(Y\\): number of snacks purchased by a customer"
  },
  {
    "objectID": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#joint-probability",
    "href": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#joint-probability",
    "title": "16: Discrete Joint Distributions",
    "section": "Joint Probability",
    "text": "Joint Probability\n\nWhat is the probability that a randomly selected customer purchased one beverage and one snack?"
  },
  {
    "objectID": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#marginal-probability-mass-functions",
    "href": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#marginal-probability-mass-functions",
    "title": "16: Discrete Joint Distributions",
    "section": "Marginal Probability Mass Functions",
    "text": "Marginal Probability Mass Functions\n\n\n\n\n\n\nMarginal Probability Mass Functions\n\n\n\nThe marginal probability mass functions with respect to \\(X\\) and \\(Y\\) respectively are\n\\[{\\color{blue}p_{X}(a_{i}) = \\displaystyle\\sum_{j = 1}^{n} p(a_{i}, b_{j})}, \\quad {\\color{red}p_{Y}(b_{j}) = \\displaystyle\\sum_{i = 1}^{m} p(a_{i}, b_{j})}\\]\n\n\nIn our example setting, we have the following joint PMF with marginal probabilities:\n\n\n\n\n\n\n\nMarginal Probability Mass Functions\n\n\n\n\n\nMore succinctly, the marginal probability mass function of \\(X\\) is\n\nand the marginal probability mass function of \\(Y\\) is\n\n\n\n\nWhat is the probability that a randomly selected customer purchased one beverage or one snack?"
  },
  {
    "objectID": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#conditional-probability",
    "href": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#conditional-probability",
    "title": "16: Discrete Joint Distributions",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\nCompute the probability that a randomly selected customer purchases one snack given that the customer purchased zero beverages.\nCompute the probability that a randomly selected customer purchases a beverage given that the customer purchased two snacks."
  },
  {
    "objectID": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#conditional-expectation",
    "href": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#conditional-expectation",
    "title": "16: Discrete Joint Distributions",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\n\n\n\n\n\n\nConditional Expectation\n\n\n\nThe concept of conditional probability can be extended into the concept of the expected value.\n\\[\\text{E}[{\\color{blue}A}| B = b_{j}] = \\displaystyle\\sum_{i = 1}^{m} {\\color{blue}a_{i}} \\cdot {\\color{red}P(a_{i} | B = b_{j})} = \\displaystyle\\sum_{i = 1}^{m} {\\color{blue}a_{i}} \\cdot {\\color{red}\\displaystyle\\frac{P(A = a_{i}, B = b_{j})}{P(B = b_{j})}}\\]\n\n\n\n\n\n\nWhat is the expected number of snacks purchased given that a customer purchases one beverage?"
  },
  {
    "objectID": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#joint-cumulative-distribution-function",
    "href": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#joint-cumulative-distribution-function",
    "title": "16: Discrete Joint Distributions",
    "section": "Joint Cumulative Distribution Function",
    "text": "Joint Cumulative Distribution Function\n\n\n\n\n\n\nJoint Cumulative Distribution Function\n\n\n\nAs in the univariate case, the multivariate joint cumulative distribution function (joint CDF) is defined similarly as\n\\[F(a, b) = P(X \\leq a, Y \\leq b)\\]"
  },
  {
    "objectID": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#looking-ahead",
    "href": "posts/16_-_Discrete Joint Distributions/Math_32_16_Discrete_Joint_Distributions.html#looking-ahead",
    "title": "16: Discrete Joint Distributions",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\nExam 1 will be on Wed., Mar. 1\n\nmore information in weekly announcements\n\nNo lecture session for Math 32:\n\nMar 10, Mar 24"
  },
  {
    "objectID": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#joint-probability-density-function",
    "href": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#joint-probability-density-function",
    "title": "17: Continuous Joint Distributions",
    "section": "Joint Probability Density Function",
    "text": "Joint Probability Density Function\n\n\n\n\n\n\nJoint Probability Density Function\n\n\n\nThe joint probability density function \\(f(x,y)\\) to handle simultaneous calculations of random variables \\(X\\) and \\(Y\\) can be expressed as\n\\[P({\\color{blue}a_{1} < X < a_{2}}, {\\color{red}b_{1} < Y < b_{2}}) = \\displaystyle{\\color{blue}\\int_{a_{1}}^{a_{2}}}{\\color{red}\\int_{b_{1}}^{b_{2}}} \\! {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx}\\]\n\n\n\n\n\n\n\n\nProperties\n\n\n\n\nEach probability is between zero and one inclusively \\[0 \\leq {\\color{purple}f(x,y)} \\leq 1 \\text{ for all } {\\color{blue}$x$}, {\\color{red}$y$}\\]\nAll probabilities add up to 100 percent \\[{\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}}{\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{purple}f(x,y)} \\, {\\color{blue}dx} \\, {\\color{red}dy} = 1\\]"
  },
  {
    "objectID": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#setting",
    "href": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#setting",
    "title": "17: Continuous Joint Distributions",
    "section": "Setting",
    "text": "Setting\nFor the examples in this lecture session, we will model the queues at In-n-Out with random variables\n\n\n\n\n\nIn-n-Out\n\n\n\n\n\\(X\\): wait time to order food\n\\(Y\\): wait time to receive food\n\nand a function of the form\n\\[{\\color{purple}f(x,y)} = k{\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}},\\]\n\\[{\\color{blue}x > 0}, \\quad {\\color{red}y > 0}\\]"
  },
  {
    "objectID": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#normalization",
    "href": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#normalization",
    "title": "17: Continuous Joint Distributions",
    "section": "Normalization",
    "text": "Normalization\nFind the value of \\(k\\) so that \\(f\\) is a probability density function.\n\\[{\\color{purple}f(x,y)} = k{\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}}, \\quad {\\color{blue}x > 0}, \\quad {\\color{red}y > 0}\\]"
  },
  {
    "objectID": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#joint-probability",
    "href": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#joint-probability",
    "title": "17: Continuous Joint Distributions",
    "section": "Joint Probability",
    "text": "Joint Probability\n\\[{\\color{purple}f(x,y)} = \\displaystyle\\frac{1}{25}{\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}}, \\quad {\\color{blue}x > 0}, \\quad {\\color{red}y > 0}\\]\nCompute the probability that you will take {between 1 and 2 minutes to order} and wait {between 3 and 4 minutes to receive} your food."
  },
  {
    "objectID": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#joint-cumulative-distribution-function",
    "href": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#joint-cumulative-distribution-function",
    "title": "17: Continuous Joint Distributions",
    "section": "Joint Cumulative Distribution Function",
    "text": "Joint Cumulative Distribution Function\n\n\n\n\n\n\nJoint Cumulative Distribution Function\n\n\n\nIn general, we handle the probability calculations with the joint cumulative distribution function \\(F(a,b)\\)\n% joint cumulative distribution function \\[{\\color{purple}F(a,b)} = P({\\color{blue}X \\leq a}, {\\color{red}Y \\leq b}) = {\\color{blue}\\displaystyle\\int_{-\\infty}^{a}}{\\color{red}\\displaystyle\\int_{-\\infty}^{b}} \\! {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx} \\]\n\n\n\n\n\n\n\n\nProperties\n\n\n\nWe can verify that the joint CDF starts at zero\n\\[{\\color{purple}F(0,0)} = 0\\]\nand that the joint CDF collects all probabilities\n\\[\\displaystyle\\lim_{a \\to \\infty, b \\to \\infty} F(a,b) = 1\\]\nIf need be, we can recover the joint PDF from the joint CDF as the mixed second-order partial derivatives\n\\[{\\color{purple}f(x,y)} = \\displaystyle\\frac{{\\color{purple}\\partial^{2}}}{{\\color{blue}\\partial x} {\\color{red}\\partial y}} {\\color{purple}F(x,y)}\\]\n\n\nWhat is the joint CDF for the In-n-Out setting?"
  },
  {
    "objectID": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#marginal-probabilities",
    "href": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#marginal-probabilities",
    "title": "17: Continuous Joint Distributions",
    "section": "Marginal Probabilities",
    "text": "Marginal Probabilities\n\n\n\n\n\n\nMarginal Cumulative Distribution Functions\n\n\n\nThe marginal cumulative distribution functions can be computed as\n% marginal CDF \\[\\begin{array}{rcl}\n  {\\color{blue}F_{X}(a)} & = & \\displaystyle\\lim_{{\\color{red}b \\to \\infty}} {\\color{purple}F(a,b)} \\\\\n  {\\color{red}F_{Y}(b)} & = & \\displaystyle\\lim_{{\\color{blue}a \\to \\infty}} {\\color{purple}F(a,b)} \\\\\n\\end{array}\\]\n\n\n\n\n\n\n\n\nProperties\n\n\n\nIntuition: the marginal CDF is seeking to analyze the probabilities in just one variable regardless of the other variables, so ``eliminate’’ the other varibles by taking their limits to infinity.\nWe can verify that the marginal CDFs start at zero\n\\[{\\color{blue}F_{X}(0)} = 0 \\text{ and } {\\color{red}F_{Y}(0)} = 0\\]\nand that the marginal CDFs collect all probabilities\n\\[\\displaystyle\\lim_{{\\color{blue}a \\to \\infty}} {\\color{blue}F_{X}(a)} = 1 \\text{ and } \\displaystyle\\lim_{{\\color{red}b \\to \\infty}} {\\color{red}F_{Y}(b)} = 1\\]\n\n\nWhat are the marginal CDFs for the In-n-Out setting?"
  },
  {
    "objectID": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#marginal-probabilities-1",
    "href": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#marginal-probabilities-1",
    "title": "17: Continuous Joint Distributions",
    "section": "Marginal Probabilities",
    "text": "Marginal Probabilities\n\n\n\n\n\n\nMarginal Cumulative Distribution Functions\n\n\n\nThe marginal probability density functions can be computed as\n\\[\\begin{array}{rcl}\n  {\\color{blue}f_{X}(x)} & = & {\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\\\\n  {\\color{red}f_{Y}(y)} & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{purple}f(x,y)} \\, {\\color{blue}dx} \\\\\n\\end{array}\\]\n\n\n\n\n\n\n\n\nIntuition\n\n\n\nIntuition: the marginal PDF is seeking to analyze the probabilities in just one variable regardless of the other variables, so ``integrate out’’ the other variables.\nAlternatively,\n\\[{\\color{blue}f_{X}(x) = \\displaystyle\\frac{d}{dx} F_{X}(x)} \\text{ and } {\\color{red}f_{Y}(y) = \\displaystyle\\frac{d}{dy} F_{Y}(y)} \\]\n\n\nWhat are the marginal PDFs for the In-n-Out setting?"
  },
  {
    "objectID": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#marginal-expectation",
    "href": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#marginal-expectation",
    "title": "17: Continuous Joint Distributions",
    "section": "Marginal Expectation",
    "text": "Marginal Expectation\n\n\n\n\n\nIn-N-Out\n\n\n\n\nWhat is the expected wait time to order food?\nWhat is the expected wait time to receive food?"
  },
  {
    "objectID": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#independence",
    "href": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#independence",
    "title": "17: Continuous Joint Distributions",
    "section": "Independence",
    "text": "Independence\n\n\n\n\n\n\nIndependence\n\n\n\nRecall that two events \\(A\\) and \\(B\\) are independent if\n\\[{\\color{purple}P(AB)} = {\\color{blue}P(A)} \\cdot {\\color{red}P(B)}\\]\n\n\nHere, the variables \\(X\\) and \\(Y\\) in the In-n-Out example were independent, which can be easily verified by noting that the integrals were separable.\n\\[\\begin{array}{rcl}\n  {\\color{purple}f(x,y)} & = & {\\color{blue}f_{X}(x)} \\cdot {\\color{red}f_{Y}(y)} \\\\\n  {\\color{red}\\displaystyle\\frac{1}{25}} {\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}} & = & {\\color{blue}xe^{-x}} \\cdot {\\color{red}\\displaystyle\\frac{y}{25}e^{-y/5}} \\\\\n\\end{array}\\]\nand\n\\[\\begin{array}{rcl}\n      {\\color{purple}F(a,b)} & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{a}} {\\color{red}\\displaystyle\\int_{-\\infty}^{b}} \\! {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx} \\\\\n      ~ & = & {\\color{red}\\displaystyle\\frac{1}{25}} {\\color{blue}\\displaystyle\\int_{0}^{a}} {\\color{red}\\displaystyle\\int_{0}^{b}} \\! {\\color{blue}x}{\\color{red}y}{\\color{blue}e^{-x}}{\\color{red}e^{-y/5}} \\, {\\color{red}dy} \\, {\\color{blue}dx}\\\\\n      ~ & = & {\\color{red}\\displaystyle\\frac{1}{25}} {\\color{blue}\\left(\\displaystyle\\int_{0}^{a} xe^{-x} \\, dx \\right)} {\\color{red}\\left(\\displaystyle\\int_{0}^{b} \\! ye^{-y/5} \\, dy \\right)} \\\\\n      \\end{array}\\]\n\n\n\n\n\n\nDependence\n\n\n\nUpcoming lectures: dependent variables"
  },
  {
    "objectID": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#looking-ahead",
    "href": "posts/17_-_Continuous Joint Distributions/Math_32_17_Continuous_Joint_Distributions.html#looking-ahead",
    "title": "17: Continuous Joint Distributions",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Mar. 10:\n\nWHW7\nLHW6\nInternet Connection (survey)\n\nExam 2 will be on Mon., Apr. 10\nno lecture on Mar. 10, Mar. 24\n\n\n\n\n\n\ntweet source"
  },
  {
    "objectID": "posts/18_-_Linear Operators/Math_32_18_Linear_Operators.html#linear-operators",
    "href": "posts/18_-_Linear Operators/Math_32_18_Linear_Operators.html#linear-operators",
    "title": "18: Linear Operators",
    "section": "Linear Operators",
    "text": "Linear Operators\n\n\n\n\n\n\nLinear Operators\n\n\n\nWe say that \\(L\\) is a linear operator if\n\\[\\begin{array}{rcl}\n  L(a{\\color{blue}f(x)}) & = & aL({\\color{blue}f(x)}) \\\\\n  L({\\color{blue}f(x)} + {\\color{red}g(x)}) & = & L({\\color{blue}f(x)}) + L({\\color{red}g(x)}) \\\\\n\\end{array}\\]\n\n\n\n\n\n\n\n\nLinear Operators\n\n\n\nLoosely translated, \\(L\\) is a linear operator if\n\nwe can factor out a scalar multiple\nwe can split the operator across a sum or difference"
  },
  {
    "objectID": "posts/18_-_Linear Operators/Math_32_18_Linear_Operators.html#calculus-review",
    "href": "posts/18_-_Linear Operators/Math_32_18_Linear_Operators.html#calculus-review",
    "title": "18: Linear Operators",
    "section": "Calculus Review",
    "text": "Calculus Review\n\n\n\nShow that the derivative operator is a linear operator.\n\nShow that the integral operator is a linear operator.\n\n\n\n\n\n\n\n\n\n\n\nClaim: the derivative operator is a linear operator\n\n\n\n\n\nProof\n\\[\\begin{array}{rcl}\n      \\displaystyle\\frac{d}{dx}(a{\\color{blue}f(x)} + b{\\color{red}g(x)}) & = & \\displaystyle\\frac{d}{dx} a{\\color{blue}f(x)} + \\displaystyle\\frac{d}{dx} b{\\color{red}g(x)} \\\\\n      ~ & = &  a\\displaystyle\\frac{d}{dx} {\\color{blue}f(x)} + b\\displaystyle\\frac{d}{dx} {\\color{red}g(x)} \\\\\n    \\end{array}\\]\n\\[\\displaystyle\\frac{d}{dx}(a{\\color{blue}f(x)} + b{\\color{red}g(x)}) = a\\displaystyle\\frac{d}{dx} {\\color{blue}f(x)} + b\\displaystyle\\frac{d}{dx} {\\color{red}g(x)},\\]\nso \\(\\displaystyle\\frac{d}{dx}\\) is a linear operator.\n\n\n\n\n\n\n\n\n\nClaim: the integral operator is a linear operator\n\n\n\n\n\nProof\n\\[\\begin{array}{rcl}\n      \\displaystyle\\int \\! (a{\\color{blue}f(x)} + b{\\color{red}g(x)}) \\, dx & = & \\displaystyle\\int \\! a{\\color{blue}f(x)} \\, dx + \\displaystyle\\int \\! b{\\color{red}g(x)} \\, dx \\\\\n      ~ & = & a\\displaystyle\\int \\! {\\color{blue}f(x)} \\, dx + b\\displaystyle\\int \\! {\\color{red}g(x)} \\, dx \\\\\n    \\end{array}\\]\n\\[\\displaystyle\\int \\! (a{\\color{blue}f(x)} + b{\\color{red}g(x)}) \\, dx = a\\displaystyle\\int \\! {\\color{blue}f(x)} \\, dx + b\\displaystyle\\int \\! {\\color{red}g(x)} \\, dx,\\]\nso \\(\\displaystyle\\int\\) is a linear operator."
  },
  {
    "objectID": "posts/18_-_Linear Operators/Math_32_18_Linear_Operators.html#expected-value",
    "href": "posts/18_-_Linear Operators/Math_32_18_Linear_Operators.html#expected-value",
    "title": "18: Linear Operators",
    "section": "Expected Value",
    "text": "Expected Value\nIs the expectation operator \\(\\text{E}\\) a linear operator?\n\n\n\n\n\n\nClaim: \\(\\text{E}[aX] = a\\text{E}[X]\\)\n\n\n\n\n\nProof\n\\[\\text{E}[aX] = \\displaystyle\\int_{-\\infty}^{\\infty} \\! ax \\cdot f(x) \\, dx = a{\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty} \\! x \\cdot f(x) \\, dx} = a{\\color{blue}\\text{E}[X]}\\]\nWe have shown that we can factor out a scalar multiple across the expectation operator.\n\n\n\n\n\n\n\n\n\nClaim: \\(\\text{E}[X + c] = \\text{E}[X] + c\\)\n\n\n\n\n\nProof\n\\[\\begin{array}{rcl}\n          \\text{E}[X + c] & = & \\displaystyle\\int_{-\\infty}^{\\infty} \\! (x + c) \\cdot f(x) \\, dx \\\\\n          ~ & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty} \\! x \\cdot f(x) \\, dx} + \\displaystyle\\int_{-\\infty}^{\\infty} \\! c \\cdot f(x) \\, dx\\\\\n          ~ & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty} \\! x \\cdot f(x) \\, dx} + c\\displaystyle\\int_{-\\infty}^{\\infty} \\! f(x) \\, dx \\\\\n          ~ & = & {\\color{blue}\\text{E}[X]} + c \\\\\n        \\end{array}\\]\nWe have shown that a horizontal shift of \\(c\\) units in the data also affects the expected value by \\(c\\) units\n\n\n\n\n\n\n\n\n\nClaim: \\(\\text{E}[X + Y] = \\text{E}[X] + \\text{E}[Y]\\)\n\n\n\n\n\nProof\n\\[\\begin{array}{rcl}\n          {\\color{purple}\\text{E}[X + Y]} & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}}{\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{purple}(x + y) \\cdot f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx} \\\\\n          ~ & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}}{\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{blue}x} \\cdot {\\color{purple}f(x,y)} \\, {\\color{red}dy} \\, {\\color{blue}dx}  + {\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty}}{\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}} \\! {\\color{red}y} \\cdot {\\color{purple}f(x,y)} \\, {\\color{blue}dx} \\, {\\color{red}dy} \\\\\n          ~ & = & {\\color{blue}\\displaystyle\\int_{-\\infty}^{\\infty}\\! x \\cdot f_{X}(x) \\, dx}  + {\\color{red}\\displaystyle\\int_{-\\infty}^{\\infty} \\! y \\cdot f_{Y}(y) \\, dy} \\\\\n          ~ & = & {\\color{blue}\\text{E}[X]} + {\\color{red}\\text{E}[Y]} \\\\\n        \\end{array}\\]\nWe have shown that the expected value of a sum \\ is the sum of the expected values.\n\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nCombining the above results, since\n\\[\\text{E}[aX + bY] = a\\text{E}[X] + b\\text{E}[Y]\\]\nwe have shown that the expectation operator \\(\\text{E}\\) is a linear operator.\nAlso,\n\\[\\text{E}[aX + bY + c] = a\\text{E}[X] + b\\text{E}[Y] + c\\]"
  },
  {
    "objectID": "posts/18_-_Linear Operators/Math_32_18_Linear_Operators.html#variance",
    "href": "posts/18_-_Linear Operators/Math_32_18_Linear_Operators.html#variance",
    "title": "18: Linear Operators",
    "section": "Variance",
    "text": "Variance\nIs the variance \\(\\text{Var}(X)\\) function a linear operator?\n\n\n\n\n\n\nClaim: \\(\\text{Var}(aX) = a\\text{Var}(X)\\)\n\n\n\n\n\nCounterpoint:\nRecall the ``practical formula for variance’’\n\\[\\text{Var}(X) = \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2}\\]\nand tracking the scaling factor proceeds as follows\n\\[\\begin{array}{rcl}\n          \\text{Var}(aX) & = & \\text{E}[(aX)^{2}] - \\left(\\text{E}[aX]\\right)^{2} \\\\\n          ~ & = & \\displaystyle\\int_{-\\infty}^{\\infty}\\! (ax)^{2} \\cdot f(x) \\, dx - \\left(a\\text{E}[X]\\right)^{2} \\\\\n          ~ & = & \\displaystyle\\int_{-\\infty}^{\\infty}\\! a^{2}x^{2} \\cdot f(x) \\, dx - a^{2}\\left(\\text{E}[X]\\right)^{2} \\\\\n          ~ & = & a^{2} \\left[ \\displaystyle\\int_{-\\infty}^{\\infty}\\! x^{2} \\cdot f(x) \\, dx - \\left(\\text{E}[X]\\right)^{2} \\right] \\\\\n          ~ & = & a^{2} \\left( \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2} \\right) \\\\\n          ~ & = & a^{2} \\text{Var}(X) \\\\\n        \\end{array}\\]\nWhen factoring out a scalar from the variance function, the factor is squared.\nFurthermore, since \\(\\text{Var}(aX) \\neq a\\text{Var}(X)\\), we have shown that the variance function is not a linear operator.\n\n\n\n\n\n\n\n\n\nClaim: \\(\\text{Var}(X + c) = \\text{Var}(X) + c\\)\n\n\n\n\n\nCounterpoint:\nRecall the ``practical formula for variance’’\n\\[\\begin{array}{rcl}\n          \\text{Var}(X + c) & = & \\text{E}[(X + c)^{2}] - \\left(\\text{E}[X + c]\\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2} + 2cX + c^{2}] - \\left(\\text{E}[X] + c \\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] + \\text{E}[2cX] + \\text{E}[c^{2}] - \\left(\\text{E}[X]\\right)^{2} - 2c\\text{E}[X] - c^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] + 2c\\text{E}[X] + c^{2} - \\left(\\text{E}[X]\\right)^{2} - 2c\\text{E}[X] - c^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2} \\\\\n          ~ & = & \\text{Var}(X) \\\\\n        \\end{array}\\]\nWe have shown that \\(\\text{Var}(X + c) = \\text{Var}(X)\\). That is, variance is not affected by a horizontal shift (phase shift)!\nFurthermore, since \\(\\text{Var}(X + c) \\neq \\text{Var}(X) + c\\), we have shown that the variance function is not a linear operator.\n\n\n\n\n\n\n\n\n\nClaim: \\(\\text{Var}(X + Y) = {\\color{blue}\\text{Var}(X)} + {\\color{red}\\text{Var}(Y)}\\)\n\n\n\n\n\nCounterpoint:\n\\[\\begin{array}{rcl}\n          \\text{Var}(X + Y) & = & \\text{E}[(X + Y)^{2}] - \\left(\\text{E}[X + Y]\\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2} + 2XY + Y^{2}] - \\left(\\text{E}[X] + \\text{E}[Y]\\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] + \\text{E}[2XY] + \\text{E}[Y^{2}] - \\left(\\text{E}[X]\\right)^{2}+ 2\\text{E}[X]\\text{E}[Y] + \\left(\\text{E}[Y]\\right)^{2} \\\\\n          ~ & = & \\text{E}[X^{2}] - \\left(\\text{E}[X]\\right)^{2}  + \\text{E}[Y^{2}] - \\left(\\text{E}[Y]\\right)^{2} + 2\\text{E}[XY] - 2\\text{E}[X]\\text{E}[Y] \\\\\n          ~ & = & {\\color{blue}\\text{Var}(X)} + {\\color{red}\\text{Var}(Y)} + 2\\left( {\\color{purple}\\text{E}[XY] - \\text{E}[X]\\text{E}[Y] } \\right) \\\\\n        \\end{array}\\]\nWe have shown that \\(\\text{Var}(X + Y) \\neq \\text{Var}(X) + \\text{Var}(Y)\\). That is, the variance of the sum is not the sum of the variances (unless …?)\nFurthermore, since \\(\\text{Var}(X + Y) \\neq \\text{Var}(X) + \\text{Var}(Y)\\), we have shown that the variance function is not a linear operator.\n\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nWe have shown that the variance function is not a linear operator.\nNext time: working with\n\\[{\\color{purple}\\text{E}[XY] - \\text{E}[X]\\text{E}[Y]}\\]\nwhich is called the covariance!"
  },
  {
    "objectID": "posts/18_-_Linear Operators/Math_32_18_Linear_Operators.html#looking-ahead",
    "href": "posts/18_-_Linear Operators/Math_32_18_Linear_Operators.html#looking-ahead",
    "title": "18: Linear Operators",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Mar. 10:\n\nWHW7\nLHW6\nInternet Connection (survey)\n\nExam 2 will be on Mon., Apr. 10\nno lecture on Mar. 10, Mar. 24\n\n\n\ntweet source"
  },
  {
    "objectID": "posts/19_-_Covariance/Math_32_19_Covariance.html#setting",
    "href": "posts/19_-_Covariance/Math_32_19_Covariance.html#setting",
    "title": "19: Covariance",
    "section": "Setting",
    "text": "Setting\n\n\nWe will once again visualize the act of ordering food at In-n-Out.\n\n\\(X\\): number of fries orders\n\\(Y\\): number of beef patties ordered\n\n\n\n\njoint PMF\n\n\n\n\n\n\nIn-n-Out"
  },
  {
    "objectID": "posts/19_-_Covariance/Math_32_19_Covariance.html#independence",
    "href": "posts/19_-_Covariance/Math_32_19_Covariance.html#independence",
    "title": "19: Covariance",
    "section": "Independence",
    "text": "Independence\n\nAre \\(X\\) and \\(Y\\) independent?"
  },
  {
    "objectID": "posts/19_-_Covariance/Math_32_19_Covariance.html#covariance",
    "href": "posts/19_-_Covariance/Math_32_19_Covariance.html#covariance",
    "title": "19: Covariance",
    "section": "Covariance",
    "text": "Covariance\n\n\n\n\n\n\nTrue or False? \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)\n\n\n\n\n\nFalse. In general,\n\\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\left( \\text{E}[XY] - \\text{E}[X]\\text{E}[Y] \\right)\\]\n\n\n\n\n\n\n\n\n\nMotivation for Independence\n\n\n\n\n\nAs you probably suspected, \\(\\text{Var}(X + Y)\\) does equal \\(\\text{Var}(X) + \\text{Var}(Y)\\) if \\(X\\) and \\(Y\\) are independent (exercise left to reader).\n\n\n\n\n\n\n\n\n\nCovariance\n\n\n\nWe define the covariance of random variables as\n\\[\\text{Cov}(X,Y) = \\text{E}[XY] - \\text{E}[X]\\text{E}[Y]\\]\n\n\n\n\n\n\n\n\nDerek’s Intuition\n\n\n\n\n\nAs an analogy, the random variables somewhat act like waves in that they can work together and grow or somewhat cancel each other out.\n\n\nImage source: https://www.physics-and-radio-electronics.com/physics/waveinterference.html\n\n\n\n\n\n\n\n\n\n\nThe Pythagorean Theorem of Statistics\n\n\n\n\n\n\n\nImage credit: Bioinformatics professor Dr. David Ardell"
  },
  {
    "objectID": "posts/19_-_Covariance/Math_32_19_Covariance.html#covariance-2",
    "href": "posts/19_-_Covariance/Math_32_19_Covariance.html#covariance-2",
    "title": "19: Covariance",
    "section": "Covariance",
    "text": "Covariance\n\n\nCompute the covariance in the In-n-Out setting"
  },
  {
    "objectID": "posts/19_-_Covariance/Math_32_19_Covariance.html#continuous-joint-probability-distribution-functions",
    "href": "posts/19_-_Covariance/Math_32_19_Covariance.html#continuous-joint-probability-distribution-functions",
    "title": "19: Covariance",
    "section": "Continuous Joint Probability Distribution Functions",
    "text": "Continuous Joint Probability Distribution Functions\n\n\nWe will once again visualize the act of ordering food at In-n-Out.\n\n\\(X\\): number of fries orders\n\\(Y\\): number of beef patties ordered\n\nwith joint PDF\n\\[f(x,y) = \\frac{1}{30}(x + y)e^{-x}e^{-y/5}\\]\n\nAre \\(X\\) and \\(Y\\) independent?\nCompute the covariance in the In-n-Out setting\n\n\n\n\n\nIn-n-Out"
  },
  {
    "objectID": "posts/19_-_Covariance/Math_32_19_Covariance.html#looking-ahead",
    "href": "posts/19_-_Covariance/Math_32_19_Covariance.html#looking-ahead",
    "title": "19: Covariance",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Mar. 10:\n\nWHW7\nLHW6\nInternet Connection (survey)\n\nExam 2 will be on Mon., Apr. 10\nno lecture on Mar. 10, Mar. 24\n\n\n tweet source"
  },
  {
    "objectID": "posts/20_-_Correlation/Math_32_20_Correlation.html#setting",
    "href": "posts/20_-_Correlation/Math_32_20_Correlation.html#setting",
    "title": "20: Correlation",
    "section": "Setting",
    "text": "Setting\nWe will once again visualize the act of ordering food at In-n-Out.\n\n\\(X\\): number of fries orders\n\\(Y\\): number of beef patties ordered\n\n\n\n\njoint PMF\n\n\n\n\n\n\n\n\nCovariance\n\n\n\nWe define the covariance of random variables as\n\\[\\text{Cov}(X,Y) = \\text{E}[XY] - \\text{E}[X]\\text{E}[Y]\\]"
  },
  {
    "objectID": "posts/20_-_Correlation/Math_32_20_Correlation.html#correlation",
    "href": "posts/20_-_Correlation/Math_32_20_Correlation.html#correlation",
    "title": "20: Correlation",
    "section": "Correlation",
    "text": "Correlation\n\n\n\n\n\n\nCorrelation\n\n\n\nJust like how the \\(z\\)-score is a standardized and unitless measure, the correlation was designed to be standardized and unitless (i.e. units cancel out).\n\\[r = \\text{Corr}(X,Y) = \\displaystyle\\frac{ \\text{Cov}(X,Y) }{ \\sqrt{ \\text{Var}(X) \\cdot \\text{Var}(Y)} }\\]\n\nIf \\(\\text{Var}(X) = 0\\), the data \\(X\\) are constant, and simply return \\(r = 0\\)\nIf \\(\\text{Var}(Y) = 0\\), the data \\(Y\\) are constant, and simply return \\(r = 0\\)\n\n\n\n\n\nCompute the correlation in the In-n-Out setting"
  },
  {
    "objectID": "posts/20_-_Correlation/Math_32_20_Correlation.html#interpretation-of-correlation",
    "href": "posts/20_-_Correlation/Math_32_20_Correlation.html#interpretation-of-correlation",
    "title": "20: Correlation",
    "section": "Interpretation of Correlation",
    "text": "Interpretation of Correlation\n\n\n\n\n\n\nRanges\n\n\n\n\n\n\n\n\nRanges\n\n\nAside: the infinity-sized expected values might happen in continuous distributions.\n\n\n\n\n\n\n\n\n\nInterpretation of Correlation\n\n\n\nEarly development of the concept of correlation was done by Karl Pearson. Pearson suggested the following interpretations of the correlation (but there is no strict rule for this):\n\n\\(|r| < 0.4\\): virtually uncorrelated\n\\(0.4 \\leq |r| < 0.7\\): slightly correlated\n\\(0.7 \\leq |r| \\leq 1.0\\): strongly correlated"
  },
  {
    "objectID": "posts/20_-_Correlation/Math_32_20_Correlation.html#examples-of-correlation",
    "href": "posts/20_-_Correlation/Math_32_20_Correlation.html#examples-of-correlation",
    "title": "20: Correlation",
    "section": "Examples of Correlation",
    "text": "Examples of Correlation\n\nR12345\n\n\n\ncorrelatedValues = function(x, r = 0.9){\n  r2 = r**2\n  ve = 1-r2\n  SD = sqrt(ve)\n  e  = rnorm(length(x), mean=0, sd=SD)\n  y  = r*x + e\n  return(y)\n}\n\nx1 = rnorm(100, mean = 0, sd = 1)\ny1 = correlatedValues(x1, r = -0.9)\nx2 = rnorm(100, mean = 0, sd = 1)\ny2 = correlatedValues(x2, r = -0.4)\nx3 = rnorm(100, mean = 0, sd = 1)\ny3 = correlatedValues(x3, r = 0.0)\nx4 = rnorm(100, mean = 0, sd = 1)\ny4 = correlatedValues(x4, r = 0.4)\nx5 = rnorm(100, mean = 0, sd = 1)\ny5 = correlatedValues(x5, r = 0.9)\n\ndf1 <- data.frame(x1, y1, \"group 1\")\ndf2 <- data.frame(x2, y2, \"group 2\")\ndf3 <- data.frame(x3, y3, \"group 3\")\ndf4 <- data.frame(x4, y4, \"group 4\")\ndf5 <- data.frame(x5, y5, \"group 5\")\nnames(df1) <- c(\"xdata\", \"ydata\", \"group\")\nnames(df2) <- c(\"xdata\", \"ydata\", \"group\")\nnames(df3) <- c(\"xdata\", \"ydata\", \"group\")\nnames(df4) <- c(\"xdata\", \"ydata\", \"group\")\nnames(df5) <- c(\"xdata\", \"ydata\", \"group\")\nmain_df <- rbind(df1, df2, df3, df4, df5)"
  },
  {
    "objectID": "posts/20_-_Correlation/Math_32_20_Correlation.html#continuous-joint-probability-distribution-functions",
    "href": "posts/20_-_Correlation/Math_32_20_Correlation.html#continuous-joint-probability-distribution-functions",
    "title": "20: Correlation",
    "section": "Continuous Joint Probability Distribution Functions",
    "text": "Continuous Joint Probability Distribution Functions\n\n\nWe will once again visualize the act of ordering food at In-n-Out.\n\n\\(X\\): number of fries orders\n\\(Y\\): number of beef patties ordered\n\nwith joint PDF\n\\[f(x,y) = \\frac{1}{30}(x + y)e^{-x}e^{-y/5}\\]\n\nCompute the correlation in the In-n-Out setting\n\n\n\n\n\nIn-n-Out"
  },
  {
    "objectID": "posts/20_-_Correlation/Math_32_20_Correlation.html#looking-ahead",
    "href": "posts/20_-_Correlation/Math_32_20_Correlation.html#looking-ahead",
    "title": "20: Correlation",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Mar. 17:\n\nWHW8\nLHW7\n\nExam 2 will be on Mon., Apr. 10\nno lecture on Mar. 24, Apr. 3\n\n\n tweet source"
  },
  {
    "objectID": "posts/21_-_Change in Variables/Math_32_21_Change_of_Variables.html#linear-conversion",
    "href": "posts/21_-_Change in Variables/Math_32_21_Change_of_Variables.html#linear-conversion",
    "title": "21: Change of Variables",
    "section": "Linear Conversion",
    "text": "Linear Conversion\nLet \\(F\\) be the daily high temperature in Fahrenheit in Merced, California, with a mean of 76 degrees and a standard deviation of 15 degrees. Compute those sample statistics in Celsius.\n\n\n\n\n\n\nTemperature Conversion\n\n\n\nWe know that the conversion formula is\n\\[C = \\displaystyle\\frac{5}{9}(F - 32)\\]"
  },
  {
    "objectID": "posts/21_-_Change in Variables/Math_32_21_Change_of_Variables.html#range-rule-of-thumb",
    "href": "posts/21_-_Change in Variables/Math_32_21_Change_of_Variables.html#range-rule-of-thumb",
    "title": "21: Change of Variables",
    "section": "Range Rule of Thumb",
    "text": "Range Rule of Thumb\n\n\n\n\n\n\nRange Rule of Thumb\n\n\n\nRecall\n\nAbout 67 percent of data falls within one standard deviation of the mean\nAbout 95 percent of data falls within two standard deviations of the mean\n\n\\[\\left( \\mu - 2\\sigma, \\mu + 2\\sigma \\right)\\]\n\n\nWe had computed\n\n\\(\\mu_{F} \\approx 76\\) and \\(\\sigma_{F} \\approx 15\\) degrees Fahrenheit\n\\(\\mu_{C} \\approx 24.4444\\) and \\(\\sigma_{C} \\approx 8.3333\\) degrees Celsius\n\nBuild range-rule-of-thumb intervals for the Merced high temperatures in Fahrenheit and in Celsius."
  },
  {
    "objectID": "posts/21_-_Change in Variables/Math_32_21_Change_of_Variables.html#distributions",
    "href": "posts/21_-_Change in Variables/Math_32_21_Change_of_Variables.html#distributions",
    "title": "21: Change of Variables",
    "section": "Distributions",
    "text": "Distributions\nDetermine the distribution and density functions for\n\\[Y = \\displaystyle\\frac{5}{9}(X - 32)\\]"
  },
  {
    "objectID": "posts/21_-_Change in Variables/Math_32_21_Change_of_Variables.html#change-of-coordinates",
    "href": "posts/21_-_Change in Variables/Math_32_21_Change_of_Variables.html#change-of-coordinates",
    "title": "21: Change of Variables",
    "section": "Change of Coordinates",
    "text": "Change of Coordinates\n\n\n\n\n\n\nChange of Coordinates\n\n\n\nLet \\(X\\) be a continuous random variable with distribution function \\(F_{X}\\) and density function \\(f_{X}\\). If we apply a linear transformation\n\\[Y = aX + c\\]\nwhere \\(a >0\\) and \\(c\\) are constants, then\n\\[F_{Y}(y) = F_{X}\\left(\\displaystyle\\frac{y - c}{a}\\right) \\text{ and } f_{Y}(y) = \\displaystyle\\frac{1}{a}f_{X}\\left(\\displaystyle\\frac{y - c}{a}\\right)\\]\n\n\nIf \\(X \\sim Exp(1/2)\\), then what kind of distribution does \\(Y = 32X\\) have?"
  },
  {
    "objectID": "posts/21_-_Change in Variables/Math_32_21_Change_of_Variables.html#nonlinear-transformations",
    "href": "posts/21_-_Change in Variables/Math_32_21_Change_of_Variables.html#nonlinear-transformations",
    "title": "21: Change of Variables",
    "section": "Nonlinear Transformations",
    "text": "Nonlinear Transformations\n\nConcaveConvex\n\n\nLet \\(X \\sim U\\left(0, \\displaystyle\\frac{\\pi}{2}\\right)\\) and \\(Y = \\sin(X)\\).\nCompare \\(\\text{E}[\\sin X]\\) and \\(\\sin(\\text{E}[X])\\)\n\n\n\nSuppose that a disease outbreak can be modeled where \\(X\\) is the population density of a city and \\(Y\\) is the number of diagnosed cases with\n\\[X \\sim U(0,100), \\quad Y = X^{3.2}\\]\nCompare \\(\\text{E}[X^{3.2}]\\) and \\(\\left(\\text{E}[X]\\right)^{3.2}\\)\n\n\n\n\n\n\n\n\n\n\nJensen’s Inequality\n\n\n\nThe previous two examples were demonstrations of , which states that\n\nIf \\(g\\) is a convex function of random variable \\(X\\), then\n\n\\[g(\\text{E}[X]) \\leq \\text{E}[g(X)]\\] - If \\(g\\) is a concave function of random variable \\(X\\), then\n\\[g(\\text{E}[X]) \\geq \\text{E}[g(X)]\\]\nwhere the equal signs are not included when the function \\(g\\) is strictly convex or strictly concave."
  },
  {
    "objectID": "posts/21_-_Change in Variables/Math_32_21_Change_of_Variables.html#looking-ahead",
    "href": "posts/21_-_Change in Variables/Math_32_21_Change_of_Variables.html#looking-ahead",
    "title": "21: Change of Variables",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Mar. 17:\n\nWHW8\nLHW7\n\nno lecture on Mar. 24, Apr. 3\nExam 2 will be on Mon., Apr. 10"
  },
  {
    "objectID": "posts/21_-_Change in Variables/Math_32_21_Change_of_Variables.html#misc",
    "href": "posts/21_-_Change in Variables/Math_32_21_Change_of_Variables.html#misc",
    "title": "21: Change of Variables",
    "section": "Misc",
    "text": "Misc"
  },
  {
    "objectID": "posts/22_-_Poisson Process/Math_32_22_Poisson_Process.html#motivation-for-the-poisson-process",
    "href": "posts/22_-_Poisson Process/Math_32_22_Poisson_Process.html#motivation-for-the-poisson-process",
    "title": "22: Poisson Process",
    "section": "Motivation for the Poisson Process",
    "text": "Motivation for the Poisson Process\n\nAssume a constant \\(\\lambda\\) of arrivals\nLet \\(N_{t}\\) be the number of arrivals in time interval \\([0,t]\\)\nHomogeneity: \\(\\text{E}[N_{t}] = \\lambda t\\) (``rate times time’’)\nIndependence: numbers of arrivals in disjoint time intervals are independent random variables\n\nGoal: Derive distribution of number of arrivals\n\nWe expect \\(\\text{E}[N_{t}] = \\lambda t\\) (``rate times time’’)\nPartition time interval \\([0,t]\\) into \\(n\\) subintervals\nAssuming \\(n\\) is large enough so that each subinterval has zero or one arrival (i.e. Bernoulli trial)\nProbability of arrival in a random subinterval: \\(p = \\displaystyle\\frac{\\lambda t}{n}\\)\n\nSo far, we are assuming \\(N_{t} \\sim \\text{Bin}(n,p)\\)\n\\[P(N_{t} = k) = \\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k} \\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n-k}\\]"
  },
  {
    "objectID": "posts/22_-_Poisson Process/Math_32_22_Poisson_Process.html#infinitessimal",
    "href": "posts/22_-_Poisson Process/Math_32_22_Poisson_Process.html#infinitessimal",
    "title": "22: Poisson Process",
    "section": "Infinitessimal",
    "text": "Infinitessimal\nHowever,\n\n\\(n\\) was arbitrary\ntime is a continuous variable\n\nSo let’s take the limit as \\(n\\) goes to infinity.\n\\[\\displaystyle\\lim_{n \\to \\infty} P(N_{t} = k) = \\displaystyle\\lim_{n \\to \\infty} {\\color{purple}\\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k}} {\\color{blue}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n}} {\\color{red}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{-k}}\\]\n\n\n\n\n\n\nInfinitessimal\n\n\n\n\n\nHandling the limit by its factors: \\[\\displaystyle\\lim_{n \\to \\infty} {\\color{red}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{-k}} = 1, \\quad \\displaystyle\\lim_{n \\to \\infty} {\\color{blue}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n}} = e^{-\\lambda t}\\]\n\\[\\begin{array}{rcl}\n  \\displaystyle\\lim_{n \\to \\infty} {\\color{purple}\\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k}} & = & (\\lambda t)^{k} \\displaystyle\\lim_{n \\to \\infty} \\binom{n}{k} \\left(\\displaystyle\\frac{1}{n}\\right)^{k} \\\\\n  ~ & = & (\\lambda t)^{k} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n!}{k!(n-k)!} \\cdot \\displaystyle\\frac{1}{n^{k}} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n!}{(n-k)!} \\cdot \\displaystyle\\frac{1}{n^{k}} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\prod_{i = 0}^{k-1} \\displaystyle\\frac{n - i}{n} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!}  \\displaystyle\\prod_{i = 0}^{k-1} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n - i}{n} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!}  \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/22_-_Poisson Process/Math_32_22_Poisson_Process.html#probability-density-function",
    "href": "posts/22_-_Poisson Process/Math_32_22_Poisson_Process.html#probability-density-function",
    "title": "22: Poisson Process",
    "section": "Probability Density Function",
    "text": "Probability Density Function\n\n\n\n\n\n\nProbability Density Function\n\n\n\nThus, if \\(X\\) is the number of observed events in this model, let \\(\\mu = \\lambda t\\) (was assumed to be constant), and \\[P(X = k) = \\displaystyle\\frac{\\mu^{k}e^{-\\mu}}{k!}\\]\nThis is called the probability mass function for the Poisson distribution. The Poisson distribution is a discrete distribution that tends to be used to model rare events."
  },
  {
    "objectID": "posts/22_-_Poisson Process/Math_32_22_Poisson_Process.html#expected-value",
    "href": "posts/22_-_Poisson Process/Math_32_22_Poisson_Process.html#expected-value",
    "title": "22: Poisson Process",
    "section": "Expected Value",
    "text": "Expected Value\nHere we will derive the expected value for a \\(\\text{Pois}(\\mu)\\) distribution.\n\n\n\n\n\n\nExpected Value\n\n\n\n\n\n\\[\\begin{array}{rcl}\n  \\text{E}[X] & = & \\displaystyle\\sum_{k = 0}^{\\infty} k \\cdot P(x = k) \\\\\n  ~ & = & \\displaystyle\\sum_{k = 0}^{\\infty} k \\cdot \\displaystyle\\frac{\\mu^{k}e^{-\\mu}}{k!} \\\\\n  ~ & = & 0 + \\displaystyle\\sum_{k = 1}^{\\infty} k \\cdot \\displaystyle\\frac{\\mu^{k}e^{-\\mu}}{k!} \\\\\n  ~ & = & \\displaystyle\\sum_{k = 1}^{\\infty} \\displaystyle\\frac{\\mu^{k}e^{-\\mu}}{(k-1)!} \\\\\n  ~ & = & \\displaystyle\\sum_{k = 1}^{\\infty} \\displaystyle\\frac{\\mu\\mu^{k-1}e^{-\\mu}}{(k-1)!} \\\\\n  ~ & = & \\mu e^{-\\mu}\\displaystyle\\sum_{k = 1}^{\\infty} \\displaystyle\\frac{\\mu^{k-1}}{(k-1)!} \\\\\n  ~ & = & \\mu e^{-\\mu}\\displaystyle\\sum_{k = 0}^{\\infty} \\displaystyle\\frac{\\mu^{k}}{k!} \\\\\n  ~ & = & \\mu e^{-\\mu}e^{\\mu} \\\\\n\\end{array}\\]\nRecall: By Taylor series, \\(e^{x} = \\displaystyle\\sum_{n = 0}^{\\infty} \\displaystyle\\frac{x^{n}}{n!}\\)\nTherefore \\(\\text{E}[X] = \\mu\\) for a \\(\\text{Pois}(\\mu)\\) distribution. Furthermore, \\(\\text{Var}(X) = \\mu\\) too for a \\(\\text{Pois}(\\mu)\\) distribution."
  },
  {
    "objectID": "posts/22_-_Poisson Process/Math_32_22_Poisson_Process.html#examples",
    "href": "posts/22_-_Poisson Process/Math_32_22_Poisson_Process.html#examples",
    "title": "22: Poisson Process",
    "section": "Examples",
    "text": "Examples\n\nExample 1Example 2\n\n\nCampus Safety: Stalking The following data on reported incidents on stalking comes from the University of California Merced report Safety Matters. Assume a Poisson distribution.\n\n\nFind the mean of the data.\nCompute the probability that exactly 2 stalking incidents will be reported this year.\n\n\n\n\n\n\n\nProbability Density Function\n\n\n\n\n\n\n\n\n\n\nCompute the probability that at least one stalking incident will be reported this year.\n\n\n\n\n\n\n\nProbability Density Function\n\n\n\n\n\n\n\n\n\n\n\nCampus Safety: Drug Law Violations The following data on arrests for drug law violations comes from the University of California Merced report Safety Matters. Assume a Poisson distribution.\n\n\nFind the mean of the data.\nCompute the probability that exactly 3 arrests will be made this year.\n\n\n\n\n\n\n\nProbability Density Function\n\n\n\n\n\n\n\n\n\n\nCompute the probability that at most one arrest will be made this year.\n\n\n\n\n\n\n\nProbability Density Function"
  },
  {
    "objectID": "posts/22_-_Poisson Process/Math_32_22_Poisson_Process.html#looking-ahead",
    "href": "posts/22_-_Poisson Process/Math_32_22_Poisson_Process.html#looking-ahead",
    "title": "22: Poisson Process",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Mar. 17:\n\nWHW8\nLHW7\n\nno lecture on Mar. 24, Apr. 3\nExam 2 will be on Mon., Apr. 10"
  },
  {
    "objectID": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#today-law-of-large-numbers",
    "href": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#today-law-of-large-numbers",
    "title": "23: Law of Large Numbers",
    "section": "Today: Law of Large Numbers",
    "text": "Today: Law of Large Numbers\nGoal: start to understand error as it relates to sample size\nObjectives:\n\nDistribution of the mean \\(\\bar{X}_{n}\\)\nChebyshev’s Inequality\nLaw of Large Numbers"
  },
  {
    "objectID": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#i.i.d.",
    "href": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#i.i.d.",
    "title": "23: Law of Large Numbers",
    "section": "i.i.d.",
    "text": "i.i.d.\n\n\n\n\n\n\ni.i.d.\n\n\n\nLet \\(X_{1}, X_{2}, X_{3}, ...\\) be an independent and identically distributed sequence of random variables (denoted ``i.i.d’’)\n\neach \\(X_{i}\\) is independent of each other\neach \\(X_{i}\\) has mean \\(\\mu\\)\neach \\(X_{i}\\) has variance \\(\\sigma^{2}\\)\n\n\n\nLet us now seek the distribution of the mean \\[\\bar{X}_{n} = \\displaystyle\\frac{X_{1} + X_{2} + ... + X_{n}}{n}\\]\n\nexpected value\nvariance"
  },
  {
    "objectID": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#distribution-of-mean",
    "href": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#distribution-of-mean",
    "title": "23: Law of Large Numbers",
    "section": "Distribution of Mean",
    "text": "Distribution of Mean\n\n\n\n\n\n\nDistribution of Mean\n\n\n\nIf \\(\\bar{X}_{n}\\) is the mean of \\(n\\) independent and identical random variables, each with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\), then we can describe the distribution of \\(\\bar{X}_{n}\\) with\n\\[\\text{E}[\\bar{X}_{n}] = \\mu \\quad\\text{and}\\quad \\text{Var}(\\bar{X}_{n}) = \\displaystyle\\frac{\\sigma^{2}}{n}\\]"
  },
  {
    "objectID": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#far-from-the-mean",
    "href": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#far-from-the-mean",
    "title": "23: Law of Large Numbers",
    "section": "Far from the Mean",
    "text": "Far from the Mean\n\n\n\n\n\n\nFar from the Mean\n\n\n\nIdea: we can get a sense of the probability that, for a particular boundary location \\(a\\), an observation lies outside of the interval \\[(\\mu - a, \\mu + a)\\]\n\n\n\n\\(\\mu\\): population average\n\\(a\\): tolerance\n\nClaim: \\(P(|X - \\mu| \\geq a) \\leq \\displaystyle\\frac{\\text{Var}(X)}{a^{2}}\\)"
  },
  {
    "objectID": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#chebyshevs-inequality",
    "href": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#chebyshevs-inequality",
    "title": "23: Law of Large Numbers",
    "section": "Chebyshev’s Inequality",
    "text": "Chebyshev’s Inequality\n\n\n\n\n\n\nChebyshev’s Inequality\n\n\n\nFor a random variable \\(X\\) and boundary location \\(a\\), \\[P(|X - \\mu| \\geq a) \\leq \\displaystyle\\frac{\\text{Var}(X)}{a^{2}}\\]\n\n\nThat is, if we know the variance of a distribution, we can compute an upper bound for the probability of rare events!"
  },
  {
    "objectID": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#law-of-large-numbers",
    "href": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#law-of-large-numbers",
    "title": "23: Law of Large Numbers",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\nThe Law of Large Numbers basically combines Chebyshev’s Inequality with the earlier work for the distribution of the mean:\n\n\\(\\text{E}[\\bar{X}_{n}] = \\mu \\quad\\text{and}\\quad \\text{Var}(\\bar{X}_{n}) = \\displaystyle\\frac{\\sigma^{2}}{n}\\)\n\\(P(|X - \\mu| \\geq a) \\leq \\displaystyle\\frac{\\text{Var}(X)}{a^{2}}\\)\n\nIdea: What happens when we observe a lot of data?\n\n\n\n\n\n\nLaw of Large Numbers\n\n\n\nTaking the limit as \\(n\\) goes to infinity, we arrive at the Law of Large Numbers:\n\\[\\displaystyle\\lim_{n \\to \\infty} P(|\\bar{X}_{n} - \\mu| \\geq a) \\leq \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{\\sigma^{2}}{a^{2} n} = 0\\]\n\n\nThat is, the probability that the mean of a sample of random variables is ``far’’ from the inherent expected value eventually goes to zero."
  },
  {
    "objectID": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#nerdy-example",
    "href": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#nerdy-example",
    "title": "23: Law of Large Numbers",
    "section": "Nerdy Example",
    "text": "Nerdy Example\nHow many numbers between zero and one do we have to add up to have a sum that is greater than one?\n\nLet \\(X_{i} \\sim U(0,1)\\) be i.i.d.\nLet \\(Y\\) be the amount of \\(X_{i}\\) added together to get a sum greater than one\nFor a conservative estimate, suppose \\(Y \\sim U(2,6)\\), then\n\n\\[\\text{Var}(Y) = \\displaystyle\\frac{4}{3}\\] - Empirically (i.e. computer simulation), we saw convergence toward\n\\[\\bar{Y}_{n} = e \\approx 2.7183\\]\nHow many trials are needed so that the simulations converge to a mean within 0.01 of the true answer with at least 95 percent probability?"
  },
  {
    "objectID": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#looking-ahead",
    "href": "posts/23_-_Law of Large Numbers/Math_32_23_Law_of_Large_Numbers.html#looking-ahead",
    "title": "23: Law of Large Numbers",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Mar. 24:\n\nLHW8\n\nno lecture on Mar. 24, Apr. 3\nExam 2 will be on Mon., Apr. 10\n\n\n\ntweet source"
  },
  {
    "objectID": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#today-estimators",
    "href": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#today-estimators",
    "title": "24: Estimators",
    "section": "Today: Estimators",
    "text": "Today: Estimators\nGoal: Explore generalization from samples to populations\nObjectives: Show the biased or unbiased estimation via\n\nsample mean \\(\\bar{x}\\)\nsample variance \\(s^{2}\\)\nsample standard deviation \\(s\\)"
  },
  {
    "objectID": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#demographics-example",
    "href": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#demographics-example",
    "title": "24: Estimators",
    "section": "Demographics Example",
    "text": "Demographics Example\nFrom our Demographics Survey data of Math 32 students, suppose that the following is a sample of observations of heights (in inches):\n\\[\\{x_{11} = 72, x_{12} = 61, x_{13} = 60, x_{14} = 75, x_{15} = 69\\}\\]\n\nthen \\(t_{1} = 67.4\\) inches is the sample mean.\n\nSuppose that the following is another sample of heights:\n\\[\\{x_{21} = 66, x_{22} = 78, x_{23} = 78, x_{24} = 77, x_{25} = 64\\}\\]\n\nthen \\(t_{2} = 72.6\\) inches is the sample mean.\n\nSuppose that the following is another sample of heights:\n\\[\\{x_{31} = 61, x_{32} = 59, x_{33} = 70, x_{34} = 61, x_{35} = 65\\}\\]\n\nthen \\(t_{3} = 63.2\\) inches is the sample mean.\n\n\n\n\n\n\n\nSampling Itself is Probabilistic\n\n\n\n\n\nObserve: the sample mean (usually) changes upon a new set of observations\n\n\n\n\nCan we calculate the average height of UC Merced students?\nHow can we calculate the average height of UC Merced students?\n\nThought: what if we take a mean of the sample means?"
  },
  {
    "objectID": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#estimators",
    "href": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#estimators",
    "title": "24: Estimators",
    "section": "Estimators",
    "text": "Estimators\n\n\n\n\n\n\nEstimators\n\n\n\nLet \\(T\\) be a random variable and \\(f\\) be some calculation \\[T = f(x_{1}, x_{2}, x_{3}, ...)\\]\nIf we are trying to estimate a population parameter \\(\\theta\\), we say that \\(T\\) is an unbiased estimator of \\(\\theta\\) if \\[\\text{E}[T] = \\theta\\]\n\n\nToday, we will look at situations where \\(f\\) is calculating the\n\nmean\nvariance\nstandard deviation"
  },
  {
    "objectID": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#mean",
    "href": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#mean",
    "title": "24: Estimators",
    "section": "Mean",
    "text": "Mean\n\nSetupCodeSimulationProof\n\n\nWe will run simulations with \\(X \\sim U(0,1)\\) because we know what the answers should be. The population mean is\n\\[\\mu = \\displaystyle\\frac{a + b}{2} = \\displaystyle\\frac{1}{2}\\]\n\n\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- mean(these_numbers) #record average\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = 1/2, color = \"red\", size = 3) +\n  labs(title = \"Simulation Sample Mean\",\n       subtitle = paste(\"black: sample distribution\\nred: true population mean\\nmean of sample means: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nLoosely speaking, since the sampling distribution “lines up” with the population mean, we say that the sample median is an unbiased estimator of the population mean.\n\n\n\\[\\begin{array}{rcl}\n      \\text{E}[\\bar{X}_{n}] & = & \\text{E}\\left( \\displaystyle\\frac{X_{1} + X_{2} + ... + X_{n}}{n} \\right) \\\\\n      ~ & = & \\displaystyle\\frac{1}{n}\\text{E}\\left( X_{1} + X_{2} + ... + X_{n} \\right) \\\\\n      ~ & = & \\displaystyle\\frac{1}{n}\\left(\\text{E}[X_{1}] + \\text{E}[X_{2}] + ... + \\text{E}[X_{n}] \\right) \\\\\n      ~ & = & \\displaystyle\\frac{1}{n}\\left(\\mu + \\mu + ... + \\mu \\right) \\\\\n      ~ & = & \\displaystyle\\frac{1}{n}\\left(n \\mu \\right) \\\\\n    \\end{array}\\]\nTherefore \\(\\text{E}[\\bar{X}_{n}] = \\mu\\)"
  },
  {
    "objectID": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#population-variance",
    "href": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#population-variance",
    "title": "24: Estimators",
    "section": "Population Variance",
    "text": "Population Variance\n\nSetupCodeSimulation\n\n\nWe will run simulations with \\(X \\sim U(0,1)\\) because we know what the answers should be. The population variance is\n\\[\\sigma^{2} = \\displaystyle\\frac{(b-a)^{2}}{12} = \\displaystyle\\frac{1}{12}\\]\nWe will explore what happens if we apply the population variance formula\n\\[\\sigma^{2} = \\frac{1}{N}\\displaystyle\\sum_{i=1}^{N} (x_{i} - \\mu)^{2}\\]\nto samples.\n\n\n\n# user-defined function\npop_var <- function(x){\n  N <- length(!is.na(x)) #population size\n  mu <- mean(x, na.rm = TRUE) #population mean\n  \n  # return population mean (note use of \"N\")\n  sum( (x - mu)^2 ) / N\n}\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- pop_var(these_numbers) #record population variance\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = 1/12, color = \"red\", size = 3) +\n  labs(title = \"Simulation of Population Variances\",\n       subtitle = paste(\"black: sample distribution\\nred: true population variance\\nmean of population variances: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLoosely speaking, since the sampling distribution tends to underestimate the population variance, we say that the population variance (with \\(N\\)) is a biased estimator of the population variance."
  },
  {
    "objectID": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#bessels-correction",
    "href": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#bessels-correction",
    "title": "24: Estimators",
    "section": "Bessel’s Correction",
    "text": "Bessel’s Correction\nCan we rescale the process for computing variance so that the operation is an unbiased estimator for the population variance?\nLet \\(X_{i}\\) be a set of \\(n\\) i.i.d. random variables from the same distribution with the same population variance \\(\\sigma^{2}\\). By independence, there is zero covariance.\nWe will compute the value of \\(k\\) so that\n\\[\\text{E}\\left[k \\cdot \\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X}_{n})^{2}}{n}\\right] = \\sigma^{2}\\]\nLemma: \\(\\text{Var}(X_{i} - \\bar{X}_{n}) = \\displaystyle\\frac{n-1}{n} \\cdot \\sigma^{2}\\)\n\n\n\n\n\n\nBessel’s Correction\n\n\n\n\n\nWe have derived the formula for the sample variance\n\\[S_{n}^{2} = \\displaystyle\\frac{1}{n-1}\\displaystyle\\sum_{i=1}^{n}(X_{i} - \\bar{X}_{n})^{2}\\]\nThat is, the “\\(n-1\\)” (Bessel’s correction) is in place so that the sample variance \\(s^{2}\\) is an unbiased estimator of the population variance \\(\\sigma^{2}\\)"
  },
  {
    "objectID": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#sample-variance",
    "href": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#sample-variance",
    "title": "24: Estimators",
    "section": "Sample Variance",
    "text": "Sample Variance\n\nSetupCodeSimulation\n\n\nWe will run simulations with \\(X \\sim U(0,1)\\) because we know what the answers should be. The population variance is\n\\[\\sigma^{2} = \\displaystyle\\frac{(b-a)^{2}}{12} = \\displaystyle\\frac{1}{12}\\]\nWe will explore what happens if we apply the sample variance formula\n\\[s^{2} = \\frac{1}{n-1}\\displaystyle\\sum_{i=1}^{n} (x_{i} - \\mu)^{2}\\]\nto samples.\n\n\n\n# user-defined function\nsamp_var <- function(x){\n  n  <- length(!is.na(x)) #sample size\n  xbar <- mean(x, na.rm = TRUE) #sample mean\n  \n  # return population mean (note use of \"n-1\")\n  sum( (x - xbar)^2 ) / (n-1)\n}\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- samp_var(these_numbers) #record sample variance\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = 1/12, color = \"red\", size = 3) +\n  labs(title = \"Simulation of Sample Variances\",\n       subtitle = paste(\"black: sample distribution\\nred: true population variance\\nmean of sample variances: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLoosely speaking, since the sampling distribution “lines up” with the population variance, we say that the sample variance (with \\(n-1\\)) is an unbiased estimator of the population variance."
  },
  {
    "objectID": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#sample-standard-deviation",
    "href": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#sample-standard-deviation",
    "title": "24: Estimators",
    "section": "Sample Standard Deviation",
    "text": "Sample Standard Deviation\n\nSetupCodeSimulationCommentary\n\n\nWe will run simulations with \\(X \\sim U(0,1)\\) because we know what the answers should be. The population standard deviation is\n\\[\\sigma = \\sqrt{\\displaystyle\\frac{(b-a)^{2}}{12}} = \\sqrt{\\displaystyle\\frac{1}{12}}\\]\nWe will explore what happens if we apply the sample variance formula\n\\[s = \\sqrt{\\frac{1}{n-1}\\displaystyle\\sum_{i=1}^{n} (x_{i} - \\mu)^{2}}\\]\nto samples.\n\n\n\n# user-defined function\nsamp_var <- function(x){\n  n  <- length(!is.na(x)) #sample size\n  xbar <- mean(x, na.rm = TRUE) #sample mean\n  \n  # return population mean (note use of \"n-1\")\n  sum( (x - xbar)^2 ) / (n-1)\n}\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- sqrt(samp_var(these_numbers)) #record sample standard deviation\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = sqrt(1/12), color = \"red\", size = 3) +\n  labs(title = \"Simulation of Sample Variances\",\n       subtitle = paste(\"black: sample distribution\\nred: true population variance\\nmean of sample standard deviations: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nLet \\(X_{i}\\) be a set of \\(n\\) i.i.d. random variables from the same distribution with the same population standard deviation \\(\\sigma\\). To avoid trivial situations, assume non-zero variance, so \\(\\sigma \\neq 0\\).\nIf \\(s = \\sqrt{ \\displaystyle\\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X}_{n})^{2}}{n-1} }\\) was an unbiased estimator, then \\(\\text{E}[s] = \\sigma\\)\nHowever, by Jensen’s Inequality, since \\(g(x) = x^{2}\\) is a convex function,\n\\[\\sigma^{2}  = \\text{E}[S_{n}^{2}] > \\left(\\text{E}[S_{n}]\\right)^{2}\\]\nand it follows that \\(\\text{E}[S_{n}] < \\sigma\\). Due to the underestimation, sample standard deviation \\(s\\) is a biased estimator of population standard deviation \\(\\sigma\\).\n\\[~\\]\nHowever, in practice, the discrepancy is usually so small that it is ignored."
  },
  {
    "objectID": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#looking-ahead",
    "href": "posts/24_-_Estimators/Math_32_24_Law_of_Large_Numbers.html#looking-ahead",
    "title": "24: Estimators",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\ndue Fri., Mar. 24:\n\nLHW8\n\nno lecture on Mar. 24, Apr. 3\nExam 2 will be on Mon., Apr. 10"
  },
  {
    "objectID": "posts/25_-_Central Limit Theorem/Math_32_25_Central_Limit_Theorem.html#today-central-limit-theorem",
    "href": "posts/25_-_Central Limit Theorem/Math_32_25_Central_Limit_Theorem.html#today-central-limit-theorem",
    "title": "25: Central Limit Theorem",
    "section": "Today: Central Limit Theorem",
    "text": "Today: Central Limit Theorem\nGoal: Consolidate our understanding of variance\nObjectives: Update our understanding of the normal distribution"
  },
  {
    "objectID": "posts/25_-_Central Limit Theorem/Math_32_25_Central_Limit_Theorem.html#bringing-it-all-together",
    "href": "posts/25_-_Central Limit Theorem/Math_32_25_Central_Limit_Theorem.html#bringing-it-all-together",
    "title": "25: Central Limit Theorem",
    "section": "Bringing it all Together",
    "text": "Bringing it all Together\n\n\n\n\n\n\nLinear Operators\n\n\n\nRecall, from our studies into linear operators,\n\n\\(\\text{E}[aX + bY + c] = a\\text{E}[X] + b\\text{E}[Y] + c\\)\n\\(\\text{Var}[aX + bY + c] = a^{2}\\text{Var}[X] + b^{2}\\text{Var}[Y] + 2ab\\text{Cov}(X,Y)\\)\n\nLet \\(X_{i}\\) be a set of \\(n\\) i.i.d. random variables from the same distribution with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). By induction, we can extrapolate these results for summations:\n\n\\(\\text{E}\\left[ \\displaystyle\\sum_{i=1}^{n} X_{i} \\right] = \\displaystyle\\sum_{i=1}^{n}\\text{E}\\left[ X_{i} \\right] = n\\mu\\)\n\\(\\text{Var}\\left[ \\displaystyle\\sum_{i=1}^{n} X_{i} \\right] = \\displaystyle\\sum_{i=1}^{n}\\text{Var}\\left[ X_{i} \\right] = n\\sigma^{2}\\)\n\nRecall, independence implies zero covariance.\n\n\n\n\n\n\n\n\nLaw of Large Numbers\n\n\n\nRecall, from our look at the Law of Large Numbers, for the sample mean of random variables \\(\\bar{X}_{n}\\),\n\n\\(\\text{E}[\\bar{X}_{n}] = \\mu\\)\n\\(\\text{Var}(\\bar{X}_{n}) = \\displaystyle\\frac{\\sigma^{2}}{n}\\)\n\n\n\n\n\n\n\n\n\nEstimators\n\n\n\nFinally,\n\nsample mean \\(\\bar{X}_{n}\\) is an unbiased estimator of population mean \\(\\mu\\)\nsample variance \\(S_{n}^{2}\\) is an unbiased estimator of population variance \\(\\sigma^{2}\\)\n\nand the sample standard deviation \\(s\\) is almost an unbiased estimator of the population standard deviation \\(\\sigma\\)"
  },
  {
    "objectID": "posts/25_-_Central Limit Theorem/Math_32_25_Central_Limit_Theorem.html#summation",
    "href": "posts/25_-_Central Limit Theorem/Math_32_25_Central_Limit_Theorem.html#summation",
    "title": "25: Central Limit Theorem",
    "section": "Summation",
    "text": "Summation\n\nLarger VarianceExample\n\n\nFollowing from how \\(\\text{Var}\\left[ \\displaystyle\\sum_{i=1}^{n} X_{i} \\right] = n\\sigma^{2}\\), it follows that the standard deviation for a summation is \\[\\sigma_{n} = \\sigma\\sqrt{n}\\]\nThink: as sample size \\(n\\) increases, does \\(\\sigma_{n}\\) increase or decrease?\n\\[z\\text{-score:} \\quad z = \\displaystyle\\frac{x - \\mu}{\\sigma} \\quad\\rightarrow\\quad Z_{n} = \\displaystyle\\frac{\\sum X_{i} - \\text{E}\\left[ \\displaystyle\\sum_{i=1}^{n} X_{i} \\right]}{\\sigma_{n}} = \\displaystyle\\frac{\\sum X_{i} - n\\mu}{\\sigma\\sqrt{n}}\\]\n\n\nFor Covid-19, the population statistics for sick patients are a mean of \\(\\mu = 4\\) days and standard deviation \\(\\sigma = 2\\) days (assuming a normal distribution since the number of confirmed cases is numerous). For an incoming case load of 9 sick patients, what is the probability that they need at least 32 days combined in the hospital?"
  },
  {
    "objectID": "posts/25_-_Central Limit Theorem/Math_32_25_Central_Limit_Theorem.html#central-limit-theorem",
    "href": "posts/25_-_Central Limit Theorem/Math_32_25_Central_Limit_Theorem.html#central-limit-theorem",
    "title": "25: Central Limit Theorem",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nSmaller VarianceExample\n\n\nFollowing from how \\(\\text{Var}\\left[ \\bar{X}_{n} \\right] = \\displaystyle\\frac{\\sigma^{2}}{n}\\), it follows that the standard deviation for an average is \\[\\sigma_{n} = \\displaystyle\\frac{\\sigma}{\\sqrt{n}}\\]\n\nThis \\(\\displaystyle\\frac{\\sigma}{\\sqrt{n}}\\) is also called the standard error.\n\nThink: as sample size \\(n\\) increases, does \\(\\sigma_{n}\\) increase or decrease?\n\\[z\\text{-score:} \\quad z = \\displaystyle\\frac{x - \\mu}{\\sigma} \\quad\\rightarrow\\quad Z_{n} = \\displaystyle\\frac{\\bar{X}_{n} - \\text{E}[\\bar{X}_{n}]}{\\sigma_{n}} = \\displaystyle\\frac{\\bar{X}_{n} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\]\n\n\nFor Covid-19, the population statistics for the incubation period are a mean of \\(\\mu = 8\\) days and standard deviation \\(\\sigma = 3\\) days (assuming a normal distribution since the number of confirmed cases is numerous). For a sample of 25 infected people, what is the probability that their average incubation period is fewer than 7 days?"
  },
  {
    "objectID": "posts/25_-_Central Limit Theorem/Math_32_25_Central_Limit_Theorem.html#looking-ahead",
    "href": "posts/25_-_Central Limit Theorem/Math_32_25_Central_Limit_Theorem.html#looking-ahead",
    "title": "25: Central Limit Theorem",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\nWHW9\nExam 2, Mon., Apr. 10\n\nmore information in weekly announcement\n\n\n\n\n\n\nLow battery?"
  },
  {
    "objectID": "posts/26_-_Likelihood/Math_32_26_Likelihood.html#notation",
    "href": "posts/26_-_Likelihood/Math_32_26_Likelihood.html#notation",
    "title": "26: Likelihood",
    "section": "Notation",
    "text": "Notation\n\n\n\n\n\n\nNotation\n\n\n\nRecall,\n\nLower-case \\(\\{x_{1}, x_{2}, x_{3}, ..., x_{n}\\}\\) is a set of observations\nUpper-case \\(\\{X_{1}, X_{2}, X_{3}, ..., X_{n}\\}\\) is a set of random variables (i.e. a data set)\nTreating \\(\\{X_{1}, X_{2}, ..., X_{n}\\}\\) as a set of \\(n\\) i.i.d. (independent and identically distributed) random variables is a common assumption.\nWith independence, \\[P(X_{1}, X_{2}, ..., X_{n}) = P(X_{1}) \\cdot P(X_{2}) \\cdot ... \\cdot P(X_{n})\\]\nEach individual probability is computed (at least theoretically) with a PDF (probability density function) \\[P(x_{i}) = f_{X}(x_{i})\\]"
  },
  {
    "objectID": "posts/26_-_Likelihood/Math_32_26_Likelihood.html#inverse",
    "href": "posts/26_-_Likelihood/Math_32_26_Likelihood.html#inverse",
    "title": "26: Likelihood",
    "section": "Inverse",
    "text": "Inverse\n\n\n\n\n\n\nInverse\n\n\n\nSuppose that we have a sample of data \\(\\{x_{1}, x_{2}, x_{3}, ..., x_{n}\\}\\). Now we want to model with a probability distribution, but we need to figure out the distribution’s parameters. Let us think about this in a Bayesian way:\n\\[{\\color{purple}{P(\\text{model} | \\text{data})}} = \\displaystyle\\frac{ {\\color{blue}{P(\\text{data} | \\text{model})} \\cdot P(\\text{model})} }{ {\\color{red}{P(\\text{data})}} }\\]\n\n\\({\\color{purple}{P(\\text{model} | \\text{data})}}\\) is the posterior probability that we want\n\\({\\color{blue}{P(\\text{data} | \\text{model})}}\\) is a likelihood\nSince the prior probability \\({\\color{red}{P(\\text{data})}}\\) is a constant …\n\n… we say that the posterior probability is proportional to the likelihood."
  },
  {
    "objectID": "posts/26_-_Likelihood/Math_32_26_Likelihood.html#likelihood",
    "href": "posts/26_-_Likelihood/Math_32_26_Likelihood.html#likelihood",
    "title": "26: Likelihood",
    "section": "Likelihood",
    "text": "Likelihood\n\nDefinitionExample\n\n\n\n\n\n\n\n\nLikelihood Function\n\n\n\nLet the likelihood function, in terms of a parameter \\(\\theta\\), be the joint probability\n\\[L(\\theta) = P(X_{1} = x_{1}, X_{2} = x_{2}, ..., X_{n} = x_{n}) = f_{X}(x_{1}) \\cdot f_{X}(x_{2}) \\cdots f_{X}(x_{n})\\]\nor\n\\[L\\left(\\theta; \\left\\{x_{i}\\right\\}_{i=1}^{n}\\right) = \\displaystyle\\prod_{i = 1}^{n} f_{X}(x_{i})\\]\n\n\n\n\nSuppose that we have data for how long a certain type and brand of light bulb operated (in the same working conditions), and that data in months was\n\\[6, \\quad 18, \\quad 29, \\quad 44, \\quad 48\\] Goal: characterize the top 5 percent of light bulbs.\n\nBuild the likelihood function assuming an exponential distribution.\nCompute the likelihood that \\(\\mu = 25\\).\nCompute the likelihood that \\(\\mu = 50\\)."
  },
  {
    "objectID": "posts/26_-_Likelihood/Math_32_26_Likelihood.html#log-likelihood",
    "href": "posts/26_-_Likelihood/Math_32_26_Likelihood.html#log-likelihood",
    "title": "26: Likelihood",
    "section": "Log Likelihood",
    "text": "Log Likelihood\n\nLogarithmsExample\n\n\n\n\n\n\n\n\nLogarithms\n\n\n\nYou know that logarithms make large numbers smaller. More precisely, \\[\\ln(x) < x, \\quad x > 1\\]\nExample: \\(\\ln(1234) \\approx 7.1180\\)\nDid you know that logarithms make small numbers larger (in size). More precisely, \\[|\\ln(x)| > x, \\quad 0 < x < 1\\]\nExample: \\(|\\ln(0.1234)| \\approx 2.0923\\)\nFrom pre-calculus, recall the properties of logarithms: \\[\\ln(AB) = \\ln(A) + \\ln(B), \\quad \\ln\\left(\\displaystyle\\frac{A}{B}\\right) = \\ln A - \\ln B, \\quad \\ln(A^{c}) = c\\ln A\\]\n\n\n\n\nFor modeling with the exponential distribution, we saw that the likelihood function was\n\\[L\\left(\\lambda; \\{x_{i}\\}_{i=1}^{n}\\right) = \\displaystyle\\prod_{i=1}^{n} f_{X}(x_{i}) = \\lambda^{n}e^{-\\lambda\\sum x_{i}}\\]\nWe take the natural logarithm to compute the log-likelihood function\n\\[\\ell\\left(\\lambda; \\{x_{i}\\}_{i=1}^{n}\\right) = \\ln L\\left(\\lambda; \\{x_{i}\\}_{i=1}^{n}\\right) = n\\ln\\lambda - \\lambda\\displaystyle\\sum_{i=1}^{n} x_{i}\\]\n\nCompute the log-likelihood that \\(\\mu = 25\\).\nCompute the log-likelihood that \\(\\mu = 50\\)."
  },
  {
    "objectID": "posts/26_-_Likelihood/Math_32_26_Likelihood.html#visuals",
    "href": "posts/26_-_Likelihood/Math_32_26_Likelihood.html#visuals",
    "title": "26: Likelihood",
    "section": "Visuals",
    "text": "Visuals"
  },
  {
    "objectID": "posts/26_-_Likelihood/Math_32_26_Likelihood.html#looking-ahead",
    "href": "posts/26_-_Likelihood/Math_32_26_Likelihood.html#looking-ahead",
    "title": "26: Likelihood",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\nWHW9\nExam 2, Mon., Apr. 10\n\nmore information in weekly announcement\n\n\n\n\ntweet source"
  },
  {
    "objectID": "posts/27_-_Maximum Likelihood/Math_32_27_Maximum_Likelihood.html#today-maximum-likelihood",
    "href": "posts/27_-_Maximum Likelihood/Math_32_27_Maximum_Likelihood.html#today-maximum-likelihood",
    "title": "27: Maximum Likelihood",
    "section": "Today: Maximum Likelihood",
    "text": "Today: Maximum Likelihood\nGoal: Modify distribution parameters based on observed data\nObjectives:\n\nderive maximum likelihood estimate for the exponential distribution\nderive maximum likelihood estimate for the Poisson distribution"
  },
  {
    "objectID": "posts/27_-_Maximum Likelihood/Math_32_27_Maximum_Likelihood.html#notation",
    "href": "posts/27_-_Maximum Likelihood/Math_32_27_Maximum_Likelihood.html#notation",
    "title": "27: Maximum Likelihood",
    "section": "Notation",
    "text": "Notation\n\n\n\n\n\n\nNotation\n\n\n\nRecall,\n\nLower-case \\(\\{x_{1}, x_{2}, x_{3}, ..., x_{n}\\}\\) is a set of observations\nUpper-case \\(\\{X_{1}, X_{2}, X_{3}, ..., X_{n}\\}\\) is a set of random variables (i.e. a data set)\nTreating \\(\\{X_{1}, X_{2}, ..., X_{n}\\}\\) as a set of \\(n\\) i.i.d. (independent and identically distributed) random variables is a common assumption.\nWith independence, \\[P(X_{1}, X_{2}, ..., X_{n}) = P(X_{1}) \\cdot P(X_{2}) \\cdot ... \\cdot P(X_{n})\\]\nEach individual probability is computed (at least theoretically) with a PDF (probability density function) \\[P(x_{i}) = f_{X}(x_{i})\\]"
  },
  {
    "objectID": "posts/27_-_Maximum Likelihood/Math_32_27_Maximum_Likelihood.html#likelihood",
    "href": "posts/27_-_Maximum Likelihood/Math_32_27_Maximum_Likelihood.html#likelihood",
    "title": "27: Maximum Likelihood",
    "section": "Likelihood",
    "text": "Likelihood\n::::: {.panel-tabset}"
  },
  {
    "objectID": "posts/27_-_Maximum Likelihood/Math_32_27_Maximum_Likelihood.html#definition",
    "href": "posts/27_-_Maximum Likelihood/Math_32_27_Maximum_Likelihood.html#definition",
    "title": "27: Maximum Likelihood",
    "section": "Definition",
    "text": "Definition\n\n\n\n\n\n\nLikelihood Function\n\n\n\nLet the likelihood function, in terms of a parameter \\(\\theta\\), be the joint probability\n\\[L(\\theta) = P(X_{1} = x_{1}, X_{2} = x_{2}, ..., X_{n} = x_{n}) = f_{X}(x_{1}) \\cdot f_{X}(x_{2}) \\cdots f_{X}(x_{n})\\]\nor\n\\[L\\left(\\theta; \\left\\{x_{i}\\right\\}_{i=1}^{n}\\right) = \\displaystyle\\prod_{i = 1}^{n} f_{X}(x_{i})\\]"
  },
  {
    "objectID": "posts/27_-_Maximum Likelihood/Math_32_27_Maximum_Likelihood.html#maximum-likelihood",
    "href": "posts/27_-_Maximum Likelihood/Math_32_27_Maximum_Likelihood.html#maximum-likelihood",
    "title": "27: Maximum Likelihood",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nDefinitionVisualsExample 1Example 2\n\n\n\n\n\n\n\n\nLikelihood Function\n\n\n\n Given a data set \\(\\{x_{1}, x_{2}, ..., x_{n}\\}\\), we seek the desired parameter(s) that makes realizing the data set most likely.\n\\[L\\left(\\theta; \\left\\{x_{i}\\right\\}_{i=1}^{n}\\right) = \\displaystyle\\prod_{i = 1}^{n} f_{X}(x_{i})\\]\n\n\n\n\n\n\n\n\nMaximization\n\n\n\nFrom calculus, recall that the main step in maximizing the value of a function is setting the first derivative equal to zero.\n\n\n\n\n \n\n\nGiven a data set \\(\\{x_{1}, x_{2}, ..., x_{n}\\}\\), assume an \\(\\text{Exp}(\\lambda)\\) distribution.\n\nCompute the value of rate parameter \\(\\lambda\\) that maximizes the likelihood of the data set.\nCompute the likelihood at the maximum likelihood estimate (MLE).\nCharacterize the top 5 percent of light bulbs.\n\n\n\nGiven a data set \\(\\{x_{1}, x_{2}, ..., x_{n}\\}\\), assume an \\(\\text{Pois}(\\lambda)\\) distribution. Compute the value of parameter \\(\\lambda\\) that maximizes the likelihood of the data set."
  },
  {
    "objectID": "posts/27_-_Maximum Likelihood/Math_32_27_Maximum_Likelihood.html#estimators-revisited",
    "href": "posts/27_-_Maximum Likelihood/Math_32_27_Maximum_Likelihood.html#estimators-revisited",
    "title": "27: Maximum Likelihood",
    "section": "Estimators Revisited",
    "text": "Estimators Revisited\nIf we sample from a theoretical \\(U(0, M)\\) distribution, the sample maximum \\(s_{M}\\) of each sample is less than or equal to \\(M\\)\n\\[s_{M} \\leq M\\]\nIt would follow that the average of the sample maxima underestimates the true maximum\n\\[\\text{E}[s_{M}] \\leq M\\]\nTherefore the sample maximum is a biased estimator of the true maximum.\n\\[~\\]\nSimilarly, the sample minimum \\(s_{m}\\) from a \\(U(m, 0)\\) distribution overestimates\n\\[\\text{E}[s_{m}] \\geq m\\]\nTherefore the sample min-mum is a biased estimator of the true minumum."
  },
  {
    "objectID": "posts/27_-_Maximum Likelihood/Math_32_27_Maximum_Likelihood.html#looking-ahead",
    "href": "posts/27_-_Maximum Likelihood/Math_32_27_Maximum_Likelihood.html#looking-ahead",
    "title": "27: Maximum Likelihood",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\nFinal Exam will be on May 6\n\n\ntweet source"
  },
  {
    "objectID": "posts/28_-_Beta Distribution/Math_32_28_Beta_Distribution.html#today-beta-distribution",
    "href": "posts/28_-_Beta Distribution/Math_32_28_Beta_Distribution.html#today-beta-distribution",
    "title": "28: Beta Distribution",
    "section": "Today: Beta Distribution",
    "text": "Today: Beta Distribution\nGoal: Explore a distribution of proportions\nObjectives:\n\nexplore the beta distribution\nexplore the gamma function\n\n\n\n\n\n\n\nOdds\n\n\n\n\n\nIn probability, the saying\n\\[\\text{the odds of observing } c \\text{ is }a \\text{ to } b\\]\nis equivalent to the probability\n\\[P(c) = \\displaystyle\\frac{a}{a + b}\\]\n\n\n\n\n\n\n\n\n\nBinomial Likelihood\n\n\n\n\n\nSince we have a two-state situation of observing \\(a\\) or not among \\(a + b\\) trials, we can envision a binomial situation \\(\\text{Bin}(a + b, a)\\), while probability \\(x\\) obeys \\(0 \\leq x \\leq 1\\). That is, we might want some flexibility in understanding our probability \\(x\\). By Bayes’ Rule, the posterior distribution is\n\\[P(X = x | N = a) = \\displaystyle\\frac{P(N = a | X = x) \\cdot P(X = x)}{P(N = a)}\\]\nwhile the likelihood is seen from a binomial distribution\n\\[P(N = a | X = x) = \\binom{a+b}{a}x^{a}(1-x)^{b}\\]"
  },
  {
    "objectID": "posts/28_-_Beta Distribution/Math_32_28_Beta_Distribution.html#beta-distribution",
    "href": "posts/28_-_Beta Distribution/Math_32_28_Beta_Distribution.html#beta-distribution",
    "title": "28: Beta Distribution",
    "section": "Beta Distribution",
    "text": "Beta Distribution\nIf \\(X \\sim \\text{Beta}(\\alpha, \\beta)\\), then the probability density function (PDF) is\n\\[f(X = x) = \\begin{cases}\n  \\displaystyle\\frac{1}{B(\\alpha, \\beta)}x^{\\alpha - 1}(1-x)^{\\beta - 1}, & 0 < x < 1 \\\\\n  0, & \\text{otherwise}\n\\end{cases}\\]\nwhere the normalization constant\n\\[B(\\alpha, \\beta) = \\displaystyle\\int_{0}^{1} \\! x^{\\alpha - 1}(1-x)^{\\beta - 1} \\, dx\\]\nto ensure that the total area under the curve is one unit.\n\n\n\n\n\n\nOffsets\n\n\n\n\n\nThe offset notation \\[\\begin{array}{rcl}\n  \\alpha & = & a + 1 \\\\\n  \\beta & = & b + 1 \\\\\n\\end{array}\\] is there to streamline the statistics seen later (such as the expected value and the variance)."
  },
  {
    "objectID": "posts/28_-_Beta Distribution/Math_32_28_Beta_Distribution.html#gamma-function",
    "href": "posts/28_-_Beta Distribution/Math_32_28_Beta_Distribution.html#gamma-function",
    "title": "28: Beta Distribution",
    "section": "Gamma Function",
    "text": "Gamma Function\n\n\n\n\n\n\nGamma Function\n\n\n\n\n\nThat normalization constant\n\\[B(\\alpha, \\beta) = \\displaystyle\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\]\ncan be viewed in terms of the \n\\[\\Gamma(x) = \\displaystyle\\int_{0}^{\\infty} \\! t^{x-1}e^{-t} \\, dt\\]\n\n\n\nClaim: \\(\\Gamma(x+1) = x\\Gamma(x)\\)\n\n\n\n\n\n\nConnection to Factorials\n\n\n\n\n\nAlong with computing \\(\\Gamma(1) = 1\\), it follows that for natural numbers \\[\\Gamma(x) = (x-1)!\\]\n\n\n\n\n\n\n\n\n\nGeneralized Factorial Function\n\n\n\n\n\nHowever, the gamma function allows us to input real numbers. For example,\n\\[\\begin{array}{rcll}\n  \\Gamma\\left(\\displaystyle\\frac{1}{2}\\right) & = & \\displaystyle\\int_{0}^{\\infty} \\! t^{-1/2}e^{-t} \\, dt & \\text{defintion of gamma function} \\\\\n  ~ & = & \\displaystyle\\int_{0}^{\\infty} \\! u^{-1}e^{-u^{2}}(2u) \\, du & \\text{substitution } t = u^{2} \\rightarrow dt = 2u \\, du \\\\\n  ~ & = & 2\\displaystyle\\int_{0}^{\\infty} \\! e^{-u^{2}} \\, du & \\text{algebra} \\\\\n  ~ & = & \\displaystyle\\int_{-\\infty}^{\\infty} \\! e^{-u^{2}} \\, du & \\text{even function}  \\\\\n  ~ & = & \\sqrt{\\pi} & \\text{Gaussian} \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/28_-_Beta Distribution/Math_32_28_Beta_Distribution.html#beta-distribution-1",
    "href": "posts/28_-_Beta Distribution/Math_32_28_Beta_Distribution.html#beta-distribution-1",
    "title": "28: Beta Distribution",
    "section": "Beta Distribution",
    "text": "Beta Distribution\nIn terms of the gamma function, we now have that the beta distribution PDF is\n\\[f(X = x) = \\begin{cases}\n  \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha - 1}(1-x)^{\\beta - 1}, & 0 < x < 1 \\\\\n  0, & \\text{otherwise}\n\\end{cases}\\] To understand the probabilistic environment, let us derive the expected value.\n\n\n\n\n\n\nExpected Value\n\n\n\n\n\n\\[\\begin{array}{rcl}\n  \\text{E}[X] & = & \\displaystyle\\int_{-\\infty}^{\\infty} \\! x \\cdot f_{X}(x) \\, dx \\\\\n  ~ & = & \\displaystyle\\int_{0}^{1} \\! x \\cdot \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha - 1}(1-x)^{\\beta - 1} \\, dx \\\\\n  ~ & = & \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\displaystyle\\int_{0}^{1} \\! x^{(\\alpha+1) - 1}(1-x)^{\\beta - 1} \\, dx \\\\\n  ~ & = & \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\cdot \\displaystyle\\frac{\\Gamma(\\alpha + 1)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta + 1)}\\\\\n  ~ & = & \\displaystyle\\frac{\\Gamma(\\alpha + 1)}{\\Gamma(\\alpha)} \\cdot \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha + \\beta + 1)} \\\\\n  ~ & = & \\displaystyle\\frac{\\alpha\\Gamma(\\alpha)}{\\Gamma(\\alpha)} \\cdot \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{(\\alpha + \\beta)\\Gamma(\\alpha + \\beta)} \\\\\n  ~ & = & \\displaystyle\\frac{\\alpha}{\\alpha + \\beta} \\\\\n\\end{array}\\]\n\n\n\n\n\n\n\n\n\nVariance\n\n\n\n\n\nSimilarly, the variance for \\(X \\sim \\text{Beta}(\\alpha, \\beta)\\) is \\[\\sigma^{2} = \\displaystyle\\frac{\\alpha\\beta}{(\\alpha + \\beta + 1)(\\alpha + \\beta)^{2}}\\]"
  },
  {
    "objectID": "posts/28_-_Beta Distribution/Math_32_28_Beta_Distribution.html#example",
    "href": "posts/28_-_Beta Distribution/Math_32_28_Beta_Distribution.html#example",
    "title": "28: Beta Distribution",
    "section": "Example",
    "text": "Example\nIf we have 3 heads and 2 tails in a trial of flipping an unfair coin, assume a beta distribution and build a range-rule-of-thumb interval \\((\\mu - 2\\sigma, \\mu + 2\\sigma)\\) for the posterior probability.\n\n\n\n\n\n\nParameters\n\n\n\n\n\n\\[a = 3, b = 2 \\quad\\Rightarrow\\quad \\alpha = 4, \\beta = 3\\] \\[\\mu = \\text{E}[X] = \\displaystyle\\frac{\\alpha}{\\alpha + \\beta} = \\displaystyle\\frac{4}{7}, \\quad \\sigma^{2} = \\displaystyle\\frac{\\alpha\\beta}{(\\alpha + \\beta + 1)(\\alpha + \\beta)^{2}} = \\displaystyle\\frac{12}{8(7)^{2}}\\] and our range-rule-of-thumb interval is \\[\\displaystyle\\frac{4}{7} \\pm \\displaystyle\\frac{2}{7}\\sqrt{\\displaystyle\\frac{3}{2}}\\] or approximately \\((0.2215, 0.9214)\\)"
  },
  {
    "objectID": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#today-linear-regression",
    "href": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#today-linear-regression",
    "title": "29: Linear Regression",
    "section": "Today: Linear Regression",
    "text": "Today: Linear Regression\nGoal: Summarize bivariate data\nObjectives:\n\ndetermine a best-fit line from a bivariate data set\nmake predictions based on a linear regression model"
  },
  {
    "objectID": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#moxillation",
    "href": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#moxillation",
    "title": "29: Linear Regression",
    "section": "Moxillation",
    "text": "Moxillation\n\nQuery: predict how much moxillation will take place at 70 traxolline."
  },
  {
    "objectID": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#residuals",
    "href": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#residuals",
    "title": "29: Linear Regression",
    "section": "Residuals",
    "text": "Residuals\n\n\nGoal: Given a bivariate data set \\(\\{x_{i}, y_{i}\\}_{i=1}^{n}\\), form a linear regression model\n\\[\\hat{y} = a + bx\\]\nthat ``best fits’’ the data. Note that such a line will not go through all of the data (except in linear, deterministic situations), so\n\ndenote \\(y_{i}\\) for true outcomes\ndenote \\(\\hat{y}_{i}\\) for estimates (or predictions)\nthen \\(y_{i} - \\hat{y}_{i}\\) is the \\(i^{\\text{th}}\\) residual\n\n\n\n\n\nimage credit: www.jmp.com"
  },
  {
    "objectID": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#method-of-least-squares",
    "href": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#method-of-least-squares",
    "title": "29: Linear Regression",
    "section": "Method of Least Squares",
    "text": "Method of Least Squares\nLike our derivation of formulas for variance and standard deviation, scientists decided to square the residuals (focus on size of residuals, avoid positive versus negative signs). Let the total error be\n\\[E(a,b) = \\displaystyle\\sum_{i = 1}^{n} (y_{i} - \\hat{y}_{i})^{2} = \\displaystyle\\sum_{i = 1}^{n} (y_{i} - a - bx_{i})^{2} \\]\n\nThe ``best-fit line’’ minimizes the error.\nTo minimize the error, start by setting the partial derivatives equal to zero:\n\n\\[\\displaystyle\\frac{\\partial E}{\\partial a} = 0, \\quad \\displaystyle\\frac{\\partial E}{\\partial b} = 0\\] Thankfully, the function \\(E(a,b)\\) is an elliptical paraboloid, so there is a global minimum at the critical point, and that minimum is found where\n\\[a = \\displaystyle\\frac{ (\\sum y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }, \\quad b = \\displaystyle\\frac{ n\\sum x_{i}y_{i} - (\\sum x_{i})(\\sum y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }\\]\n\n\n\n\n\n\nBest-Fit Linear Regression Model\n\n\n\nIf sample means \\(\\bar{x}\\) and \\(\\bar{y}\\), sample standard deviations \\(s_{x}\\) and \\(s_{y}\\), and correlation coefficient \\(r\\) were previously computed, then the best-fit linear regression line \\(\\hat{y} = mx + b\\) is computed with\n\\[m = \\displaystyle\\frac{ rs_{y} }{ s_{x} }, \\quad b = \\bar{y} - m\\bar{x}\\]\n\nIf correlation \\(r > 0\\), then the slope of the regression line is also positive\nIf correlation \\(r < 0\\), then the slope of the regression line is also negative\n\n\n\nIn a scatterplot, an outlier is a point lying far away from the other data points. Paired sample data may include one or more influential points, which are points that strongly affect the graph of the regression line."
  },
  {
    "objectID": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#estimators-for-the-coefficients",
    "href": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#estimators-for-the-coefficients",
    "title": "29: Linear Regression",
    "section": "Estimators for the Coefficients",
    "text": "Estimators for the Coefficients\n\n\n\n\n\n\nEstimators for the Coefficients\n\n\n\nClaim: Treating \\(Y\\) as a random variable for the true outcomes, the least-squares estimators\n\\[A = \\displaystyle\\frac{ (\\sum Y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}Y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }, \\quad B = \\displaystyle\\frac{ n\\sum x_{i}Y_{i} - (\\sum x_{i})(\\sum Y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }\\]\nare unbiased estimators of \\(a\\) and \\(b\\) respectively.\n\n\nProof: We need to show that the expected values \\(\\text{E}[A] = a\\) and \\(\\text{E}[B] = b\\)\n\\[\\begin{array}{rcl}\n  \\text{E}[B] & = & \\displaystyle\\frac{ n\\sum x_{i}\\text{E}[Y_{i}] - (\\sum x_{i})(\\sum \\text{E}[Y_{i}]) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n  ~ & = & \\displaystyle\\frac{ n\\sum x_{i}(a + bx_{i}) - (\\sum x_{i})[\\sum (a + bx_{i})] }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n  ~ & = & \\displaystyle\\frac{ n\\sum x_{i}(a + bx_{i}) - (\\sum x_{i})(na + b\\sum x_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n  ~ & = & \\displaystyle\\frac{ an\\sum x_{i} + bn\\sum x_{i}y_{i} - an\\sum x_{i} + b(\\sum x_{i})^{2} }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n  ~ & = & \\displaystyle\\frac{ b[n\\sum x_{i}^{2} - (\\sum x_{i})^{2}] }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n\\end{array}\\]\nWe have shown that expected value \\(\\text{E}[B] = b\\), so \\(B\\) is an unbiased estimator of \\(b\\).\nThen,\n\\[\\begin{array}{rcl}\n  \\text{E}[A] & = & \\text{E}[\\bar{Y}_{n}] - \\text{E}[B]\\bar{x} \\\\\n  ~ & = & \\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} \\text{E}[Y_{i}] - b\\bar{x} \\\\\n  ~ & = & \\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} (a + bx_{i}) - b\\bar{x} \\\\\n  ~ & = & a + b\\bar{x} - b\\bar{x} \\\\\n\\end{array}\\]\nWe have shown that expected value \\(\\text{E}[A] = a\\), so \\(A\\) is an unbiased estimator of \\(a\\)."
  },
  {
    "objectID": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#estimator-for-the-variance",
    "href": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#estimator-for-the-variance",
    "title": "29: Linear Regression",
    "section": "Estimator for the Variance",
    "text": "Estimator for the Variance\n\nthis slide is optional (i.e. not on exams)\n\n\n\n\n\n\n\nEstimator for the Variance\n\n\n\n\nBut can we estimate the population variance \\(\\sigma_{y}^{2}\\) without knowing the relationship between \\(x\\) and \\(y\\)?\nStarting with the average of the squared residuals, \\[R_{n}^{2} = \\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} (Y_{i} - a - bx_{i})^{2}\\] it can be computed that expected value \\(\\text{E}[R_{n}^{2}] = \\displaystyle\\frac{n-2}{n}\\sigma_{y}^{2}\\)\nRescaling, \\[S_{n}^{2} = \\displaystyle\\frac{1}{n-2}\\displaystyle\\sum_{i=1}^{n} (Y_{i} - a - bx_{i})^{2}\\] is an unbiased estimator of the population variance \\(\\sigma_{y}^{2}\\)"
  },
  {
    "objectID": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#predictions",
    "href": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#predictions",
    "title": "29: Linear Regression",
    "section": "Predictions",
    "text": "Predictions\n\n\n\n\n\n\nPredictions\n\n\n\nFinally, with a linear regression model \\(\\hat{y} = a + bx\\) in place, plug in a data value \\(x\\) to form a prediction \\(\\hat{y}\\)\n\n\n\nExample 1Example 2\n\n\nThe table below displays data for enrollment levels at UC Merced between the years 2011 and 2016.\n\nIf the enrollment numbers are \\(\\{x_{i}\\}\\) data, then the sample mean is \\(\\bar{x} = 6240.5\\) students and the sample standard deviation is \\(s_{x} = 737.3091\\) students. If the drug abuse violations are \\(\\{y_{i}\\}\\) data, then the sample mean is \\(\\bar{y} = 22.5\\) violations and the sample standard deviation is \\(s_{y} = 9.7108\\) violations. Finally, the correlation coefficient is \\(r \\approx 0.0471\\).\n\nFind the best-fit linear regression line in the \\(\\hat{y} = mx + b\\) form.\nWhat does the regression model predict for the number of drug abuse violations for the year 2017 enrollment of 7967 students?\nWhat does the regression model predict for the enrollment if there are 36 drug abuse violations reported?\n\n\n\n\n\n\n\nJoyce Byers notices that magnets are falling off the shelves. She consults the kids’ science teacher Mr. Clarke. Suppose that they conduct an investigation where they strength of magnetic fields versus the distance from Joyce’s house and then record the amounts (in the table below). Let us treat the distances as the \\(\\{x_{i}\\}\\) data and the magnetic flux density as the \\(\\{y_{i}\\}\\) data. Joyce fortunately has a ``scientific calculator’’ from the government lab, and she calculates the sample means of \\(\\bar{x} = 15\\) miles and \\(\\bar{y} \\approx 8.9999\\) teslas. Mr. Clarke then calculates sample standard deviations of \\(s_{x} \\approx 1.9850\\) and \\(s_{y} \\approx 1.3742\\) along with a correlation of \\(r \\approx 0.9630\\). Build a linear regression model to predict the magnetic flux density when they are 18 miles away from Joyce’s house."
  },
  {
    "objectID": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#looking-ahead",
    "href": "posts/29_-_Linear Regression/Math_32_29_Linear_Regression.html#looking-ahead",
    "title": "29: Linear Regression",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\nWHW10\nLHW9\n\nFinal Exam will be on May 6\n\nmore information in weekly announcement\n\n\n\ntweet source"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#regression-analysis",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#regression-analysis",
    "title": "30: Regression Analysis",
    "section": "30: Regression Analysis",
    "text": "30: Regression Analysis\nGoal: Discuss the validity of regression models\nObjectives:\n\nexplore nonlinear regression models\npractice using the coefficient of determination"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#setting",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#setting",
    "title": "30: Regression Analysis",
    "section": "Setting",
    "text": "Setting\nKaggle was founded in April 2010 and is a data science resource that hosts many public data sets and hosts machine learning competitions.\n\npredictor variable\n\n\\[X: \\text{months since founding}\\]\n\nresponse variable\n\n\\[Y: \\text{number of users (in millions)}\\]"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#data",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#data",
    "title": "30: Regression Analysis",
    "section": "Data",
    "text": "Data\nThis data set is relatively small, so we can quickly transcribe it into an R data frame.\n\ndf <- data.frame(\n  months = c(0,86, 101, 110, 118, 123, 129, 134, 139, 143, 147),\n  users = 0:10\n)\n\nSource: https://www.kaggle.com/discussions/general/332147"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#linear-model",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#linear-model",
    "title": "30: Regression Analysis",
    "section": "Linear Model",
    "text": "Linear Model\n\\[\\hat{y} = a + bx\\]\n\n\n\n\n\n\nDependent Variable\n\n\n\n\n\nIn model equations, the response variable is listed first\n\\[\\sim \\quad \\text{:} \\quad \\text{explained by}\\]\nthen the predictor variables.\n\n\n\n\nlinear_model <- lm(users ~ months, data = df)\n\n\nlinear_model\n\n\nCall:\nlm(formula = users ~ months, data = df)\n\nCoefficients:\n(Intercept)       months  \n   -2.38965      0.06609"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#prediction",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#prediction",
    "title": "30: Regression Analysis",
    "section": "Prediction",
    "text": "Prediction\nExample: As of this writing (April 2023), there have been 156 months since Kaggle was established. Use a linear regression model to predict how many users are on Kaggle.\n\npredict(linear_model, newdata = data.frame(months = 156))\n\n       1 \n7.919814"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#data-visualization",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#data-visualization",
    "title": "30: Regression Analysis",
    "section": "Data Visualization",
    "text": "Data Visualization\nIn ggplot2, the geom_smooth layer can quickly graph the linear regression model.\n\nGraphCode\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nsubtitle_string = \"<span style='color:#FF0000'>red: <b>regression model</b></span>, \n<span style='color:#0000FF'>blue: <b>true data</b></span>\"\n\ndf |>\n  ggplot(aes(x = months, y = users)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(color = \"red\", method = \"lm\", se = FALSE) +\n  labs(title = \"Kaggle Community\",\n       subtitle = subtitle_string,\n       caption = \"Math 32\",\n       x = \"months since April 2010 founding\",\n       y = \"number of users (in millions)\") +\n  theme_minimal() +\n  theme(plot.subtitle = element_markdown())"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#quadratic-regression",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#quadratic-regression",
    "title": "30: Regression Analysis",
    "section": "Quadratic Regression",
    "text": "Quadratic Regression\n\\[\\hat{y} = a + bx + cx^{2}\\]\nBy default, R uses orthogonal polynomials in model creation. If we would rather gather easy-to-interpret coefficients in the model form above, use raw = TRUE.\n\nd2_model <- lm(users ~ poly(months, 2, raw = TRUE), data = df)\n\n\nd2_model\n\n\nCall:\nlm(formula = users ~ poly(months, 2, raw = TRUE), data = df)\n\nCoefficients:\n                 (Intercept)  poly(months, 2, raw = TRUE)1  \n                     0.06293                      -0.08169  \npoly(months, 2, raw = TRUE)2  \n                     0.00100  \n\n\nExample: As of this writing (April 2023), there have been 156 months since Kaggle was established. Use a quadratic regression model to predict how many users are on Kaggle.\n\npredict(d2_model, newdata = data.frame(months = 156))\n\n       1 \n11.66495"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#data-visualization-1",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#data-visualization-1",
    "title": "30: Regression Analysis",
    "section": "Data Visualization",
    "text": "Data Visualization\nIn ggplot2, the geom_smooth layer can quickly graph the polynomial regression model.\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\nsubtitle_string = \"<span style='color:#FF0000'>red: <b>regression model</b></span>, \n<span style='color:#0000FF'>blue: <b>true data</b></span>, quadratic regression\"\n\ndf |>\n  ggplot(aes(x = months, y = users)) +\n  geom_point(color = \"blue\", size = 4) +\n  geom_smooth(color = \"red\", \n              formula = y ~ poly(x,2, raw = TRUE),\n              method = \"lm\", \n              se = FALSE) +\n  labs(title = \"Kaggle Community\",\n       subtitle = subtitle_string,\n       caption = \"Math 32\",\n       x = \"months since April 2010 founding\",\n       y = \"number of users (in millions)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\",\n                                  hjust = 0.5,\n                                  size = 20),\n        plot.subtitle = element_markdown(hjust = 0.5,\n                                         size = 15),\n        plot.caption = element_text(size = 10))"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#cubic-regression",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#cubic-regression",
    "title": "30: Regression Analysis",
    "section": "Cubic Regression",
    "text": "Cubic Regression\n\\[\\hat{y} = a + bx + cx^{2} + dx^{3}\\]\nBy default, R uses orthogonal polynomials in model creation. If we would rather gather easy-to-interpret coefficients in the model form above, use raw = TRUE.\n\nd3_model <- lm(users ~ poly(months, 3, raw = TRUE), data = df)\n\n\nd3_model\n\n\nCall:\nlm(formula = users ~ poly(months, 3, raw = TRUE), data = df)\n\nCoefficients:\n                 (Intercept)  poly(months, 3, raw = TRUE)1  \n                   1.360e-03                     1.485e-02  \npoly(months, 3, raw = TRUE)2  poly(months, 3, raw = TRUE)3  \n                  -6.207e-04                     6.691e-06  \n\n\nExample: As of this writing (April 2023), there have been 156 months since Kaggle was established. Use a cubic regression model to predict how many users are on Kaggle.\n\npredict(d3_model, newdata = data.frame(months = 156))\n\n       1 \n12.61522"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#data-visualization-2",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#data-visualization-2",
    "title": "30: Regression Analysis",
    "section": "Data Visualization",
    "text": "Data Visualization\nIn ggplot2, the geom_smooth layer can quickly graph the polynomial regression model.\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\nsubtitle_string = \"<span style='color:#FF0000'>red: <b>regression model</b></span>, \n<span style='color:#0000FF'>blue: <b>true data</b></span>, cubic regression\"\n\ndf |>\n  ggplot(aes(x = months, y = users)) +\n  geom_point(color = \"blue\", size = 5) +\n  geom_smooth(color = \"red\", \n              formula = y ~ poly(x,3, raw = TRUE),\n              method = \"lm\", \n              se = FALSE) +\n  labs(title = \"Kaggle Community\",\n       subtitle = subtitle_string,\n       caption = \"Math 32\",\n       x = \"months since April 2010 founding\",\n       y = \"number of users (in millions)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\",\n                                  hjust = 0.5,\n                                  size = 20),\n        plot.subtitle = element_markdown(hjust = 0.5,\n                                         size = 15),\n        plot.caption = element_text(size = 10))"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#analysis-of-variance",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#analysis-of-variance",
    "title": "30: Regression Analysis",
    "section": "Analysis of Variance",
    "text": "Analysis of Variance\n\ndenote \\(y_{i}\\) for true outcomes\ndenote \\(\\hat{y}_{i}\\) for estimates (or predictions)\nthen \\(y_{i} - \\hat{y}_{i}\\) is the \\(i^{\\text{th}}\\) residual\n\n\\[SS_{\\text{res}} = \\displaystyle\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}\\]\n\\[SS_{\\text{tot}} = \\displaystyle\\sum_{i=1}^{n}(y_{i} - \\bar{y_{i}})^{2}\\]"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#variation",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#variation",
    "title": "30: Regression Analysis",
    "section": "Variation",
    "text": "Variation\n\\[R^{2} = 1 - \\displaystyle\\frac{SS_{\\text{res}}}{SS_{\\text{tot}}} = \\displaystyle\\frac{\\text{explained variation}}{\\text{total variation}}\\]\n\nfor linear regression, \\(R^{2}\\) is the square of correlation\nrange of \\(R^{2}\\) is \\([0,1]\\)\nhigher \\(R^{2}\\) implies better model"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#adjusted-coefficient-of-determination",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#adjusted-coefficient-of-determination",
    "title": "30: Regression Analysis",
    "section": "Adjusted Coefficient of Determination",
    "text": "Adjusted Coefficient of Determination\nTo later mitigate issues such as the curse of dimensionality in more complex models, statisticians recommend use of an adjusted \\(R^{2}\\) such as\n\\[R^{2} = 1 - \\displaystyle\\frac{SS_{\\text{res}}}{SS_{\\text{tot}}} \\cdot \\displaystyle\\frac{df_{\\text{tot}}}{df_{\\text{res}}}\\]\n\nsummary(linear_model)\n\n\nCall:\nlm(formula = users ~ months, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2938 -1.6442 -0.1355  1.5715  2.6750 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -2.38965    1.78199  -1.341  0.21278   \nmonths       0.06609    0.01503   4.398  0.00172 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.97 on 9 degrees of freedom\nMultiple R-squared:  0.6825,    Adjusted R-squared:  0.6472 \nF-statistic: 19.35 on 1 and 9 DF,  p-value: 0.001724"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#exponential-regression",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#exponential-regression",
    "title": "30: Regression Analysis",
    "section": "Exponential Regression",
    "text": "Exponential Regression\n\\[\\hat{y} = a*b^{x}\\]\n\n# shouldn't compute ln 0\ndf$users[1] <- 0.1\n\nexp_model <- lm(log(users) ~ months, data = df)\n\n\n# coefficients of ln a and ln b\nexp_model\n\n\nCall:\nlm(formula = log(users) ~ months, data = df)\n\nCoefficients:\n(Intercept)       months  \n   -2.44344      0.03226  \n\n\nExample: As of this writing (April 2023), there have been 156 months since Kaggle was established. Use an exponential regression model to predict how many users are on Kaggle.\n\na <- exp(summary(exp_model)$coefficients[1])\nb <- exp(summary(exp_model)$coefficients[2])\na*b^{156}\n\n[1] 13.31783"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#data-visualization-3",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#data-visualization-3",
    "title": "30: Regression Analysis",
    "section": "Data Visualization",
    "text": "Data Visualization\nIn ggplot2, the geom_function layer can quickly graph a custom function.\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\nsubtitle_string = \"<span style='color:#FF0000'>red: <b>regression model</b></span>, \n<span style='color:#0000FF'>blue: <b>true data</b></span>, exponential regression\"\n\nf <- function(x) { a*b^x }\n\ndf |>\n  ggplot(aes(x = months, y = users)) +\n  geom_point(color = \"blue\", size = 5) +\n  geom_function(fun = f, color = \"red\", linewidth = 2) +\n  labs(title = \"Kaggle Community\",\n       subtitle = subtitle_string,\n       caption = \"Math 32\",\n       x = \"months since April 2010 founding\",\n       y = \"number of users (in millions)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\",\n                                  hjust = 0.5,\n                                  size = 20),\n        plot.subtitle = element_markdown(hjust = 0.5,\n                                         size = 15),\n        plot.caption = element_text(size = 10))"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#model-selection",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#model-selection",
    "title": "30: Regression Analysis",
    "section": "Model Selection",
    "text": "Model Selection\nLet us use the adjusted \\(R^{2}\\) values to judge our models.\n\nsummary(linear_model)$adj.r.squared\n\n[1] 0.6472128\n\n\n\nsummary(d2_model)$adj.r.squared\n\n[1] 0.9923513\n\n\n\nsummary(d3_model)$adj.r.squared\n\n[1] 0.9994897\n\n\n\nsummary(exp_model)$adj.r.squared\n\n[1] 0.9899031"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#what-happened",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#what-happened",
    "title": "30: Regression Analysis",
    "section": "What Happened",
    "text": "What Happened\nOn April 17, 2023, Kaggle surpassed 13 million users!\n\n\n\nsource\n\n\ntweet source\n\npredict(d3_model, newdata = data.frame(months = 156))\n\n       1 \n12.61522"
  },
  {
    "objectID": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#looking-ahead",
    "href": "posts/30_-_Regression Analysis/Math_32_30_Regression_Analysis.html#looking-ahead",
    "title": "30: Regression Analysis",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\nWHW10\nLHW9\n\nFinal Exam will be on May 6\n\nmore information in weekly announcement\n\n\n\n\n\nEven Kaggle makes mistakes!\n\n\ntweet source"
  },
  {
    "objectID": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#confidence-intervals-concept",
    "href": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#confidence-intervals-concept",
    "title": "Confidence Intervals",
    "section": "31: Confidence Intervals (Concept)",
    "text": "31: Confidence Intervals (Concept)\n\nSource: Statistical Inference via Data Science: A Modern Dive into R and the Tidyverse\nChapter 8: Bootstrapping and Confidence Intervals\nhttps://moderndive.com/8-confidence-intervals.html"
  },
  {
    "objectID": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#setting-pennies",
    "href": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#setting-pennies",
    "title": "Confidence Intervals",
    "section": "Setting: Pennies",
    "text": "Setting: Pennies\nAmong pennies in circulation in 2019, what was the average year of minting? We have a sample size of 50 pennies.\n\n# looking at the data set\npennies_sample\n\n# A tibble: 50 × 2\n      ID  year\n   <int> <dbl>\n 1     1  2002\n 2     2  1986\n 3     3  2017\n 4     4  1988\n 5     5  2008\n 6     6  1983\n 7     7  2008\n 8     8  1996\n 9     9  2004\n10    10  2000\n# ℹ 40 more rows"
  },
  {
    "objectID": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#sample-distribution",
    "href": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#sample-distribution",
    "title": "Confidence Intervals",
    "section": "Sample Distribution",
    "text": "Sample Distribution\n\n# visualizing the the pennies\npennies_sample %>%\n  ggplot(aes(x = year)) +\n  geom_dotplot(binwidth = 1, color = \"tan3\", fill = \"tan4\") +\n  labs(title = \"Pennies Sample\",\n       subtitle = \"observed in 2019\",\n       caption = \"Source: Modern Dive\",\n       x = \"year\",\n       y = \"proportion\")\n\n\n\n\n\n# visualizing the distribution of the pennies\np1 <- pennies_sample %>%\n  ggplot(aes(x = year)) +\n  geom_histogram(binwidth = 10, color = \"tan3\", fill = \"tan4\") +\n  labs(title = \"Pennies Sample\",\n       subtitle = \"observed in 2019\",\n       caption = \"Source: Modern Dive\")\n\n# display graph (in addition to storing the graph in a variable)\np1\n\n\n\n\n\n# sample mean\npennies_sample %>% summarize(xbar = mean(year))\n\n# A tibble: 1 × 1\n   xbar\n  <dbl>\n1 1995."
  },
  {
    "objectID": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#resampling",
    "href": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#resampling",
    "title": "Confidence Intervals",
    "section": "Resampling",
    "text": "Resampling\nUsing the available sample of data to fabricate another sample is called resampling."
  },
  {
    "objectID": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#resampling-once",
    "href": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#resampling-once",
    "title": "Confidence Intervals",
    "section": "Resampling Once",
    "text": "Resampling Once\nSuppose that we took the 50 pennies and resampled once while sampling with replacement.\n\npennies_resampled_once <- pennies_sample %>%\n  sample_n(size = 50, replace = TRUE)\n\n\n# visualizing the distribution of the pennies\np2 <- pennies_resampled_once %>%\n  ggplot(aes(x = year)) +\n  geom_histogram(binwidth = 10, color = \"tan3\", fill = \"tan4\") +\n  labs(title = \"Pennies Resampled Once\",\n       subtitle = \"sampled with replacement\",\n       caption = \"Source: Modern Dive\")\n\n# (using `patchwork` package to arrange plots side-by-side\np1 + p2\n\n\n\n\n\n# a different sample mean\npennies_resampled_once %>% summarize(xbar = mean(year))\n\n# A tibble: 1 × 1\n   xbar\n  <dbl>\n1 1995."
  },
  {
    "objectID": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#resampled-many-times",
    "href": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#resampled-many-times",
    "title": "Confidence Intervals",
    "section": "Resampled Many Times",
    "text": "Resampled Many Times\nSuppose now that we have each person in a 30-student discussion section repeat the act of drawing those 50 pennies with replacement.\n\npennies_resampled_many <- pennies_sample %>%\n  rep_sample_n(size = 50, replace = TRUE, reps = 30)\n\nNow we have each virtual student report their mean year.\n\npennies_resampled_many %>%\n  group_by(replicate) %>%\n  summarize(mean_year = mean(year))\n\n# A tibble: 30 × 2\n   replicate mean_year\n       <int>     <dbl>\n 1         1     1998.\n 2         2     1991.\n 3         3     1996.\n 4         4     1991.\n 5         5     1998.\n 6         6     1995.\n 7         7     1993.\n 8         8     1994.\n 9         9     1997.\n10        10     1995.\n# ℹ 20 more rows\n\n\n\nsummary(pennies_sample$year)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1962    1983    1996    1995    2008    2018 \n\n\n\npennies_resampled_many %>%\n  group_by(replicate) %>%\n  mutate(mean_year = mean(year)) %>%\n  ungroup() %>%\n  select(replicate, mean_year) %>%\n  distinct() %>%\n  ggplot(aes(x = mean_year)) +\n  geom_histogram(binwidth = 1, color = \"tan3\", fill = \"tan4\") +\n  labs(title = \"Resampling Results\",\n       subtitle = \"N = 30 resamples\",\n       caption = \"Source: Modern Dive\")\n\n\n\n\nOut of curiosity, let us push this process to \\(N = 1337\\) resamples.\n\npennies_resampled_means <- pennies_sample %>%\n  rep_sample_n(size = 50, replace = TRUE, reps = 1337) %>%\n  group_by(replicate) %>%\n  mutate(mean_year = mean(year)) %>%\n  ungroup() %>%\n  select(replicate, mean_year) %>%\n  distinct() \n\npennies_resampled_means %>%\n  ggplot(aes(x = mean_year)) +\n  geom_histogram(binwidth = 1, color = \"tan3\", fill = \"tan4\") +\n  labs(title = \"Resampling Results\",\n       subtitle = \"N = 1337 resamples\",\n       caption = \"Source: Modern Dive\")"
  },
  {
    "objectID": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#toward-confidence-intervals",
    "href": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#toward-confidence-intervals",
    "title": "Confidence Intervals",
    "section": "Toward Confidence Intervals",
    "text": "Toward Confidence Intervals\nThe standard deviation of a sampling distribution is called the standard error.\n\nxbar <- mean(pennies_resampled_means$mean_year)\nSE   <- sd(pennies_resampled_means$mean_year)\n\nWe can build a 95% confidence interval by computing \\(\\bar{x} \\pm 1.96*SE\\)\n\nc(xbar - 1.96*SE, xbar + 1.96*SE)\n\n[1] 1991.377 1999.624\n\n\n\npennies_resampled_means %>%\n  ggplot(aes(x = mean_year)) +\n  geom_histogram(binwidth = 1, color = \"tan3\", fill = \"tan4\") +\n  geom_vline(xintercept = c(xbar - 1.96*SE, xbar + 1.96*SE), color = \"yellow\", linewidth = 2) +\n  labs(title = \"Resampling Results\",\n       subtitle = \"N = 1337 resamples\",\n       caption = \"Source: Modern Dive\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#using-the-infer-package",
    "href": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#using-the-infer-package",
    "title": "Confidence Intervals",
    "section": "Using the infer package",
    "text": "Using the infer package\n\npennies_sample %>%\n  specify(response = year)\n\nResponse: year (numeric)\n# A tibble: 50 × 1\n    year\n   <dbl>\n 1  2002\n 2  1986\n 3  2017\n 4  1988\n 5  2008\n 6  1983\n 7  2008\n 8  1996\n 9  2004\n10  2000\n# ℹ 40 more rows\n\n\n\npennies_sample %>%\n  specify(response = year) %>%\n  calculate(stat = \"mean\")\n\nResponse: year (numeric)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1 1995.\n\n\n\npennies_sample %>%\n  specify(response = year) %>%\n  generate(reps = 1337, type = \"bootstrap\")\n\nResponse: year (numeric)\n# A tibble: 66,850 × 2\n# Groups:   replicate [1,337]\n   replicate  year\n       <int> <dbl>\n 1         1  1983\n 2         1  1992\n 3         1  2015\n 4         1  2018\n 5         1  1997\n 6         1  1988\n 7         1  2017\n 8         1  1976\n 9         1  1985\n10         1  2015\n# ℹ 66,840 more rows\n\n\n\nbootstrap_distribution <- pennies_sample %>%\n  specify(response = year) %>%\n  generate(reps = 1337, type = \"bootstrap\") %>%\n  calculate(stat = \"mean\")\n\n# print\nbootstrap_distribution\n\nResponse: year (numeric)\n# A tibble: 1,337 × 2\n   replicate  stat\n       <int> <dbl>\n 1         1 1998.\n 2         2 1994.\n 3         3 1990.\n 4         4 1998.\n 5         5 1996.\n 6         6 1996.\n 7         7 1998.\n 8         8 1996.\n 9         9 1998.\n10        10 1993.\n# ℹ 1,327 more rows"
  },
  {
    "objectID": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#bootstrap-distribution",
    "href": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#bootstrap-distribution",
    "title": "Confidence Intervals",
    "section": "Bootstrap Distribution",
    "text": "Bootstrap Distribution\nThe resulting distribution from sampling without replacement is called a bootstrap distribution\n\nvisualise(bootstrap_distribution)"
  },
  {
    "objectID": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#infer-get_ci",
    "href": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#infer-get_ci",
    "title": "Confidence Intervals",
    "section": "Infer get_ci()",
    "text": "Infer get_ci()\nThere are also wrappers in the infer package to extract the confidence interval\n\nbootstrap_distribution %>%\n  get_confidence_interval(point_estimate = mean(bootstrap_distribution$stat), \n                          level = 0.95, type = \"se\")\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1    1991.    2000.\n\n\nAlternatively, we can use percentiles to build our confidence intervals. This is useful when the data is not normally distributed.\n\nbootstrap_distribution %>%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1    1991.    1999.\n\n\n\nSE_CI <- bootstrap_distribution %>%\n  get_ci(point_estimate = mean(bootstrap_distribution$stat),\n         level = 0.95, type = \"se\")\n\nvisualize(bootstrap_distribution) +\n  shade_ci(endpoints = SE_CI, color = \"#DAA900\", fill = \"#002856\")"
  },
  {
    "objectID": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#inference",
    "href": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#inference",
    "title": "Confidence Intervals",
    "section": "Inference",
    "text": "Inference\nHow do we describe confidence intervals?"
  },
  {
    "objectID": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#example-bowl-of-marbles",
    "href": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#example-bowl-of-marbles",
    "title": "Confidence Intervals",
    "section": "Example: Bowl of Marbles",
    "text": "Example: Bowl of Marbles\nThe bowl data was literally a classroom bowl of red and white marbles\n\nbowl %>%\n  ggplot(aes(x = color, fill = color)) +\n  geom_bar(stat = \"count\", color = \"black\") +\n  scale_fill_manual(values = c(\"red\", \"white\")) +\n  labs(title = \"Bowl of Marbles\",\n       subtitle = \"population is known\",\n       caption = \"Source: Modern Dive\")\n\n\n\n\nwhere we know the true proportion of red marbles.\n\nbowl %>%\n  summarize(proportion_red = mean(color == \"red\"))\n\n# A tibble: 1 × 1\n  proportion_red\n           <dbl>\n1          0.375"
  },
  {
    "objectID": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#simulations",
    "href": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#simulations",
    "title": "Confidence Intervals",
    "section": "Simulations",
    "text": "Simulations\n\nCI_simulation <- function(confidence = 95, sample_size = 25, num_intervals = 10){\n  # Constants\n  alpha <- 1 - confidence/100\n  n <- sample_size\n  N <- num_intervals\n  proportion_red <- 0.375 #true population proportion\n  \n  # vector allocation\n  left <- rep(NA, N)\n  right <- rep(NA, N)\n  captured <- rep(NA, N)\n  \n  for(i in 1:N){\n    this_sample <- sample(bowl$color, n, replace = TRUE)\n    phat <- mean(this_sample == \"red\") #sample proportion\n    \n    #margin of error\n    E <- qnorm(1 - alpha/2)*sqrt( phat*(1-phat)/n)\n    \n    #this confidence interval\n    left[i] <- phat - E\n    right[i] <- phat + E\n    \n    #did the confidence interval capture the true proportion?\n    captured[i] <- ifelse(left[i] <= proportion_red & right[i] >= proportion_red, TRUE, FALSE)\n  }\n  \n  # graph\n  df <- data.frame(left, right, captured)\n  ggplot(df, aes(x = left, y = 1:N)) +\n    geom_vline(xintercept = proportion_red, color = \"black\") +\n    geom_segment(aes(x = left, y = 1:N, \n                     xend = right, yend = 1:N,\n                     color = captured)) +\n    labs(title = \"Simulation of bowl samples\",\n         subtitle = paste0(\"alpha = \", alpha, \", n = \", n),\n         caption = \"Bio 175\", \n         x = \"proportion red\",\n         y = \"iteration\") +\n    theme_minimal()\n}\n\n\nCI_simulation(confidence = 95, sample_size = 25, num_intervals = 100)\n\n\n\n\n\nSignificance LevelsSample Sizes\n\n\n\np1 <- CI_simulation(80, 25, 100) + theme(legend.position = \"none\")\np2 <- CI_simulation(95, 25, 100) + theme(legend.position = \"none\")\np3 <- CI_simulation(99, 25, 100) + theme(legend.position = \"none\")\n\np1 + p2 + p3\n\n\n\n\nAs we request more confidence, the confidence intervals are more likely to include the true population parameter.\n\n\n\np4 <- CI_simulation(95, 25, 100) + theme(legend.position = \"none\")\np5 <- CI_simulation(95, 100, 100) + theme(legend.position = \"none\")\np6 <- CI_simulation(95, 400, 100) + theme(legend.position = \"none\")\n\np4 + p5 + p6\n\n\n\n\nAs we use larger sample sizes, the confidence intervals are more likely to include the true population parameter."
  },
  {
    "objectID": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#looking-ahead",
    "href": "posts/31_-_Confidence Intervals_-_concept/Math_32_-_31_-_Confidence_Intervals_concept.html#looking-ahead",
    "title": "Confidence Intervals",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\nWHW10 (due today)\nWHW11\nLHW9\nLHW10\n\nFinal Exam will be on May 6\n\nmore information in weekly announcement\n\n\n\ntweet source"
  },
  {
    "objectID": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#introduction-to-machine-learning",
    "href": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#introduction-to-machine-learning",
    "title": "34: Introduction to Machine Learning",
    "section": "34: Introduction to Machine Learning",
    "text": "34: Introduction to Machine Learning\nGoal: introduce machine learning (ideas and terminology)\nObjectives:\n\nintroduce tidymodels package\npractice with a TidyTuesday data set"
  },
  {
    "objectID": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#data-tour-de-france",
    "href": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#data-tour-de-france",
    "title": "34: Introduction to Machine Learning",
    "section": "Data: Tour de France",
    "text": "Data: Tour de France\nSource: TidyTuesday data set from April 7, 2020\n\ntdf_winners <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-04-07/tdf_winners.csv')\n\n\nstr(tdf_winners, give.attr = FALSE)\n\nspc_tbl_ [106 × 19] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ edition      : num [1:106] 1 2 3 4 5 6 7 8 9 10 ...\n $ start_date   : Date[1:106], format: \"1903-07-01\" \"1904-07-02\" ...\n $ winner_name  : chr [1:106] \"Maurice Garin\" \"Henri Cornet\" \"Louis Trousselier\" \"René Pottier\" ...\n $ winner_team  : chr [1:106] \"La Française\" \"Conte\" \"Peugeot–Wolber\" \"Peugeot–Wolber\" ...\n $ distance     : num [1:106] 2428 2428 2994 4637 4488 ...\n $ time_overall : num [1:106] 94.6 96.1 NA NA NA ...\n $ time_margin  : num [1:106] 2.99 2.27 NA NA NA ...\n $ stage_wins   : num [1:106] 3 1 5 5 2 5 6 4 2 3 ...\n $ stages_led   : num [1:106] 6 3 10 12 5 13 13 3 13 13 ...\n $ height       : num [1:106] 1.62 NA NA NA NA NA 1.78 NA NA NA ...\n $ weight       : num [1:106] 60 NA NA NA NA NA 88 NA NA NA ...\n $ age          : num [1:106] 32 19 24 27 24 25 22 22 26 23 ...\n $ born         : Date[1:106], format: \"1871-03-03\" \"1884-08-04\" ...\n $ died         : Date[1:106], format: \"1957-02-19\" \"1941-03-18\" ...\n $ full_name    : chr [1:106] NA NA NA NA ...\n $ nickname     : chr [1:106] \"The Little Chimney-sweep\" \"Le rigolo (The joker)\" \"Levaloy / Trou-trou\" NA ...\n $ birth_town   : chr [1:106] \"Arvier\" \"Desvres\" \"Paris\" \"Moret-sur-Loing\" ...\n $ birth_country: chr [1:106] \"Italy\" \"France\" \"France\" \"France\" ...\n $ nationality  : chr [1:106] \" France\" \" France\" \" France\" \" France\" ...\n\n\n\ncolnames(tdf_winners)\n\n [1] \"edition\"       \"start_date\"    \"winner_name\"   \"winner_team\"  \n [5] \"distance\"      \"time_overall\"  \"time_margin\"   \"stage_wins\"   \n [9] \"stages_led\"    \"height\"        \"weight\"        \"age\"          \n[13] \"born\"          \"died\"          \"full_name\"     \"nickname\"     \n[17] \"birth_town\"    \"birth_country\" \"nationality\""
  },
  {
    "objectID": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#early-look",
    "href": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#early-look",
    "title": "34: Introduction to Machine Learning",
    "section": "Early Look",
    "text": "Early Look\n\ntdf_winners %>%\n  ggplot(aes(x = height, y = time_overall)) +\n  geom_point(color = \"blue\") +\n  labs(title = \"Are taller bicyclists faster?\",\n       subtitle = \"featuring Tour de France winners\",\n       caption = \"Source: TidyTuesday\",\n       x = \"height (meters)\",\n       y = \"time (hours)\") +\n  theme_minimal()\n\nWarning: Removed 41 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#cleaning-data",
    "href": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#cleaning-data",
    "title": "34: Introduction to Machine Learning",
    "section": "Cleaning Data",
    "text": "Cleaning Data\nSometimes we like to perform some preprocessing of the data. In this example, we will\n\nfocus on the champions that were more athletic than in the early years.\nfocus on biker pace (response variable) as the route changes from year to year\n\n\ndf <- tdf_winners %>%\n  select(c(distance, time_overall, \n           height, weight, age)) %>%\n  filter(complete.cases(.)) %>%\n  filter(height >= 1.7) %>%\n  mutate(pace = distance / time_overall) %>%\n  select(c(pace, height, weight, age))\n\n# dimensions\ndim(df)\n\n[1] 62  4\n\n\n\nhead(df)\n\n# A tibble: 6 × 4\n   pace height weight   age\n  <dbl>  <dbl>  <dbl> <dbl>\n1  31.6   1.72     66    23\n2  33.4   1.72     66    33\n3  32.1   1.77     68    29\n4  32.2   1.77     68    30\n5  34.6   1.79     75    26\n6  33.2   1.79     75    29"
  },
  {
    "objectID": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#multiple-predictor-variables",
    "href": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#multiple-predictor-variables",
    "title": "34: Introduction to Machine Learning",
    "section": "Multiple Predictor Variables",
    "text": "Multiple Predictor Variables\n\nHeightAgeWeightR code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf %>%\n  ggplot(aes(x = height, y = pace)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_smooth(method = \"lm\", linewidth = 3,\n              se = FALSE, color = \"red\") +\n  labs(title = \"Are taller bicyclists faster?\",\n       subtitle = \"featuring Tour de France winners\",\n       caption = \"Source: TidyTuesday\",\n       x = \"height (meters)\",\n       y = \"pace (km/hr)\") +\n  theme_minimal()\n\ndf %>%\n  ggplot(aes(x = age, y = pace)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_smooth(method = \"lm\", linewidth = 3,\n              se = FALSE, color = \"red\") +\n  labs(title = \"Are older bicyclists faster?\",\n       subtitle = \"featuring Tour de France winners\",\n       caption = \"Source: TidyTuesday\",\n       x = \"age\",\n       y = \"pace (km/hr)\") +\n  theme_minimal()\n\ndf %>%\n  ggplot(aes(x = weight, y = pace)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_smooth(method = \"lm\", linewidth = 3,\n              se = FALSE, color = \"red\") +\n  labs(title = \"Are heavier bicyclists faster?\",\n       subtitle = \"featuring Tour de France winners\",\n       caption = \"Source: TidyTuesday\",\n       x = \"weight (kg)\",\n       y = \"pace (km/hr)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#regression-via-tidymodels",
    "href": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#regression-via-tidymodels",
    "title": "34: Introduction to Machine Learning",
    "section": "Regression via TidyModels",
    "text": "Regression via TidyModels\n\nStartModel EngineFitting a ModelExamining a Model\n\n\n“With tidymodels, we start by specifying the functional form of the model that we want using the parsnip package.”\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n“However, now that the type of model has been specified, a method for fitting or training the model can be stated using the engine. The engine value is often a mash-up of the software that can be used to fit or train the model as well as the estimation method.”\n\nlinear_reg() %>% \n  set_engine(\"lm\") #linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n\nlm_fit <- linear_reg() %>% \n  set_engine(\"lm\") %>%\n  fit(pace ~ height + weight + age, data = df)\nlm_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = pace ~ height + weight + age, data = data)\n\nCoefficients:\n(Intercept)       height       weight          age  \n     3.8455      21.0987      -0.1387       0.2113  \n\n\n\n\n\ntidy(lm_fit)\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    3.85    12.3        0.313  0.755 \n2 height        21.1      8.06       2.62   0.0112\n3 weight        -0.139    0.0685    -2.03   0.0474\n4 age            0.211    0.0979     2.16   0.0350\n\n\nObserve where we have p-values < 0.05"
  },
  {
    "objectID": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#interaction-terms",
    "href": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#interaction-terms",
    "title": "34: Introduction to Machine Learning",
    "section": "Interaction Terms",
    "text": "Interaction Terms\n\nlm_fit_with_interaction <- linear_reg() %>% \n  set_engine(\"lm\") %>%\n  fit(pace ~ height + weight + age + height:weight + height:age + weight:age +\n        height:weight:age,\n      data = df)\nlm_fit_with_interaction\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = pace ~ height + weight + age + height:weight + \n    height:age + weight:age + height:weight:age, data = data)\n\nCoefficients:\n      (Intercept)             height             weight                age  \n         924.8499          -444.1560           -15.6339           -27.8628  \n    height:weight         height:age         weight:age  height:weight:age  \n           7.9297            13.9352             0.4802            -0.2425  \n\n\n\ntidy(lm_fit_with_interaction)\n\n# A tibble: 8 × 5\n  term              estimate std.error statistic p.value\n  <chr>                <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        925.     2272.        0.407   0.686\n2 height            -444.     1287.       -0.345   0.731\n3 weight             -15.6      32.8      -0.477   0.635\n4 age                -27.9      80.3      -0.347   0.730\n5 height:weight        7.93     18.5       0.428   0.670\n6 height:age          13.9      45.5       0.306   0.761\n7 weight:age           0.480     1.16      0.414   0.680\n8 height:weight:age   -0.243     0.656    -0.370   0.713\n\n\nThis may be foreshadowing of overfitting."
  },
  {
    "objectID": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#prediction",
    "href": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#prediction",
    "title": "34: Introduction to Machine Learning",
    "section": "Prediction",
    "text": "Prediction\n\nNew DataPredictionsConfidence IntervalsError BarsPlot\n\n\n\n\n\nSpongeBob is a 26-year-old, 1.77 m tall bicyclist who weighs 55 kg\nPatrick is a 25-year-old, 1.81 m tall bicyclist who weighs 75 kg\nSquidward is a 31-year-old, 1.89 m tall bicyclist who weighs 65 kg\n\n\n\n\n\n\n\n\nnew_contestants <- data.frame(name = c(\"SpongeBob\", \"Patrick\", \"Squidward\"),\n                              age = c(26, 25, 31),\n                              height = c(1.77, 1.81, 1.89),\n                              weight = c(55, 75, 65))\nmean_predictions <- predict(lm_fit, new_data = new_contestants)\nmean_predictions\n\n# A tibble: 3 × 1\n  .pred\n  <dbl>\n1  39.1\n2  36.9\n3  41.3\n\n\n\n\n\nCI_predictions <- predict(lm_fit,\n                          new_data = new_contestants,\n                          type = \"conf_int\")\nCI_predictions\n\n# A tibble: 3 × 2\n  .pred_lower .pred_upper\n        <dbl>       <dbl>\n1        37.1        41.0\n2        35.9        38.0\n3        39.0        43.5\n\n\n\n\n\ndf_for_plot <- new_contestants %>%\n  bind_cols(mean_predictions) %>%\n  bind_cols(CI_predictions)\ndf_for_plot\n\n       name age height weight    .pred .pred_lower .pred_upper\n1 SpongeBob  26   1.77     55 39.05386    37.07966    41.02807\n2   Patrick  25   1.81     75 36.91179    35.85758    37.96601\n3 Squidward  31   1.89     65 41.25491    38.97189    43.53794\n\n\n\n\n\ndf_for_plot %>%\n  ggplot(aes(x = name)) +\n  geom_errorbar(aes(ymin = .pred_lower,\n                    ymax = .pred_upper),\n                color = \"red\",\n                width = 0.32) +\n  geom_point(aes(y = .pred), color = \"blue\", size = 5) +\n  labs(title = \"Tour de Under the Sea\",\n       subtitle = \"Welcome the new contestants!\",\n       caption = \"Math 32\",\n       x = \"\",\n       y = \"pace (km/hr)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#data-splitting",
    "href": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#data-splitting",
    "title": "34: Introduction to Machine Learning",
    "section": "Data Splitting",
    "text": "Data Splitting\n\nSplitOne SplitMany SplitsR code\n\n\n\ndata_split <- initial_split(df)\ntrain_df <- training(data_split)\ntest_df <- testing(data_split)\n\nprint(paste(\"The number of observations in the training set is:\", \n            nrow(train_df)))\n\n[1] \"The number of observations in the training set is: 46\"\n\nprint(paste(\"The number of observations in the testing set is:\", \n            nrow(test_df)))\n\n[1] \"The number of observations in the testing set is: 16\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitle_string <- \"<span style='color:#000000'><b>Training Sets</b></span> <span style='color:#0000FF'>and</span> \n<span style='color:#FF0000'><b>Testing Sets</b></span>\"\n\nfor(i in 1:10){\n  \n  data_split <- initial_split(df)\n  train_df <- training(data_split)\n  test_df <- testing(data_split)\n  \n  this_plot <- train_df %>%\n    ggplot(aes(x = height, y = pace)) +\n    geom_point(aes(color = \"training set\"), \n               # color = \"black\"\n    ) +\n    geom_smooth(method = \"lm\",\n                aes(x = height, y = pace),\n                color = \"black\",\n                data = train_df,\n                formula = \"y ~ x\",\n              se = FALSE) +\n  geom_point(aes(x = height, y = pace, color = \"testing set\"),\n             # color = \"red\",\n             data = test_df,\n             size = 3) +\n  labs(title = title_string,\n       subtitle = \"approx 75-25 percent split\",\n       caption = \"Math 32\",\n       x = \"height (meters)\",\n       y = \"pace (km/hr)\") +\n  scale_color_manual(name = \"Data Split\",\n                     breaks = c(\"training set\", \"testing set\"),\n                     values = c(\"training set\" = \"black\",\n                                \"testing set\" = \"red\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_markdown(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n  \n  ggsave(paste0(\"images/plot\", i, \".png\"),\n         plot = this_plot,\n         device = \"png\")\n}"
  },
  {
    "objectID": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#metrics",
    "href": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#metrics",
    "title": "34: Introduction to Machine Learning",
    "section": "Metrics",
    "text": "Metrics\nWe then should get a sense of the validity of a model. One metric is mean square error of the test set.\n\\[\\text{MSE} = \\displaystyle\\frac{1}{n_{\\text{test}}}\\sum_{j = 1}^{n_{\\text{test}}} (y_{j} - \\hat{y}_{j})^{2}\\]\n\ndata_split <- initial_split(df)\ntrain_df <- training(data_split)\ntest_df <- testing(data_split)\n\nlm_train <- linear_reg() %>% \n  set_engine(\"lm\") %>%\n  fit(pace ~ height + weight + age, data = train_df)\n\nn_test <- nrow(test_df)\n\nMSE <- (1/n_test)*sum((\n  test_df$pace - \n    predict(lm_train, new_data = test_df |> select(-pace))\n)^2)\nMSE\n\n[1] 2.773913"
  },
  {
    "objectID": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#cross-validation",
    "href": "posts/34_-_Intro_Machine_Learning/Math_32_34_Intro_Machine_Learning.html#cross-validation",
    "title": "34: Introduction to Machine Learning",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nTo help generalize to a variety of testing sets, we can perform cross-validation by utilizing several training/testing set splits.\nWe can then compute the cross-validation error by computing the mean of the test metric.\n\nN <- 10\nMSE_vec <- rep(NA, N)\n\nfor(j in 1:N){\n  data_split <- initial_split(df)\n  train_df <- training(data_split)\n  test_df <- testing(data_split)\n  \n  lm_train <- linear_reg() %>% \n    set_engine(\"lm\") %>%\n    fit(pace ~ height + weight + age, data = train_df)\n  \n  n_test <- nrow(test_df)\n  \n  MSE_vec[j] <- (1/n_test)*sum((\n    test_df$pace - \n      predict(lm_train, new_data = test_df |> select(-pace))\n  )^2)\n}\n\n# vector of MSE\nMSE_vec\n\n [1]  6.721709  9.335262  5.487857 10.713953  3.355675  9.758514  4.483034\n [8] 12.320367  4.809474  6.830113\n\n\n\n# cross-validation error\ncv_error <- mean(MSE_vec)\ncv_error\n\n[1] 7.381596"
  },
  {
    "objectID": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html",
    "href": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html",
    "title": "35: Introduction to Machine Learning (2)",
    "section": "",
    "text": "Goal: overview of some machine learning techniques\nObjectives:\n\nintroduce random forests\nintroduce clustering\n\n\nknitr::opts_chunk$set(echo = TRUE)\n\nlibrary(\"caret\")\nlibrary(\"ggraph\")\nlibrary(\"igraph\")\nlibrary(\"patchwork\")\nlibrary(\"randomForest\")\nlibrary(\"skimr\")          #tools to quickly extract summary statistics\nlibrary(\"tidymodels\")\nlibrary(\"tidyverse\")\n\nset.seed(123) #actually needed to turn off some randomization"
  },
  {
    "objectID": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#data-eggs",
    "href": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#data-eggs",
    "title": "35: Introduction to Machine Learning (2)",
    "section": "Data: Eggs",
    "text": "Data: Eggs\nSource: TidyTuesday data set from April 11, 2023\nThe data this week comes from The Humane League’s US Egg Production dataset by Samara Mendez. Dataset and code is available for this project on OSF at US Egg Production Data Set. This dataset tracks the supply of cage-free eggs in the United States from December 2007 to February 2021.\n\negg_df_raw <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-11/egg-production.csv')\n\n\nstr(egg_df_raw, give.attr = FALSE)\n\nspc_tbl_ [220 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ observed_month: Date[1:220], format: \"2016-07-31\" \"2016-08-31\" ...\n $ prod_type     : chr [1:220] \"hatching eggs\" \"hatching eggs\" \"hatching eggs\" \"hatching eggs\" ...\n $ prod_process  : chr [1:220] \"all\" \"all\" \"all\" \"all\" ...\n $ n_hens        : num [1:220] 57975000 57595000 57161000 56857000 57116000 ...\n $ n_eggs        : num [1:220] 1.15e+09 1.14e+09 1.09e+09 1.13e+09 1.10e+09 ...\n $ source        : chr [1:220] \"ChicEggs-09-23-2016.pdf\" \"ChicEggs-10-21-2016.pdf\" \"ChicEggs-11-22-2016.pdf\" \"ChicEggs-12-23-2016.pdf\" ...\n\n\n\nskimr::skim(egg_df_raw)\n\n\nData summary\n\n\nName\negg_df_raw\n\n\nNumber of rows\n220\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nDate\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nprod_type\n0\n1\n10\n13\n0\n2\n0\n\n\nprod_process\n0\n1\n3\n23\n0\n3\n0\n\n\nsource\n0\n1\n23\n23\n0\n108\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nobserved_month\n0\n1\n2016-07-31\n2021-02-28\n2018-11-15\n56\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nn_hens\n0\n1\n110839873\n124121204\n13500000\n17284500\n59939500\n125539250\n341166000\n▇▁▁▁▂\n\n\nn_eggs\n0\n1\n2606667580\n3082457619\n298074240\n423962023\n1154550000\n2963010996\n8601000000\n▇▁▁▁▂\n\n\n\n\n\n\negg_df_raw %>%\n  ggplot(aes(x = n_hens, y = n_eggs)) +\n  geom_point(color = \"blue\") +\n  labs(title = \"US Egg Production\",\n       subtitle = \"December 2007 to February 2021\",\n       caption = \"Source: TidyTuesday\",\n       x = \"number of hens\",\n       y = \"number of eggs\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#cleaning-data",
    "href": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#cleaning-data",
    "title": "35: Introduction to Machine Learning (2)",
    "section": "Cleaning Data",
    "text": "Cleaning Data\nSometimes we like to perform some preprocessing of the data. In this example, we will\n\nfocus on smaller farms where the number of hens is under 100 million\nseparate the date into year, month, and day columns\nturn prod_process into a factor variable (helps with R stuff for categorical variables)\n\n\negg_df <- egg_df_raw |>\n  filter(n_hens < 1e8) |>\n  separate(observed_month,\n           into = c(\"year\", \"month\", \"day\"),\n           remove = FALSE)\n\negg_df$prod_process <- factor(egg_df$prod_process,\n                              levels = c(\"cage-free (organic)\",\n                                         \"cage-free (non-organic)\",\n                                         \"all\"))\n\n# dimensions\ndim(egg_df)\n\n[1] 165   9\n\n\n\nhead(egg_df)\n\n# A tibble: 6 × 9\n  observed_month year  month day   prod_type   prod_process n_hens n_eggs source\n  <date>         <chr> <chr> <chr> <chr>       <fct>         <dbl>  <dbl> <chr> \n1 2016-07-31     2016  07    31    hatching e… all          5.80e7 1.15e9 ChicE…\n2 2016-08-31     2016  08    31    hatching e… all          5.76e7 1.14e9 ChicE…\n3 2016-09-30     2016  09    30    hatching e… all          5.72e7 1.09e9 ChicE…\n4 2016-10-31     2016  10    31    hatching e… all          5.69e7 1.13e9 ChicE…\n5 2016-11-30     2016  11    30    hatching e… all          5.71e7 1.10e9 ChicE…\n6 2016-12-31     2016  12    31    hatching e… all          5.77e7 1.13e9 ChicE…\n\n\n\negg_df %>%\n  ggplot(aes(x = n_hens, y = n_eggs)) +\n  geom_point(aes(color = prod_process),\n             size = 3, alpha = 0.75) +\n  labs(title = \"US Egg Production\",\n       subtitle = \"December 2007 to February 2021\",\n       caption = \"Source: TidyTuesday\",\n       x = \"number of hens\",\n       y = \"number of eggs\") +\n  theme_minimal()\n\n\n\n\n\negg_df %>%\n  ggplot(aes(x = month, y = n_eggs)) +\n  geom_boxplot(aes(fill = month)) +\n  labs(title = \"US Egg Production\",\n       subtitle = \"December 2007 to February 2021\",\n       caption = \"Source: TidyTuesday\",\n       x = \"month\",\n       y = \"number of eggs\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#supervised-learning",
    "href": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#supervised-learning",
    "title": "35: Introduction to Machine Learning (2)",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nIn supervised learning has the goal of making predictions with a set of known labels for the response variable.\n\nGoal: predict the production type (e.g. cage-free) of each report of the egg data.\n\nresponse variable: prod_process\npredictor variables: n_hens, n_eggs, month, year, prod_type\nmodel formula: prod_process ~ n_hens + n_eggs + month + year + prod_type\n\n\negg_split <- initial_split(egg_df)\negg_train <- training(egg_split)\negg_test  <- testing(egg_split)"
  },
  {
    "objectID": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#random-forests",
    "href": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#random-forests",
    "title": "35: Introduction to Machine Learning (2)",
    "section": "Random Forests",
    "text": "Random Forests\n“Random forest models are ensembles of decision trees. A large number of decision tree models are created for the ensemble based on slightly different versions of the training set. When creating the individual decision trees, the fitting process encourages them to be as diverse as possible. The collection of trees are combined into the random forest model and, when a new sample is predicted, the votes from each tree are used to calculate the final predicted value for the new sample.” —tidymodels.org\n\nDefine the Forest\n\nrandom_forest_model <- \n  rand_forest(trees = 250) %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"classification\")\n\n\n\nFitting the Forest\n\nrandom_forest_fit <-\n  random_forest_model %>%\n  fit(prod_process ~ n_hens + n_eggs + month + year + prod_type, \n      data = egg_train)\n\nrandom_forest_fit\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~250,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  250 \nSample size:                      123 \nNumber of independent variables:  5 \nMtry:                             2 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.001225525"
  },
  {
    "objectID": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#visualizing-the-forest",
    "href": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#visualizing-the-forest",
    "title": "35: Introduction to Machine Learning (2)",
    "section": "Visualizing the Forest",
    "text": "Visualizing the Forest\n(This is an old-fashioned code using the caret package, and Derek really should revise his knowledge here.)\n\n# OLD-FASHIONED WAY with the caret package\nmodel_rf <- caret::train(prod_process ~ n_hens + n_eggs + month + year + prod_type,\n                         data = egg_train, \n                         method = \"rf\")\nmodel_rf\n\nRandom Forest \n\n123 samples\n  5 predictor\n  3 classes: 'cage-free (organic)', 'cage-free (non-organic)', 'all' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 123, 123, 123, 123, 123, 123, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy  Kappa    \n   2    0.99433   0.9914711\n  10    1.00000   1.0000000\n  19    1.00000   1.0000000\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 10.\n\n\n\nmodel_rpart <- caret::train(prod_process ~ n_hens + n_eggs + month + year + prod_type,\n                         data = egg_train, \n                         method = \"rpart\")\nmodel_rpart\n\nCART \n\n123 samples\n  5 predictor\n  3 classes: 'cage-free (organic)', 'cage-free (non-organic)', 'all' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 123, 123, 123, 123, 123, 123, ... \nResampling results across tuning parameters:\n\n  cp        Accuracy   Kappa    \n  0.000000  1.0000000  1.0000000\n  0.474359  0.7759533  0.6646848\n  0.525641  0.5132879  0.2852174\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.\n\n\n\n#source:  https://shiring.github.io/machine_learning/2017/03/16/rf_plot_ggraph\ntree_func <- function(final_model, \n                      tree_num) {\n  \n  # get tree by index\n  tree <- randomForest::getTree(final_model, \n                                k = tree_num, \n                                labelVar = TRUE) %>%\n    tibble::rownames_to_column() %>%\n    # make leaf split points to NA, so the 0s won't get plotted\n    mutate(`split point` = ifelse(is.na(prediction), `split point`, NA))\n  \n  # prepare data frame for graph\n  graph_frame <- data.frame(from = rep(tree$rowname, 2),\n                            to = c(tree$`left daughter`, tree$`right daughter`))\n  \n  # convert to graph and delete the last node that we don't want to plot\n  graph <- graph_from_data_frame(graph_frame) %>%\n    delete_vertices(\"0\")\n  \n  # set node labels\n  V(graph)$node_label <- gsub(\"_\", \" \", as.character(tree$`split var`))\n  V(graph)$leaf_label <- as.character(tree$prediction)\n  V(graph)$split <- as.character(round(tree$`split point`, digits = 2))\n  \n  # plot\n  plot <- ggraph(graph, 'dendrogram') + \n    theme_bw() +\n    geom_edge_link() +\n    geom_node_point() +\n    geom_node_text(aes(label = node_label), na.rm = TRUE, repel = TRUE) +\n    geom_node_label(aes(label = split), vjust = 2.5, na.rm = TRUE, fill = \"white\") +\n    geom_node_label(aes(label = leaf_label, fill = leaf_label), na.rm = TRUE, \n                    repel = TRUE, colour = \"white\", fontface = \"bold\", show.legend = FALSE) +\n    theme(panel.grid.minor = element_blank(),\n          panel.grid.major = element_blank(),\n          panel.background = element_blank(),\n          plot.background = element_rect(fill = \"white\"),\n          panel.border = element_blank(),\n          axis.line = element_blank(),\n          axis.text.x = element_blank(),\n          axis.text.y = element_blank(),\n          axis.ticks = element_blank(),\n          axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          plot.title = element_text(size = 18))\n  \n  print(plot)\n}\n\ntree_num <- which.min(model_rf$finalModel$forest$ndbigtree == min(model_rf$finalModel$forest$ndbigtree))\n\ntree_func(final_model = model_rf$finalModel, tree_num)"
  },
  {
    "objectID": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#classification-error",
    "href": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#classification-error",
    "title": "35: Introduction to Machine Learning (2)",
    "section": "Classification Error",
    "text": "Classification Error\n\negg_predictions <- predict(model_rf, newdata = egg_test)\ntable(egg_predictions)\n\negg_predictions\n    cage-free (organic) cage-free (non-organic)                     all \n                     12                      16                      14 \n\n\n\nplot_1 <- egg_test %>%\n  ggplot(aes(x = n_hens, y = n_eggs)) +\n  geom_point(aes(color = egg_predictions),\n             size = 3, alpha = 0.75) +\n  labs(title = \"US Egg Production\",\n       subtitle = \"predictions\",\n       caption = \"Source: TidyTuesday\",\n       x = \"number of hens\",\n       y = \"number of eggs\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nplot_2 <- egg_test %>%\n  ggplot(aes(x = n_hens, y = n_eggs)) +\n  geom_point(aes(color = prod_process),\n             size = 3, alpha = 0.75) +\n  labs(title = \"US Egg Production\",\n       subtitle = \"true classifications\",\n       caption = \"Source: TidyTuesday\",\n       x = \"number of hens\",\n       y = \"number of eggs\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n# patchwork\nplot_1 + plot_2\n\n\n\n\n\nconfusionMatrix(egg_predictions, egg_test$prod_process)\n\nConfusion Matrix and Statistics\n\n                         Reference\nPrediction                cage-free (organic) cage-free (non-organic) all\n  cage-free (organic)                      10                       2   0\n  cage-free (non-organic)                   0                      16   0\n  all                                       0                       0  14\n\nOverall Statistics\n                                          \n               Accuracy : 0.9524          \n                 95% CI : (0.8384, 0.9942)\n    No Information Rate : 0.4286          \n    P-Value [Acc > NIR] : 5.568e-13       \n                                          \n                  Kappa : 0.9276          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: cage-free (organic) Class: cage-free (non-organic)\nSensitivity                              1.0000                         0.8889\nSpecificity                              0.9375                         1.0000\nPos Pred Value                           0.8333                         1.0000\nNeg Pred Value                           1.0000                         0.9231\nPrevalence                               0.2381                         0.4286\nDetection Rate                           0.2381                         0.3810\nDetection Prevalence                     0.2857                         0.3810\nBalanced Accuracy                        0.9688                         0.9444\n                     Class: all\nSensitivity              1.0000\nSpecificity              1.0000\nPos Pred Value           1.0000\nNeg Pred Value           1.0000\nPrevalence               0.3333\nDetection Rate           0.3333\nDetection Prevalence     0.3333\nBalanced Accuracy        1.0000"
  },
  {
    "objectID": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#unsupervised-learning",
    "href": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#unsupervised-learning",
    "title": "35: Introduction to Machine Learning (2)",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nIn unsupervised learning, we try to find structure in the data of the response variable without predetermined labels.\n\nGoal: classify farms into groups by size"
  },
  {
    "objectID": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#k-means-clustering",
    "href": "posts/35_-_Machine_Learning/Math_32_35_Machine_Learning.html#k-means-clustering",
    "title": "35: Introduction to Machine Learning (2)",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\n\nNumerical Variables\n\negg_numerical <- egg_df |>\n  select(n_hens, n_eggs)\nhead(egg_numerical)\n\n# A tibble: 6 × 2\n    n_hens     n_eggs\n     <dbl>      <dbl>\n1 57975000 1147000000\n2 57595000 1142700000\n3 57161000 1093300000\n4 56857000 1126700000\n5 57116000 1096600000\n6 57750000 1132900000\n\n\n\negg_numerical %>%\n  ggplot(aes(x = n_hens, y = n_eggs)) +\n  geom_point(color = \"black\") +\n  labs(title = \"US Egg Production\",\n       subtitle = \"How can we organize this data?\",\n       caption = \"Source: TidyTuesday\",\n       x = \"number of hens\",\n       y = \"number of eggs\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nFirst Look\nAre there 3 types of chicken farms in the data?\n\nclusters3 <- kmeans(egg_numerical, 3)\n\negg_df_with_clusters <- cbind(egg_numerical, clusters3$cluster)\ncolnames(egg_df_with_clusters) <- c(\"n_hens\", \"n_eggs\", \"cluster\")\n\n# turn cluster from a numerical variable into a factor (categorical) variable\negg_df_with_clusters$cluster <- factor(egg_df_with_clusters$cluster)\n\n# show a sample of observations\negg_df_with_clusters |>\n  slice_sample(n = 10, replace = TRUE)\n\n     n_hens     n_eggs cluster\n1  14100000  298074240       2\n2  64171000 1212900000       3\n3  27300000  636840617       1\n4  13500000  304762114       2\n5  64955000 1590747994       3\n6  60202000 1120900000       3\n7  17491500  386912160       2\n8  35680000  826794977       1\n9  62135000 1204400000       3\n10 39083000  908679086       1\n\n\n\negg_df_with_clusters %>%\n  ggplot(aes(x = n_hens, y = n_eggs)) +\n  geom_point(aes(color = cluster),\n             size = 3, alpha = 0.7) +\n  labs(title = \"US Egg Production\",\n       subtitle = \"k = 3 clusters\",\n       caption = \"Source: TidyTuesday\",\n       x = \"number of hens\",\n       y = \"number of eggs\") +\n  theme_minimal()\n\n\n\n\n\n\nHow to select the number of clusters\n\negg_df_with_clusters %>%\n  ggplot(aes(x = n_hens, y = n_eggs)) +\n  geom_point(aes(color = cluster),\n             size = 3, alpha = 0.7) +\n  geom_point(aes(x = n_hens, y = n_eggs),\n             color = \"black\",\n             data = data.frame(clusters3$centers), \n             size = 5) +\n  labs(title = \"US Egg Production\",\n       subtitle = \"k = 3 clusters (with centers)\",\n       caption = \"Source: TidyTuesday\",\n       x = \"number of hens\",\n       y = \"number of eggs\") +\n  theme_minimal()\n\n\n\n\nWe start with an \\(d\\)-dimensional data set of numerical variables and prescribe a number \\(k\\) for the number of clusters and run the kmeans algorithm.\n\nEach cluster \\(C_k\\) has \\(n_k\\) points labeled \\(x_i\\) in \\(d\\)-dimensional space\nEach cluster has a cluster center \\(\\mu_k\\)\nEach cluster has a within-sum-of-squares\n\n\\[\\text{WSS} = \\displaystyle\\sum_{x_{i} \\in C_{k}} (x_i−\\mu_k)^{2}\\]\nThus, our metric for the clustering will be the reported total-within-sum-of-squares\n\\[\\text{TWSS} = \\displaystyle\\sum_{j=1}^{k}\\sum_{x_{i} \\in C_{k}} (x_i−\\mu_k)^{2}\\]\n\nas the number \\(k\\) of clusters increases, the TWSS decreases\nbut we generally do not want a large number of clusters for later interpretation\n\n\n\nScree Plot\n\nk_vals <- 1:9\ntwss <- rep(NA, 9)\n\nfor(k in k_vals){\n  this_clustering <- kmeans(egg_numerical, k)\n  twss[k] <- this_clustering$tot.withinss\n}\n\ndf_analysis <- data.frame(k_vals, twss)\n\ndf_analysis %>%\n  ggplot(aes(x = k_vals, y = twss)) +\n  geom_line() +\n  geom_point(size = 3) +\n  labs(title = \"Scree Plot\",\n       subtitle = \"How many clusters should we pick?\",\n       caption = \"Math 32\",\n       x = \"number of clusters\",\n       y = \"total within sum of squares\") +\n  scale_x_continuous(breaks = 1:9) +\n  theme_minimal()\n\n\n\n\n\nsome advise to pick the “elbow” in the scree plot (where the concavity is greatest)\nsome advise to pick the place where TWSS starts to barely improve\n\n\nclusters2 <- kmeans(egg_numerical, 2)\n\negg_df_with_clusters <- cbind(egg_numerical, clusters2$cluster)\ncolnames(egg_df_with_clusters) <- c(\"n_hens\", \"n_eggs\", \"cluster\")\n\n# turn cluster from a numerical variable into a factor (categorical) variable\negg_df_with_clusters$cluster <- factor(egg_df_with_clusters$cluster)\n\nplot_k2 <- egg_df_with_clusters %>%\n  ggplot(aes(x = n_hens, y = n_eggs)) +\n  geom_point(aes(color = cluster),\n             size = 3, alpha = 0.7) +\n  labs(title = \"US Egg Production\",\n       subtitle = \"k = 2 clusters\",\n       # caption = \"Source: TidyTuesday\",\n       x = \"number of hens\",\n       y = \"number of eggs\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\nclusters5 <- kmeans(egg_numerical, 5)\n\negg_df_with_clusters <- cbind(egg_numerical, clusters5$cluster)\ncolnames(egg_df_with_clusters) <- c(\"n_hens\", \"n_eggs\", \"cluster\")\n\n# turn cluster from a numerical variable into a factor (categorical) variable\negg_df_with_clusters$cluster <- factor(egg_df_with_clusters$cluster)\n\nplot_k5 <- egg_df_with_clusters %>%\n  ggplot(aes(x = n_hens, y = n_eggs)) +\n  geom_point(aes(color = cluster),\n             size = 3, alpha = 0.7) +\n  labs(title = \"US Egg Production\",\n       subtitle = \"k = 5 clusters\",\n       # caption = \"Source: TidyTuesday\",\n       x = \"number of hens\",\n       y = \"number of eggs\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n#patchwork\nplot_k2 + plot_2 + plot_k5"
  }
]