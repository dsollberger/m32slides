{
  "hash": "39049d53e0021777d7d313972c9654c1",
  "result": {
    "markdown": "---\ntitle: \"27: Maximum Likelihood\"\nauthor: \"Derek Sollberger\"\ndate: \"2023-04-12\"\n# format: \n#   revealjs:\n#     scrollable: true\nformat: html\n# server: shiny\n---\n\n\n\\newcommand{\\ds}{\\displaystyle}\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today: Maximum Likelihood\n\n**Goal**: Modify distribution parameters based on observed data\n\n**Objectives**:\n\n- derive maximum likelihood estimate for the exponential distribution\n- derive maximum likelihood estimate for the Poisson distribution\n\n\n## Notation\n\n::: {.callout-tip}\n## Notation\n\nRecall,\n\n- Lower-case $\\{x_{1}, x_{2}, x_{3}, ..., x_{n}\\}$ is a set of *observations*\n- Upper-case $\\{X_{1}, X_{2}, X_{3}, ..., X_{n}\\}$ is a set of *random variables* (i.e. a data set)\n- Treating $\\{X_{1}, X_{2}, ..., X_{n}\\}$ as a set of $n$ i.i.d. (*independent and identically distributed*) random variables is a common assumption.\n- With independence,\n    $$P(X_{1}, X_{2}, ..., X_{n}) = P(X_{1}) \\cdot P(X_{2}) \\cdot ... \\cdot P(X_{n})$$\n- Each individual probability is computed (at least theoretically) with a PDF (*probability density function*)\n    $$P(x_{i}) = f_{X}(x_{i})$$\n:::\n\n\n\n## Likelihood\n\n\n::::: {.panel-tabset}\n\n## Definition\n\n::: {.callout-note}\n## Likelihood Function\n\nLet the *likelihood function*, in terms of a parameter $\\theta$, be the joint probability\n\n$$L(\\theta) = P(X_{1} = x_{1}, X_{2} = x_{2}, ..., X_{n} = x_{n}) = f_{X}(x_{1}) \\cdot f_{X}(x_{2}) \\cdots f_{X}(x_{n})$$\n\nor\n    \n$$L\\left(\\theta; \\left\\{x_{i}\\right\\}_{i=1}^{n}\\right) = \\ds\\prod_{i = 1}^{n} f_{X}(x_{i})$$\n:::\n\n\n## Maximum Likelihood\n\n::::: {.panel-tabset}\n\n## Definition\n\n::: {.callout-note}\n## Likelihood Function\n\n\\textbf{The Maximum Likelihood Principle}  Given a data set $\\{x_{1}, x_{2}, ..., x_{n}\\}$, we seek the desired parameter(s) that makes realizing the data set most likely.  \n\n$$L\\left(\\theta; \\left\\{x_{i}\\right\\}_{i=1}^{n}\\right) = \\ds\\prod_{i = 1}^{n} f_{X}(x_{i})$$\n:::\n\n::: {.callout-tip}\n## Maximization\n\nFrom calculus, recall that the main step in maximizing the value of a function is setting the first derivative equal to zero.\n:::\n\n## Visuals\n\n![simulation](likelihood1.png)\n![a better simulation](likelihood2.png)\n\n## Example 1\n\nGiven a data set $\\{x_{1}, x_{2}, ..., x_{n}\\}$, assume an $\\text{Exp}(\\lambda)$ distribution.  \n\n- Compute the value of rate parameter $\\lambda$ that maximizes the likelihood of the data set.\n- Compute the likelihood at the maximum likelihood estimate (MLE).\n- Characterize the top 5 percent of light bulbs.\n\n## Example 2\n\nGiven a data set $\\{x_{1}, x_{2}, ..., x_{n}\\}$, assume an $\\text{Pois}(\\lambda)$ distribution.  Compute the value of parameter $\\lambda$ that maximizes the likelihood of the data set.\n\n:::::\n\n\n## Estimators Revisited\n\nIf we sample from a theoretical $U(0, M)$ distribution, the sample maximum $s_{M}$ of each sample is less than or equal to $M$\n\n$$s_{M} \\leq M$$\n\nIt would follow that the average of the sample maxima underestimates the true maximum\n\n$$\\text{E}[s_{M}] \\leq M$$\n\nTherefore the sample maximum is a *biased estimator* of the true maximum.\n\n$$~$$\n\nSimilarly, the sample minimum $s_{m}$ from a $U(m, 0)$ distribution overestimates\n\n$$\\text{E}[s_{m}] \\geq m$$\n\nTherefore the sample min-mum is a *biased estimator* of the true minumum.\n\n\n## Looking Ahead\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n  \nFinal Exam will be on May 6\t\n:::\n\n::: {.column width=\"50%\"}\n![](lambda.png)\n\n[tweet source](https://mobile.twitter.com/KellyBodwin/status/1579981646844993536)\n:::\n\n::::\n\n\n\n\n\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::\n",
    "supporting": [
      "Math_32_27_Maximum_Likelihood_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}