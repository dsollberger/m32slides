{
  "hash": "6cf76a58ece3ec6b678d8bec821158b4",
  "result": {
    "markdown": "---\ntitle: \"26: Likelihood\"\nauthor: \"Derek Sollberger\"\ndate: \"2023-04-07\"\n# format: \n#   revealjs:\n#     scrollable: true\nformat: html\n# server: shiny\n---\n\n\n\\newcommand{\\ds}{\\displaystyle}\n\n\n::: {.cell}\n\n:::\n\n\n\n## Notation\n\n::: {.callout-tip}\n## Notation\n\nRecall,\n\n- Lower-case $\\{x_{1}, x_{2}, x_{3}, ..., x_{n}\\}$ is a set of *observations*\n- Upper-case $\\{X_{1}, X_{2}, X_{3}, ..., X_{n}\\}$ is a set of *random variables* (i.e. a data set)\n- Treating $\\{X_{1}, X_{2}, ..., X_{n}\\}$ as a set of $n$ i.i.d. (*independent and identically distributed*) random variables is a common assumption.\n- With independence,\n    $$P(X_{1}, X_{2}, ..., X_{n}) = P(X_{1}) \\cdot P(X_{2}) \\cdot ... \\cdot P(X_{n})$$\n- Each individual probability is computed (at least theoretically) with a PDF (*probability density function*)\n    $$P(x_{i}) = f_{X}(x_{i})$$\n:::\n\n\n## Inverse\n\n::: {.callout-note}\n## Inverse\n\nSuppose that we have a sample of data $\\{x_{1}, x_{2}, x_{3}, ..., x_{n}\\}$. Now we want to model with a probability distribution, but we need to figure out the distribution's parameters.  Let us think about this in a Bayesian way:\n\n$${\\color{purple}{P(\\text{model} | \\text{data})}} = \\ds\\frac{ {\\color{blue}{P(\\text{data} | \\text{model})} \\cdot P(\\text{model})} }{ {\\color{red}{P(\\text{data})}} }$$\n\n- ${\\color{purple}{P(\\text{model} | \\text{data})}}$ is the posterior probability that we want\n- ${\\color{blue}{P(\\text{data} | \\text{model})}}$ is a *likelihood*\n- Since the prior probability ${\\color{red}{P(\\text{data})}}$ is a constant ...\n\n... we say that the posterior probability is proportional to the likelihood.\n:::\n\n\n## Likelihood\n\n\n::::: {.panel-tabset}\n\n## Definition\n\n::: {.callout-note}\n## Likelihood Function\n\nLet the *likelihood function*, in terms of a parameter $\\theta$, be the joint probability\n\n$$L(\\theta) = P(X_{1} = x_{1}, X_{2} = x_{2}, ..., X_{n} = x_{n}) = f_{X}(x_{1}) \\cdot f_{X}(x_{2}) \\cdots f_{X}(x_{n})$$\n\nor\n    \n$$L\\left(\\theta; \\left\\{x_{i}\\right\\}_{i=1}^{n}\\right) = \\ds\\prod_{i = 1}^{n} f_{X}(x_{i})$$\n:::\n\n## Example\n\nSuppose that we have data for how long a certain type and brand of light bulb operated (in the same working conditions), and that data in months was\n\n$$6, \\quad 18, \\quad 29, \\quad 44, \\quad 48$$\nGoal: characterize the top 5 percent of light bulbs.\n\n- Build the likelihood function assuming an exponential distribution.\n- Compute the likelihood that $\\mu = 25$.\n- Compute the likelihood that $\\mu = 50$.\n\n:::::\n\n\n## Log Likelihood\n\n::::: {.panel-tabset}\n\n## Logarithms\n\n::: {.callout-tip}\n## Logarithms\n\nYou know that logarithms make large numbers smaller.  More precisely,\n$$\\ln(x) < x, \\quad x > 1$$\n\nExample: $\\ln(1234) \\approx 7.1180$\n\n\nDid you know that logarithms make small numbers larger (in size).  More precisely,\n$$|\\ln(x)| > x, \\quad 0 < x < 1$$\n\nExample: $|\\ln(0.1234)| \\approx 2.0923$\n\n\nFrom pre-calculus, recall the properties of logarithms:\n$$\\ln(AB) = \\ln(A) + \\ln(B), \\quad \\ln\\left(\\ds\\frac{A}{B}\\right) = \\ln A - \\ln B, \\quad \\ln(A^{c}) = c\\ln A$$\n:::\n\n## Example\n\nFor modeling with the exponential distribution, we saw that the likelihood function was\n\n$$L\\left(\\lambda; \\{x_{i}\\}_{i=1}^{n}\\right) = \\ds\\prod_{i=1}^{n} f_{X}(x_{i}) = \\lambda^{n}e^{-\\lambda\\sum x_{i}}$$\n\nWe take the natural logarithm to compute the log-likelihood function\n\n$$\\ell\\left(\\lambda; \\{x_{i}\\}_{i=1}^{n}\\right) = \\ln L\\left(\\lambda; \\{x_{i}\\}_{i=1}^{n}\\right) = n\\ln\\lambda - \\lambda\\ds\\sum_{i=1}^{n} x_{i}$$\n\n- Compute the log-likelihood that $\\mu = 25$.\n- Compute the log-likelihood that $\\mu = 50$.\n\n:::::\n\n\n\n\n\n## Visuals\n\n![simulation](likelihood1.png)\n![a better simulation](likelihood2.png)\n\n\n\n\n\n\n\n\n\n## Looking Ahead\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n* WHW9\n* Exam 2, Mon., Apr. 10\n\n    - more information in weekly announcement\n\t\n:::\n\n::: {.column width=\"40%\"}\n![](lambda.png)\n\n[tweet source](https://mobile.twitter.com/KellyBodwin/status/1579981646844993536)\n:::\n\n::::\n\n\n\n\n\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::\n",
    "supporting": [
      "Math_32_26_Likelihood_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}