{
  "hash": "7b73873345a458916bfdf134eec29b8b",
  "result": {
    "markdown": "---\ntitle: \"21: Estimators\"\nauthor: \"Derek Sollberger\"\ndate: \"2022-11-10\"\n# format: \n#   revealjs:\n#     scrollable: true\nformat: html\n# server: shiny\n---\n\n\n\\newcommand{\\ds}{\\displaystyle}\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today: Estimators\n\n**Goal**: Explore generalization from samples to populations\n\n**Objectives**:  Show the biased or unbiased estimation via\n\n* sample mean $\\bar{x}$\n* sample variance $s^{2}$\n* sample standard deviation $s$\n\n\n## Demographics Example\n\nFrom our Demographics Survey data of Math 32 students, suppose that the following is a sample of observations of heights (in inches):\n\n\n$$\\{x_{11} = 72, x_{12} = 61, x_{13} = 60, x_{14} = 75, x_{15} = 69\\}$$\n\n\n- then $t_{1} = 67.4$ inches is the sample mean.\n\nSuppose that the following is another sample of heights:\n\n\n$$\\{x_{21} = 66, x_{22} = 78, x_{23} = 78, x_{24} = 77, x_{25} = 64\\}$$\n\n\n- then $t_{2} = 72.6$ inches is the sample mean.\n\nSuppose that the following is another sample of heights:\n\n\n$$\\{x_{31} = 61, x_{32} = 59, x_{33} = 70, x_{34} = 61, x_{35} = 65\\}$$\n\n\n- then $t_{3} = 63.2$ inches is the sample mean.\n\n:::{.callout-tip collapse=\"true\"}\n## Sampling Itself is Probabilistic\n\nObserve: the sample mean (usually) changes upon a new set of observations\n:::\n\n- Can we calculate the average height of UC Merced students?\n- How can we calculate the average height of UC Merced students?\n\nThought: what if we take a mean of the sample means?\n\n## Estimators\n\n:::{.callout-note}\n## Estimators\n\nLet $T$ be a random variable and $f$ be some calculation\n\n$$T = f(x_{1}, x_{2}, x_{3}, ...)$$\n\n\nIf we are trying to estimate a population parameter $\\theta$, we say that $T$ is an *unbiased estimator* of $\\theta$ if\n\n$$\\text{E}[T] = \\theta$$\n\n:::\n\nToday, we will look at situations where $f$ is calculating the\n\n- mean\n- variance\n- standard deviation\n\n\n## Mean\n\n::::: {.panel-tabset}\n\n## Setup\n\nWe will run simulations with $X \\sim U(0,1)$ because we know what the answers *should* be.  The population mean is \n\n\n$$\\mu = \\ds\\frac{a + b}{2} = \\ds\\frac{1}{2}$$\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- mean(these_numbers) #record average\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = 1/2, color = \"red\", size = 3) +\n  labs(title = \"Simulation Sample Mean\",\n       subtitle = paste(\"black: sample distribution\\nred: true population mean\\nmean of sample means: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n```\n:::\n\n\n## Simulation\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Math_32_21_Law_of_Large_Numbers_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nLoosely speaking, since the sampling distribution \"lines up\" with the population mean, we say that the sample median is an *unbiased estimator* of the population mean.\n\n## Proof\n\n\\begin{array}{rcl}\n      \\text{E}[\\bar{X}_{n}] & = & \\text{E}\\left( \\ds\\frac{X_{1} + X_{2} + ... + X_{n}}{n} \\right) \\\\\n      ~ & = & \\ds\\frac{1}{n}\\text{E}\\left( X_{1} + X_{2} + ... + X_{n} \\right) \\\\\n      ~ & = & \\ds\\frac{1}{n}\\left(\\text{E}[X_{1}] + \\text{E}[X_{2}] + ... + \\text{E}[X_{n}] \\right) \\\\\n      ~ & = & \\ds\\frac{1}{n}\\left(\\mu + \\mu + ... + \\mu \\right) \\\\\n      ~ & = & \\ds\\frac{1}{n}\\left(n \\mu \\right) \\\\\n    \\end{array}\n    \nTherefore $\\text{E}[\\bar{X}_{n}] = \\mu$\n    \n:::::\n\n\n## Population Variance\n\n::::: {.panel-tabset}\n\n## Setup\n\nWe will run simulations with $X \\sim U(0,1)$ because we know what the answers *should* be.  The population variance is \n\n\n$$\\sigma^{2} = \\ds\\frac{(b-a)^{2}}{12} = \\ds\\frac{1}{12}$$\n\n\nWe will explore what happens if we apply the population variance formula\n\n\n$$\\sigma^{2} = \\frac{1}{N}\\ds\\sum_{i=1}^{N} (x_{i} - \\mu)^{2}$$\n\n\nto samples.\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# user-defined function\npop_var <- function(x){\n  N <- length(!is.na(x)) #population size\n  mu <- mean(x, na.rm = TRUE) #population mean\n  \n  # return population mean (note use of \"N\")\n  sum( (x - mu)^2 ) / N\n}\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- pop_var(these_numbers) #record population variance\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = 1/12, color = \"red\", size = 3) +\n  labs(title = \"Simulation of Population Variances\",\n       subtitle = paste(\"black: sample distribution\\nred: true population variance\\nmean of population variances: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n```\n:::\n\n\n## Simulation\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Math_32_21_Law_of_Large_Numbers_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nLoosely speaking, since the sampling distribution tends to underestimate the population variance, we say that the population variance (with $N$) is a *biased estimator* of the population variance.\n\n:::::\n\n\n## Bessel's Correction\n\nCan we rescale the process for computing variance so that the operation is an unbiased estimator for the population variance?  \n\nLet $X_{i}$ be a set of $n$ i.i.d. random variables from the same distribution with the same population variance $\\sigma^{2}$.  By independence, there is zero covariance.\n\nWe will compute the value of $k$ so that\n\n\n$$\\text{E}\\left[k \\cdot \\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X}_{n})^{2}}{n}\\right] = \\sigma^{2}$$\n\n\n**Lemma**:  $\\text{Var}(X_{i} - \\bar{X}_{n}) = \\ds\\frac{n-1}{n} \\cdot \\sigma^{2}$\n\n:::{.callout-note collapse=\"true\"}\n## Bessel's Correction\n\nWe have derived the formula for the sample variance\n\n\n$$S_{n}^{2} = \\ds\\frac{1}{n-1}\\ds\\sum_{i=1}^{n}(X_{i} - \\bar{X}_{n})^{2}$$\n\n\nThat is, the \"$n-1$\" (Bessel's correction) is in place so that the sample variance $s^{2}$ is an *unbiased* estimator of the population variance $\\sigma^{2}$\n:::\n\n\n## Sample Variance\n\n::::: {.panel-tabset}\n\n## Setup\n\nWe will run simulations with $X \\sim U(0,1)$ because we know what the answers *should* be.  The population variance is \n\n\n$$\\sigma^{2} = \\ds\\frac{(b-a)^{2}}{12} = \\ds\\frac{1}{12}$$\n\n\nWe will explore what happens if we apply the sample variance formula\n\n\n$$s^{2} = \\frac{1}{n-1}\\ds\\sum_{i=1}^{n} (x_{i} - \\mu)^{2}$$\n\n\nto samples.\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# user-defined function\nsamp_var <- function(x){\n  n  <- length(!is.na(x)) #sample size\n  xbar <- mean(x, na.rm = TRUE) #sample mean\n  \n  # return population mean (note use of \"n-1\")\n  sum( (x - xbar)^2 ) / (n-1)\n}\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- samp_var(these_numbers) #record sample variance\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = 1/12, color = \"red\", size = 3) +\n  labs(title = \"Simulation of Sample Variances\",\n       subtitle = paste(\"black: sample distribution\\nred: true population variance\\nmean of sample variances: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n```\n:::\n\n\n## Simulation\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Math_32_21_Law_of_Large_Numbers_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nLoosely speaking, since the sampling distribution \"lines up\" with the population variance, we say that the sample variance (with $n-1$) is an *unbiased estimator* of the population variance.\n\n:::::\n\n\n## Sample Standard Deviation\n\n::::: {.panel-tabset}\n\n## Setup\n\nWe will run simulations with $X \\sim U(0,1)$ because we know what the answers *should* be.  The population standard deviation is \n\n\n$$\\sigma = \\sqrt{\\ds\\frac{(b-a)^{2}}{12}} = \\sqrt{\\ds\\frac{1}{12}}$$\n\n\nWe will explore what happens if we apply the sample variance formula\n\n\n$$s = \\sqrt{\\frac{1}{n-1}\\ds\\sum_{i=1}^{n} (x_{i} - \\mu)^{2}}$$\n\n\nto samples.\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# user-defined function\nsamp_var <- function(x){\n  n  <- length(!is.na(x)) #sample size\n  xbar <- mean(x, na.rm = TRUE) #sample mean\n  \n  # return population mean (note use of \"n-1\")\n  sum( (x - xbar)^2 ) / (n-1)\n}\n\nN <- 1337 # number of iterations\nn <- 25   # sample size\n\n# pre-allocate vector of space for observations\nobs <- rep(NA, N)\n\n# run simulation\nfor(i in 1:N){\n  these_numbers <- runif(n, 0, 1) # sample n numbers from U(0,1)\n  obs[i] <- sqrt(samp_var(these_numbers)) #record sample standard deviation\n}\n\n# mean of observations\nmean_of_obs <- mean(obs)\n\n# make data frame\ndf <- data.frame(obs)\n\n# visualization\ndf |>\n  ggplot(aes(x = obs)) +\n  geom_density(color = \"black\", size = 2) +\n  geom_vline(xintercept = sqrt(1/12), color = \"red\", size = 3) +\n  labs(title = \"Simulation of Sample Variances\",\n       subtitle = paste(\"black: sample distribution\\nred: true population variance\\nmean of sample standard deviations: \", round(mean_of_obs, 4)),\n       caption = \"Math 32\") +\n  theme_minimal()\n```\n:::\n\n\n## Simulation\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Math_32_21_Law_of_Large_Numbers_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Commentary\n\nLet $X_{i}$ be a set of $n$ i.i.d. random variables from the same distribution with the same population standard deviation $\\sigma$. To avoid trivial situations, assume non-zero variance, so $\\sigma \\neq 0$.\n\n\n\nIf $s = \\sqrt{ \\ds\\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X}_{n})^{2}}{n-1} }$ was an unbiased estimator, then $\\text{E}[s] = \\sigma$\n\n\n\nHowever, by Jensen's Inequality, since $g(x) = x^{2}$ is a convex function,\n\n\n$$\\sigma^{2}  = \\text{E}[S_{n}^{2}] > \\left(\\text{E}[S_{n}]\\right)^{2}$$\n\n\nand it follows that $\\text{E}[S_{n}] < \\sigma$.  Due to the underestimation, sample standard deviation $s$ is a biased estimator of population standard deviation $\\sigma$.\n\n\n$$~$$\n\n\nHowever, in practice, the discrepancy is usually so small that it is ignored.\n\n:::::\n\n## Looking Ahead\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- WHW9\n- WHW10\n- (next LHW assignments will be given after Thanksgiving)\n  \nFinal Exam will be on Dec. 8\n\n:::\n\n::: {.column width=\"50%\"}\n![](trojan_horse.png)\n\n\n\n:::\n\n::::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::\n",
    "supporting": [
      "Math_32_21_Law_of_Large_Numbers_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}