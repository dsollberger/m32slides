{
  "hash": "a308cd636ad608806d37a98327dbad8d",
  "result": {
    "markdown": "---\ntitle: \"28: Beta Distribution\"\nauthor: \"Derek Sollberger\"\ndate: \"2023-04-14\"\n# format: \n#   revealjs:\n#     scrollable: true\nformat: html\n# server: shiny\n---\n\n\n\\newcommand{\\ds}{\\displaystyle}\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today: Beta Distribution\n\n**Goal**: Explore a distribution of proportions\n\n**Objectives**:\n\n- explore the beta distribution\n- explore the gamma function\n\n::: {.callout-note collapse=\"true\"}\n## Odds\n\nIn probability, the saying\n\n$$\\text{the odds of observing } c \\text{ is }a \\text{ to } b$$\n\nis equivalent to the probability\n\n$$P(c) = \\ds\\frac{a}{a + b}$$\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Binomial Likelihood\n\nSince we have a two-state situation of observing $a$ or not among $a + b$ trials, we can envision a binomial situation $\\text{Bin}(a + b, a)$, while probability $x$ obeys $0 \\leq x \\leq 1$.  That is, we might want some flexibility in understanding our probability $x$.  By Bayes' Rule, the posterior distribution is\n\n$$P(X = x | N = a) = \\ds\\frac{P(N = a | X = x) \\cdot P(X = x)}{P(N = a)}$$\n\nwhile the likelihood is seen from a binomial distribution\n\n$$P(N = a | X = x) = \\binom{a+b}{a}x^{a}(1-x)^{b}$$\n:::\n\n\n## Beta Distribution\n\nIf $X \\sim \\text{Beta}(\\alpha, \\beta)$, then the probability density function (PDF) is\n\n$$f(X = x) = \\begin{cases}\n  \\ds\\frac{1}{B(\\alpha, \\beta)}x^{\\alpha - 1}(1-x)^{\\beta - 1}, & 0 < x < 1 \\\\\n  0, & \\text{otherwise}\n\\end{cases}$$\n\nwhere the normalization constant\n\n$$B(\\alpha, \\beta) = \\ds\\int_{0}^{1} \\! x^{\\alpha - 1}(1-x)^{\\beta - 1} \\, dx$$\n\nto ensure that the total area under the curve is one unit.\n\n\n::: {.callout-tip collapse=\"true\"}\n## Offsets\n\nThe offset notation\n$$\\begin{array}{rcl}\n  \\alpha & = & a + 1 \\\\\n  \\beta & = & b + 1 \\\\\n\\end{array}$$\nis there to streamline the statistics seen later (such as the expected value and the variance).\n:::\n\n\n## Gamma Function\n\n::: {.callout-note collapse=\"true\"}\n## Gamma Function\n\nThat normalization constant\n\n$$B(\\alpha, \\beta) = \\ds\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}$$\n\ncan be viewed in terms of the \\textit{gamma function}\n\n$$\\Gamma(x) = \\ds\\int_{0}^{\\infty} \\! t^{x-1}e^{-t} \\, dt$$\n:::\n\n**Claim:** $\\Gamma(x+1) = x\\Gamma(x)$\n\n::: {.callout-tip collapse=\"true\"}\n## Connection to Factorials\n\nAlong with computing $\\Gamma(1) = 1$, it follows that for natural numbers\n$$\\Gamma(x) = (x-1)!$$\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Generalized Factorial Function\n\nHowever, the gamma function allows us to input real numbers.  For example,\n\n$$\\begin{array}{rcll}\n  \\Gamma\\left(\\ds\\frac{1}{2}\\right) & = & \\ds\\int_{0}^{\\infty} \\! t^{-1/2}e^{-t} \\, dt & \\text{defintion of gamma function} \\\\\n  ~ & = & \\ds\\int_{0}^{\\infty} \\! u^{-1}e^{-u^{2}}(2u) \\, du & \\text{substitution } t = u^{2} \\rightarrow dt = 2u \\, du \\\\\n  ~ & = & 2\\ds\\int_{0}^{\\infty} \\! e^{-u^{2}} \\, du & \\text{algebra} \\\\\n  ~ & = & \\ds\\int_{-\\infty}^{\\infty} \\! e^{-u^{2}} \\, du & \\text{even function}  \\\\\n  ~ & = & \\sqrt{\\pi} & \\text{Gaussian} \\\\\n\\end{array}$$\n:::\n\n## Beta Distribution\n\nIn terms of the gamma function, we now have that the beta distribution PDF is\n\n$$f(X = x) = \\begin{cases}\n  \\ds\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha - 1}(1-x)^{\\beta - 1}, & 0 < x < 1 \\\\\n  0, & \\text{otherwise}\n\\end{cases}$$\nTo understand the probabilistic environment, let us derive the expected value.\n\n::: {.callout-note collapse=\"true\"}\n## Expected Value\n\n$$\\begin{array}{rcl}\n  \\text{E}[X] & = & \\ds\\int_{-\\infty}^{\\infty} \\! x \\cdot f_{X}(x) \\, dx \\\\\n  ~ & = & \\ds\\int_{0}^{1} \\! x \\cdot \\ds\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha - 1}(1-x)^{\\beta - 1} \\, dx \\\\\n  ~ & = & \\ds\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\ds\\int_{0}^{1} \\! x^{(\\alpha+1) - 1}(1-x)^{\\beta - 1} \\, dx \\\\\n  ~ & = & \\ds\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\cdot \\ds\\frac{\\Gamma(\\alpha + 1)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta + 1)}\\\\\n  ~ & = & \\ds\\frac{\\Gamma(\\alpha + 1)}{\\Gamma(\\alpha)} \\cdot \\ds\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha + \\beta + 1)} \\\\\n  ~ & = & \\ds\\frac{\\alpha\\Gamma(\\alpha)}{\\Gamma(\\alpha)} \\cdot \\ds\\frac{\\Gamma(\\alpha + \\beta)}{(\\alpha + \\beta)\\Gamma(\\alpha + \\beta)} \\\\\n  ~ & = & \\ds\\frac{\\alpha}{\\alpha + \\beta} \\\\\n\\end{array}$$\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Variance\n\nSimilarly, the variance for $X \\sim \\text{Beta}(\\alpha, \\beta)$ is\n$$\\sigma^{2} = \\ds\\frac{\\alpha\\beta}{(\\alpha + \\beta + 1)(\\alpha + \\beta)^{2}}$$\n:::\n\n## Example\n\nIf we have 3 heads and 2 tails in a trial of flipping an unfair coin, assume a beta distribution and build a range-rule-of-thumb interval $(\\mu - 2\\sigma, \\mu + 2\\sigma)$ for the posterior probability.\n\n::: {.callout-tip collapse=\"true\"}\n## Parameters\n\n$$a = 3, b = 2 \\quad\\Rightarrow\\quad \\alpha = 4, \\beta = 3$$\n$$\\mu = \\text{E}[X] = \\ds\\frac{\\alpha}{\\alpha + \\beta} = \\ds\\frac{4}{7}, \\quad \\sigma^{2} = \\ds\\frac{\\alpha\\beta}{(\\alpha + \\beta + 1)(\\alpha + \\beta)^{2}} = \\ds\\frac{12}{8(7)^{2}}$$\nand our range-rule-of-thumb interval is\n$$\\ds\\frac{4}{7} \\pm \\ds\\frac{2}{7}\\sqrt{\\ds\\frac{3}{2}}$$\nor approximately $(0.2215, 0.9214)$\n:::\n\n\n\n\n\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::\n",
    "supporting": [
      "Math_32_28_Beta_Distribution_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}