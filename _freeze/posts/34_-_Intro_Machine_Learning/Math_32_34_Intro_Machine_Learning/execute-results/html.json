{
  "hash": "61d5890a91bddfc9804ac49ae0c93334",
  "result": {
    "markdown": "---\ntitle: \"34: Introduction to Machine Learning\"\nauthor: \"Derek Sollberger\"\ndate: \"2023-04-28\"\n# format: \n#   revealjs:\n#     scrollable: true\nformat: html\n# server: shiny\n---\n\n\n\\newcommand{\\ds}{\\displaystyle}\n\n\n::: {.cell}\n\n:::\n\n\n\n## 34: Introduction to Machine Learning\n\n**Goal**: introduce machine learning (ideas and terminology)\n\n**Objectives**:\n\n- introduce `tidymodels` package\n- practice with a `TidyTuesday` data set\n\n\n## Data: Tour de France\n\nSource: [TidyTuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-04-07/readme.md) data set from April 7, 2020\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntdf_winners <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-04-07/tdf_winners.csv')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(tdf_winners, give.attr = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nspc_tbl_ [106 × 19] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ edition      : num [1:106] 1 2 3 4 5 6 7 8 9 10 ...\n $ start_date   : Date[1:106], format: \"1903-07-01\" \"1904-07-02\" ...\n $ winner_name  : chr [1:106] \"Maurice Garin\" \"Henri Cornet\" \"Louis Trousselier\" \"René Pottier\" ...\n $ winner_team  : chr [1:106] \"La Française\" \"Conte\" \"Peugeot–Wolber\" \"Peugeot–Wolber\" ...\n $ distance     : num [1:106] 2428 2428 2994 4637 4488 ...\n $ time_overall : num [1:106] 94.6 96.1 NA NA NA ...\n $ time_margin  : num [1:106] 2.99 2.27 NA NA NA ...\n $ stage_wins   : num [1:106] 3 1 5 5 2 5 6 4 2 3 ...\n $ stages_led   : num [1:106] 6 3 10 12 5 13 13 3 13 13 ...\n $ height       : num [1:106] 1.62 NA NA NA NA NA 1.78 NA NA NA ...\n $ weight       : num [1:106] 60 NA NA NA NA NA 88 NA NA NA ...\n $ age          : num [1:106] 32 19 24 27 24 25 22 22 26 23 ...\n $ born         : Date[1:106], format: \"1871-03-03\" \"1884-08-04\" ...\n $ died         : Date[1:106], format: \"1957-02-19\" \"1941-03-18\" ...\n $ full_name    : chr [1:106] NA NA NA NA ...\n $ nickname     : chr [1:106] \"The Little Chimney-sweep\" \"Le rigolo (The joker)\" \"Levaloy / Trou-trou\" NA ...\n $ birth_town   : chr [1:106] \"Arvier\" \"Desvres\" \"Paris\" \"Moret-sur-Loing\" ...\n $ birth_country: chr [1:106] \"Italy\" \"France\" \"France\" \"France\" ...\n $ nationality  : chr [1:106] \" France\" \" France\" \" France\" \" France\" ...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(tdf_winners)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"edition\"       \"start_date\"    \"winner_name\"   \"winner_team\"  \n [5] \"distance\"      \"time_overall\"  \"time_margin\"   \"stage_wins\"   \n [9] \"stages_led\"    \"height\"        \"weight\"        \"age\"          \n[13] \"born\"          \"died\"          \"full_name\"     \"nickname\"     \n[17] \"birth_town\"    \"birth_country\" \"nationality\"  \n```\n:::\n:::\n\n\n## Early Look\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntdf_winners %>%\n  ggplot(aes(x = height, y = time_overall)) +\n  geom_point(color = \"blue\") +\n  labs(title = \"Are taller bicyclists faster?\",\n       subtitle = \"featuring Tour de France winners\",\n       caption = \"Source: TidyTuesday\",\n       x = \"height (meters)\",\n       y = \"time (hours)\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 41 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](Math_32_34_Intro_Machine_Learning_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n## Cleaning Data\n\nSometimes we like to perform some *preprocessing* of the data.  In this example, we will \n\n* focus on the champions that were more athletic than in the early years.\n* focus on biker `pace` (response variable) as the route changes from year to year\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- tdf_winners %>%\n  select(c(distance, time_overall, \n           height, weight, age)) %>%\n  filter(complete.cases(.)) %>%\n  filter(height >= 1.7) %>%\n  mutate(pace = distance / time_overall) %>%\n  select(c(pace, height, weight, age))\n\n# dimensions\ndim(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 62  4\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n   pace height weight   age\n  <dbl>  <dbl>  <dbl> <dbl>\n1  31.6   1.72     66    23\n2  33.4   1.72     66    33\n3  32.1   1.77     68    29\n4  32.2   1.77     68    30\n5  34.6   1.79     75    26\n6  33.2   1.79     75    29\n```\n:::\n:::\n\n\n## Multiple Predictor Variables\n\n::::: {.panel-tabset}\n\n### Height\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Math_32_34_Intro_Machine_Learning_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n### Age\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Math_32_34_Intro_Machine_Learning_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n### Weight\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Math_32_34_Intro_Machine_Learning_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n### R code\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>%\n  ggplot(aes(x = height, y = pace)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_smooth(method = \"lm\", linewidth = 3,\n              se = FALSE, color = \"red\") +\n  labs(title = \"Are taller bicyclists faster?\",\n       subtitle = \"featuring Tour de France winners\",\n       caption = \"Source: TidyTuesday\",\n       x = \"height (meters)\",\n       y = \"pace (km/hr)\") +\n  theme_minimal()\n\ndf %>%\n  ggplot(aes(x = age, y = pace)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_smooth(method = \"lm\", linewidth = 3,\n              se = FALSE, color = \"red\") +\n  labs(title = \"Are older bicyclists faster?\",\n       subtitle = \"featuring Tour de France winners\",\n       caption = \"Source: TidyTuesday\",\n       x = \"age\",\n       y = \"pace (km/hr)\") +\n  theme_minimal()\n\ndf %>%\n  ggplot(aes(x = weight, y = pace)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_smooth(method = \"lm\", linewidth = 3,\n              se = FALSE, color = \"red\") +\n  labs(title = \"Are heavier bicyclists faster?\",\n       subtitle = \"featuring Tour de France winners\",\n       caption = \"Source: TidyTuesday\",\n       x = \"weight (kg)\",\n       y = \"pace (km/hr)\") +\n  theme_minimal()\n```\n:::\n\n\n:::::\n\n## Regression via TidyModels\n\n::::: {.panel-tabset}\n\n### Start\n\n“With tidymodels, we start by specifying the functional form of the model that we want using the parsnip package.”\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\n\n### Model Engine\n\n“However, now that the type of model has been specified, a method for fitting or training the model can be stated using the engine. The engine value is often a mash-up of the software that can be used to fit or train the model as well as the estimation method.”\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>% \n  set_engine(\"lm\") #linear model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\n\n### Fitting a Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit <- linear_reg() %>% \n  set_engine(\"lm\") %>%\n  fit(pace ~ height + weight + age, data = df)\nlm_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = pace ~ height + weight + age, data = data)\n\nCoefficients:\n(Intercept)       height       weight          age  \n     3.8455      21.0987      -0.1387       0.2113  \n```\n:::\n:::\n\n\n### Examining a Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(lm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    3.85    12.3        0.313  0.755 \n2 height        21.1      8.06       2.62   0.0112\n3 weight        -0.139    0.0685    -2.03   0.0474\n4 age            0.211    0.0979     2.16   0.0350\n```\n:::\n:::\n\n\nObserve where we have p-values < 0.05\n\n:::::\n\n## Interaction Terms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit_with_interaction <- linear_reg() %>% \n  set_engine(\"lm\") %>%\n  fit(pace ~ height + weight + age + height:weight + height:age + weight:age +\n        height:weight:age,\n      data = df)\nlm_fit_with_interaction\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = pace ~ height + weight + age + height:weight + \n    height:age + weight:age + height:weight:age, data = data)\n\nCoefficients:\n      (Intercept)             height             weight                age  \n         924.8499          -444.1560           -15.6339           -27.8628  \n    height:weight         height:age         weight:age  height:weight:age  \n           7.9297            13.9352             0.4802            -0.2425  \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(lm_fit_with_interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 × 5\n  term              estimate std.error statistic p.value\n  <chr>                <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        925.     2272.        0.407   0.686\n2 height            -444.     1287.       -0.345   0.731\n3 weight             -15.6      32.8      -0.477   0.635\n4 age                -27.9      80.3      -0.347   0.730\n5 height:weight        7.93     18.5       0.428   0.670\n6 height:age          13.9      45.5       0.306   0.761\n7 weight:age           0.480     1.16      0.414   0.680\n8 height:weight:age   -0.243     0.656    -0.370   0.713\n```\n:::\n:::\n\n\nThis may be foreshadowing of *overfitting*.\n\n## Prediction\n\n::::: {.panel-tabset}\n\n### New Data\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n* SpongeBob is a 26-year-old, 1.77 m tall bicyclist who weighs 55 kg\n* Patrick is a 25-year-old, 1.81 m tall bicyclist who weighs 75 kg\n* Squidward is a 31-year-old, 1.89 m tall bicyclist who weighs 65 kg\n\t\n:::\n\n::: {.column width=\"50%\"}\n![](spongebob_patrick_squidward.png)\n:::\n\n::::\n\n### Predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_contestants <- data.frame(name = c(\"SpongeBob\", \"Patrick\", \"Squidward\"),\n                              age = c(26, 25, 31),\n                              height = c(1.77, 1.81, 1.89),\n                              weight = c(55, 75, 65))\nmean_predictions <- predict(lm_fit, new_data = new_contestants)\nmean_predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 1\n  .pred\n  <dbl>\n1  39.1\n2  36.9\n3  41.3\n```\n:::\n:::\n\n\n### Confidence Intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCI_predictions <- predict(lm_fit,\n                          new_data = new_contestants,\n                          type = \"conf_int\")\nCI_predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n  .pred_lower .pred_upper\n        <dbl>       <dbl>\n1        37.1        41.0\n2        35.9        38.0\n3        39.0        43.5\n```\n:::\n:::\n\n\n### Error Bars\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_for_plot <- new_contestants %>%\n  bind_cols(mean_predictions) %>%\n  bind_cols(CI_predictions)\ndf_for_plot\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       name age height weight    .pred .pred_lower .pred_upper\n1 SpongeBob  26   1.77     55 39.05386    37.07966    41.02807\n2   Patrick  25   1.81     75 36.91179    35.85758    37.96601\n3 Squidward  31   1.89     65 41.25491    38.97189    43.53794\n```\n:::\n:::\n\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_for_plot %>%\n  ggplot(aes(x = name)) +\n  geom_errorbar(aes(ymin = .pred_lower,\n                    ymax = .pred_upper),\n                color = \"red\",\n                width = 0.32) +\n  geom_point(aes(y = .pred), color = \"blue\", size = 5) +\n  labs(title = \"Tour de Under the Sea\",\n       subtitle = \"Welcome the new contestants!\",\n       caption = \"Math 32\",\n       x = \"\",\n       y = \"pace (km/hr)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Math_32_34_Intro_Machine_Learning_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n:::::\n\n## Data Splitting\n\n::::: {.panel-tabset}\n\n### Split\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_split <- initial_split(df)\ntrain_df <- training(data_split)\ntest_df <- testing(data_split)\n\nprint(paste(\"The number of observations in the training set is:\", \n            nrow(train_df)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The number of observations in the training set is: 46\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"The number of observations in the testing set is:\", \n            nrow(test_df)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The number of observations in the testing set is: 16\"\n```\n:::\n:::\n\n\n### One Split\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Math_32_34_Intro_Machine_Learning_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n### Many Splits\n\n![](images/many_splits.gif)\n\n### R code\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitle_string <- \"<span style='color:#000000'><b>Training Sets</b></span> <span style='color:#0000FF'>and</span> \n<span style='color:#FF0000'><b>Testing Sets</b></span>\"\n\nfor(i in 1:10){\n  \n  data_split <- initial_split(df)\n  train_df <- training(data_split)\n  test_df <- testing(data_split)\n  \n  this_plot <- train_df %>%\n    ggplot(aes(x = height, y = pace)) +\n    geom_point(aes(color = \"training set\"), \n               # color = \"black\"\n    ) +\n    geom_smooth(method = \"lm\",\n                aes(x = height, y = pace),\n                color = \"black\",\n                data = train_df,\n                formula = \"y ~ x\",\n              se = FALSE) +\n  geom_point(aes(x = height, y = pace, color = \"testing set\"),\n             # color = \"red\",\n             data = test_df,\n             size = 3) +\n  labs(title = title_string,\n       subtitle = \"approx 75-25 percent split\",\n       caption = \"Math 32\",\n       x = \"height (meters)\",\n       y = \"pace (km/hr)\") +\n  scale_color_manual(name = \"Data Split\",\n                     breaks = c(\"training set\", \"testing set\"),\n                     values = c(\"training set\" = \"black\",\n                                \"testing set\" = \"red\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_markdown(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n  \n  ggsave(paste0(\"images/plot\", i, \".png\"),\n         plot = this_plot,\n         device = \"png\")\n}\n```\n:::\n\n\n:::::\n\n## Metrics\n\nWe then should get a sense of the validity of a model.  One metric is *mean square error* of the test set.\n\n$$\\text{MSE} = \\ds\\frac{1}{n_{\\text{test}}}\\sum_{j = 1}^{n_{\\text{test}}} (y_{j} - \\hat{y}_{j})^{2}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_split <- initial_split(df)\ntrain_df <- training(data_split)\ntest_df <- testing(data_split)\n\nlm_train <- linear_reg() %>% \n  set_engine(\"lm\") %>%\n  fit(pace ~ height + weight + age, data = train_df)\n\nn_test <- nrow(test_df)\n\nMSE <- (1/n_test)*sum((\n  test_df$pace - \n    predict(lm_train, new_data = test_df |> select(-pace))\n)^2)\nMSE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.773913\n```\n:::\n:::\n\n\n## Cross-Validation\n\nTo help generalize to a variety of testing sets, we can perform *cross-validation* by utilizing several training/testing set splits.\n\nWe can then compute the *cross-validation error* by computing the mean of the test metric.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 10\nMSE_vec <- rep(NA, N)\n\nfor(j in 1:N){\n  data_split <- initial_split(df)\n  train_df <- training(data_split)\n  test_df <- testing(data_split)\n  \n  lm_train <- linear_reg() %>% \n    set_engine(\"lm\") %>%\n    fit(pace ~ height + weight + age, data = train_df)\n  \n  n_test <- nrow(test_df)\n  \n  MSE_vec[j] <- (1/n_test)*sum((\n    test_df$pace - \n      predict(lm_train, new_data = test_df |> select(-pace))\n  )^2)\n}\n\n# vector of MSE\nMSE_vec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  6.721709  9.335262  5.487857 10.713953  3.355675  9.758514  4.483034\n [8] 12.320367  4.809474  6.830113\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# cross-validation error\ncv_error <- mean(MSE_vec)\ncv_error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7.381596\n```\n:::\n:::\n",
    "supporting": [
      "Math_32_34_Intro_Machine_Learning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}