{
  "hash": "916c107d19a2ba0c0e66f691e865a12d",
  "result": {
    "markdown": "---\ntitle: \"24: Linear Regression\"\nauthor: \"Derek Sollberger\"\ndate: \"2022-11-28\"\n# format: \n#   revealjs:\n#     scrollable: true\nformat: html\n# server: shiny\n---\n\n\n\\newcommand{\\ds}{\\displaystyle}\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today: Linear Regression\n\n**Goal**: Summarize bivariate data\n\n**Objectives**:\n\n- determine a best-fit line from a bivariate data set\n- make predictions based on a linear regression model\n\n## Moxillation\n\n![](traxolline.png)\n\nQuery: predict how much moxillation will take place at 70 traxolline.\n\n## Residuals\n\n:::: {.columns}\n\n::: {.column width=\"70%\"}\nGoal: Given a bivariate data set $\\{x_{i}, y_{i}\\}_{i=1}^{n}$, form a **linear regression model**\n\n\n$$\\hat{y} = a + bx$$\n\n\nthat ``best fits'' the data.  Note that such a line will not go through all of the data (except in linear, deterministic situations), so\n\n- denote $y_{i}$ for true outcomes\n- denote $\\hat{y}_{i}$ for estimates (or predictions)\n- then $y_{i} - \\hat{y}_{i}$ is the $i^{\\text{th}}$ *residual*\n:::\n\n::: {.column width=\"30%\"}\n![image credit: www.jmp.com](residuals.png)\n:::\n\n::::\n\n## Method of Least Squares\n\nLike our derivation of formulas for variance and standard deviation, scientists decided to square the residuals (focus on size of residuals, avoid positive versus negative signs).  Let the *total error* be\n\n\n$$E(a,b) = \\ds\\sum_{i = 1}^{n} (y_{i} - \\hat{y}_{i})^{2} = \\ds\\sum_{i = 1}^{n} (y_{i} - a - bx_{i})^{2} $$\n\n\n- The ``best-fit line'' minimizes the error.\n- To minimize the error, start by setting the partial derivatives equal to zero:\n\n\n$$\\ds\\frac{\\partial E}{\\partial a} = 0, \\quad \\ds\\frac{\\partial E}{\\partial b} = 0$$\n\nThankfully, the function $E(a,b)$ is an elliptical paraboloid, so there is a global minimum at the critical point, and that minimum is found where\n\n\n$$a = \\ds\\frac{ (\\sum y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }, \\quad b = \\ds\\frac{ n\\sum x_{i}y_{i} - (\\sum x_{i})(\\sum y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }$$\n\n\n::: {.callout-note}\n## Best-Fit Linear Regression Model\n\nIf sample means $\\bar{x}$ and $\\bar{y}$, sample standard deviations $s_{x}$ and $s_{y}$, and correlation coefficient $r$ were previously computed, then the best-fit linear regression line $\\hat{y} = mx + b$ is computed with\n\n\n$$m = \\ds\\frac{ rs_{y} }{ s_{x} }, \\quad b = \\bar{y} - m\\bar{x}$$\n\n\n- If correlation $r > 0$, then the slope of the regression line is also positive\n- If correlation $r < 0$, then the slope of the regression line is also negative\n:::\n\nIn a scatterplot, an **outlier** is a point lying far away from the other data points.  Paired sample data may include one or more *influential points*, which are points that strongly affect the graph of the regression line.\n\n\n## Estimators for the Coefficients\n\n::: {.callout-note}\n## Estimators for the Coefficients\n\n**Claim:**  Treating $Y$ as a random variable for the true outcomes, the least-squares estimators\n\n\n$$A = \\ds\\frac{ (\\sum Y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}Y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }, \\quad B = \\ds\\frac{ n\\sum x_{i}Y_{i} - (\\sum x_{i})(\\sum Y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }$$\n\n\nare unbiased estimators of $a$ and $b$ respectively.\n:::\n\n**Proof:**  We need to show that the expected values $\\text{E}[A] = a$ and $\\text{E}[B] = b$\n\n\\begin{array}{rcl}\n  \\text{E}[B] & = & \\ds\\frac{ n\\sum x_{i}\\text{E}[Y_{i}] - (\\sum x_{i})(\\sum \\text{E}[Y_{i}]) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n  ~ & = & \\ds\\frac{ n\\sum x_{i}(a + bx_{i}) - (\\sum x_{i})[\\sum (a + bx_{i})] }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n  ~ & = & \\ds\\frac{ n\\sum x_{i}(a + bx_{i}) - (\\sum x_{i})(na + b\\sum x_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n  ~ & = & \\ds\\frac{ an\\sum x_{i} + bn\\sum x_{i}y_{i} - an\\sum x_{i} + b(\\sum x_{i})^{2} }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n  ~ & = & \\ds\\frac{ b[n\\sum x_{i}^{2} - (\\sum x_{i})^{2}] }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} } \\\\\n\\end{array}\n\nWe have shown that expected value $\\text{E}[B] = b$, so $B$ is an unbiased estimator of $b$.\n\n\nThen,\n\n\\begin{array}{rcl}\n  \\text{E}[A] & = & \\text{E}[\\bar{Y}_{n}] - \\text{E}[B]\\bar{x} \\\\\n  ~ & = & \\ds\\frac{1}{n}\\ds\\sum_{i=1}^{n} \\text{E}[Y_{i}] - b\\bar{x} \\\\\n  ~ & = & \\ds\\frac{1}{n}\\ds\\sum_{i=1}^{n} (a + bx_{i}) - b\\bar{x} \\\\\n  ~ & = & a + b\\bar{x} - b\\bar{x} \\\\\n\\end{array}\n\nWe have shown that expected value $\\text{E}[A] = a$, so $A$ is an unbiased estimator of $a$.\n\n\n## Estimator for the Variance\n\n- this slide is optional (i.e. not on exams)\n\n::: {.callout-tip}\n## Estimator for the Variance\n\n- But can we estimate the population variance $\\sigma_{y}^{2}$ without knowing the relationship between $x$ and $y$?\n- Starting with the average of the squared residuals,\n\n    $$R_{n}^{2} = \\ds\\frac{1}{n}\\ds\\sum_{i=1}^{n} (Y_{i} - a - bx_{i})^{2}$$\n\n    it can be computed that expected value $\\text{E}[R_{n}^{2}] = \\ds\\frac{n-2}{n}\\sigma_{y}^{2}$\n- Rescaling,\n\n    $$S_{n}^{2} = \\ds\\frac{1}{n-2}\\ds\\sum_{i=1}^{n} (Y_{i} - a - bx_{i})^{2}$$\n\n    is an unbiased estimator of the population variance $\\sigma_{y}^{2}$\n:::\n\n\n## Predictions\n\n::: {.callout-note}\n## Predictions\n\nFinally, with a linear regression model $\\hat{y} = a + bx$ in place, plug in a data value $x$ to form a prediction $\\hat{y}$\n:::\n\n::::: {.panel-tabset}\n\n## Example 1\n\nThe table below displays data for enrollment levels at UC Merced between the years 2011 and 2016.\n\n![](dav.png)\n\nIf the enrollment numbers are $\\{x_{i}\\}$ data, then the sample mean is $\\bar{x} = 6240.5$ students and the sample standard deviation is $s_{x} = 737.3091$ students.  If the drug abuse violations are $\\{y_{i}\\}$ data, then the sample mean is $\\bar{y} = 22.5$ violations and the sample standard deviation is $s_{y} = 9.7108$ violations.  Finally, the correlation coefficient is $r \\approx 0.0471$.\n\n- Find the best-fit linear regression line in the $\\hat{y} = mx + b$ form.\n- What does the regression model predict for the number of drug abuse violations for the year 2017 enrollment of 7967 students?\n- What does the regression model predict for the enrollment if there are 36 drug abuse violations reported?\n\n## Example 2\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n![](flux.png)\t\n:::\n\n::: {.column width=\"50%\"}\nJoyce Byers notices that magnets are falling off the shelves.  She consults the kids' science teacher Mr. Clarke.  Suppose that they conduct an investigation where they strength of magnetic fields versus the distance from Joyce's house and then record the amounts (in the table below).  Let us treat the distances as the $\\{x_{i}\\}$ data and the magnetic flux density as the $\\{y_{i}\\}$ data.  Joyce fortunately has a ``scientific calculator'' from the government lab, and she calculates the sample means of $\\bar{x} = 15$ miles and $\\bar{y} \\approx 8.9999$ teslas.  Mr. Clarke then calculates sample standard deviations of $s_{x} \\approx 1.9850$ and $s_{y} \\approx 1.3742$ along with a correlation of $r \\approx 0.9630$.  Build a linear regression model to predict the magnetic flux density when they are 18 miles away from Joyce's house.\n:::\n\n::::\n\n:::::\n\n\n## Looking Ahead\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n## Upcoming\n\n- LHW9\n- LHW10\n  \nFinal Exam will be on Dec. 8\t\n\n- more information in weekly announcement\n:::\n\n::: {.column width=\"50%\"}\n![](lambda.png)\n\n[tweet source](https://mobile.twitter.com/KellyBodwin/status/1579981646844993536)\n:::\n\n::::\n\n\n\n\n\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::\n",
    "supporting": [
      "Math_32_24_Linear_Regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}