{
  "hash": "db4617a4c60374278bc2035bc877fe1c",
  "result": {
    "markdown": "---\ntitle: \"23: Maximum Likelihood\"\nauthor: \"Derek Sollberger\"\ndate: \"2022-11-17\"\n# format: \n#   revealjs:\n#     scrollable: true\nformat: html\n# server: shiny\n---\n\n\n\\newcommand{\\ds}{\\displaystyle}\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today: Maximum Likelihood\n\n**Goal**: Modify distribution parameters based on observed data\n\n**Objectives**:\n\n- derive maximum likelihood estimate for the exponential distribution\n- derive maximum likelihood estimate for the Poisson distribution\n\n\n## Notation\n\n::: {.callout-tip}\n## Notation\n\nRecall,\n\n- Lower-case $\\{x_{1}, x_{2}, x_{3}, ..., x_{n}\\}$ is a set of *observations*\n- Upper-case $\\{X_{1}, X_{2}, X_{3}, ..., X_{n}\\}$ is a set of *random variables* (i.e. a data set)\n- Treating $\\{X_{1}, X_{2}, ..., X_{n}\\}$ as a set of $n$ i.i.d. (*independent and identically distributed*) random variables is a common assumption.\n- With independence,\n\n    $$P(X_{1}, X_{2}, ..., X_{n}) = P(X_{1}) \\cdot P(X_{2}) \\cdot ... \\cdot P(X_{n})$$\n\n- Each individual probability is computed (at least theoretically) with a PDF (*probability density function*)\n\n    $$P(x_{i}) = f_{X}(x_{i})$$\n\n:::\n\n\n## Inverse\n\n::: {.callout-note}\n## Inverse\n\nSuppose that we have a sample of data $\\{x_{1}, x_{2}, x_{3}, ..., x_{n}\\}$. Now we want to model with a probability distribution, but we need to figure out the distribution's parameters.  Let us think about this in a Bayesian way:\n\n\n$${\\color{purple}{P(\\text{model} | \\text{data})}} = \\ds\\frac{ {\\color{blue}{P(\\text{data} | \\text{model})} \\cdot P(\\text{model})} }{ {\\color{red}{P(\\text{data})}} }$$\n\n\n- ${\\color{purple}{P(\\text{model} | \\text{data})}}$ is the posterior probability that we want\n- ${\\color{blue}{P(\\text{data} | \\text{model})}}$ is a *likelihood*\n- Since the prior probability ${\\color{red}{P(\\text{data})}}$ is a constant ...\n\n... we say that the posterior probability is proportional to the likelihood.\n:::\n\n\n## Likelihood\n\n\n::::: {.panel-tabset}\n\n## Definition\n\n::: {.callout-note}\n## Likelihood Function\n\nLet the *likelihood function*, in terms of a parameter $\\theta$, be the joint probability\n\n\n$$L(\\theta) = P(X_{1} = x_{1}, X_{2} = x_{2}, ..., X_{n} = x_{n}) = f_{X}(x_{1}) \\cdot f_{X}(x_{2}) \\cdots f_{X}(x_{n})$$\n\n\nor\n    \n\n$$L\\left(\\theta; \\left\\{x_{i}\\right\\}_{i=1}^{n}\\right) = \\ds\\prod_{i = 1}^{n} f_{X}(x_{i})$$\n\n:::\n\n## Example\n\nSuppose that we have data for how long a certain type and brand of light bulb operated (in the same working conditions), and that data in months was\n\n\n$$6, \\quad 18, \\quad 29, \\quad 44, \\quad 48$$\n\nGoal: characterize the top 5 percent of light bulbs.\n\n- Build the likelihood function assuming an exponential distribution.\n- Compute the likelihood that $\\mu = 25$.\n- Compute the likelihood that $\\mu = 50$.\n\n:::::\n\n\n## Log Likelihood\n\n::::: {.panel-tabset}\n\n## Logarithms\n\n::: {.callout-tip}\n## Logarithms\n\nYou know that logarithms make large numbers smaller.  More precisely,\n\n$$\\ln(x) < x, \\quad x > 1$$\n\n\nExample: $\\ln(1234) \\approx 7.1180$\n\n\nDid you know that logarithms make small numbers larger (in size).  More precisely,\n\n$$|\\ln(x)| > x, \\quad 0 < x < 1$$\n\n\nExample: $|\\ln(0.1234)| \\approx 2.0923$\n\n\nFrom pre-calculus, recall the properties of logarithms:\n\n$$\\ln(AB) = \\ln(A) + \\ln(B), \\quad \\ln\\left(\\ds\\frac{A}{B}\\right) = \\ln A - \\ln B, \\quad \\ln(A^{c}) = c\\ln A$$\n\n:::\n\n## Example\n\nFor modeling with the exponential distribution, we saw that the likelihood function was\n\n\n$$L\\left(\\lambda; \\{x_{i}\\}_{i=1}^{n}\\right) = \\ds\\prod_{i=1}^{n} f_{X}(x_{i}) = \\lambda^{n}e^{-\\lambda\\sum x_{i}}$$\n\n\nWe take the natural logarithm to compute the log-likelihood function\n\n\n$$\\ell\\left(\\lambda; \\{x_{i}\\}_{i=1}^{n}\\right) = \\ln L\\left(\\lambda; \\{x_{i}\\}_{i=1}^{n}\\right) = n\\ln\\lambda - \\lambda\\ds\\sum_{i=1}^{n} x_{i}$$\n\n\n- Compute the log-likelihood that $\\mu = 25$.\n- Compute the log-likelihood that $\\mu = 50$.\n\n:::::\n\n\n## Maximum Likelihood\n\n::::: {.panel-tabset}\n\n## Definition\n\n::: {.callout-note}\n## Likelihood Function\n\n\\textbf{The Maximum Likelihood Principle}  Given a data set $\\{x_{1}, x_{2}, ..., x_{n}\\}$, we seek the desired parameter(s) that makes realizing the data set most likely.  \n\n\n$$L\\left(\\theta; \\left\\{x_{i}\\right\\}_{i=1}^{n}\\right) = \\ds\\prod_{i = 1}^{n} f_{X}(x_{i})$$\n\n:::\n\n::: {.callout-tip}\n## Maximization\n\nFrom calculus, recall that the main step in maximizing the value of a function is setting the first derivative equal to zero.\n:::\n\n## Visuals\n\n![simulation](likelihood1.png)\n![a better simulation](likelihood2.png)\n\n## Example 1\n\nGiven a data set $\\{x_{1}, x_{2}, ..., x_{n}\\}$, assume an $\\text{Exp}(\\lambda)$ distribution.  \n\n- Compute the value of rate parameter $\\lambda$ that maximizes the likelihood of the data set.\n- Compute the likelihood at the maximum likelihood estimate (MLE).\n- Characterize the top 5 percent of light bulbs.\n\n## Example 2\n\nGiven a data set $\\{x_{1}, x_{2}, ..., x_{n}\\}$, assume an $\\text{Pois}(\\lambda)$ distribution.  Compute the value of parameter $\\lambda$ that maximizes the likelihood of the data set.\n\n:::::\n\n\n## Estimators Revisited\n\nIf we sample from a theoretical $U(0, M)$ distribution, the sample maximum $s_{M}$ of each sample is less than or equal to $M$\n\n\n$$s_{M} \\leq M$$\n\n\nIt would follow that the average of the sample maxima underestimates the true maximum\n\n\n$$\\text{E}[s_{M}] \\leq M$$\n\n\nTherefore the sample maximum is a *biased estimator* of the true maximum.\n\n\n$$~$$\n\n\nSimilarly, the sample minimum $s_{m}$ from a $U(m, 0)$ distribution overestimates\n\n\n$$\\text{E}[s_{m}] \\geq m$$\n\n\nTherefore the sample minumum is a *biased estimator* of the true minumum.\n\n\n## Looking Ahead\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n## Upcoming\n\n- WHW10\n- (next LHW assignments will be given after Thanksgiving)\n  \nFinal Exam will be on Dec. 8\t\n:::\n\n::: {.column width=\"50%\"}\n![](lambda.png)\n\n[tweet source](https://mobile.twitter.com/KellyBodwin/status/1579981646844993536)\n:::\n\n::::\n\n\n\n\n\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::\n",
    "supporting": [
      "Math_32_23_Maximum_Likelihood_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}